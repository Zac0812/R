{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Importing Data\n",
    "\n",
    "In this second part to Importing Data in R, you will take a deeper dive into the wide range of data formats out there. More specifically, you'll learn how to import data from relational databases and how to import and work with data coming from the web. Finally, you'll get hands-on experience with importing data from statistical software packages such SAS, STATA and SPSS.\n",
    "\n",
    "## 1) Importing data from databases \n",
    "\n",
    "Many companies store their information in relational databases. The R community has also developed R packages to get data from these architectures. You'll learn how to connect to a database and how to retrieve data from it.\n",
    "\n",
    "### 1.1) Connect to a database\n",
    "First of all we need a packages that allow us relation with the DB and it will depend our SGDB, fo example:\n",
    "\n",
    "1. MySQL - RMySQL\n",
    "2. PosgresSQL- RPostgresSQL\n",
    "3. Oracle- ROracle\n",
    "4. Etc.\n",
    "\n",
    "But now, how R interact with the DB?, so which R functions you use to access and manipulate the DB, is specified in other R package called **DBI** in more technical terms, DBI is an interface and RmySQL is the implementation, so first we will install our RmySQL Packages, which automatically install the DBI package.\n",
    "\n",
    "    #First we need install our packages \n",
    "    install.packages(\"RMySQL\")\n",
    "    #library \"RmySQL not required yet!\n",
    "    library(DBI) \n",
    "    \n",
    "#### a) Establish a connection\n",
    "The first step to import data from a SQL database is creating a connection to it, you need different packages depending on the database you want to connect to. All of these packages do this in a uniform way, as specified in the DBI package.\n",
    "\n",
    "**dbConnect()** creates a connection between your R session and a SQL database. The first argument has to be a **DBIdriver** object, that specifies how connections are made and how data is mapped between R and the database. Specifically for MySQL databases, you can build such a driver with **RMySQL::MySQL()**.\n",
    "\n",
    "If the MySQL database is a remote database hosted on a server, you'll also have to specify the following arguments in **dbConnect(): dbname, host, port, user and password**. Most of these details have already been provided.\n",
    "\n",
    "#### b) Import table data\n",
    "\n",
    "After you've successfully connected to a remote MySQL database, the next step is to see what tables the database contains. You can do this with the **dbListTables()** function and the same way you can use **dbReadTable** to read a specific table.\n",
    "\n",
    "    Example\n",
    "    \n",
    "    #First we need install our packages \n",
    "    install.packages(\"RMySQL\")\n",
    "    #library \"RmySQL not required yet!\n",
    "    library(DBI) \n",
    "\n",
    "    #  dbConnect() call\n",
    "    con <- dbConnect(RMySQL::MySQL(), \n",
    "                     #dbname = \"tweater\", \n",
    "                     dbname = \"company\",\n",
    "                     host = \"courses.csrrinzqubik.us-east-1.rds.amazonaws.com\", \n",
    "                     port = 3306,\n",
    "                     user = \"student\",\n",
    "                     password = \"datacamp\")\n",
    "    \n",
    "    #To see the class\n",
    "    class(con)\n",
    "\n",
    "    #to see all tables\n",
    "    dbListTables(con)\n",
    "\n",
    "    #to read a specific table\n",
    "    dbReadTable(con,\"employees\")\n",
    "\n",
    "    #to disconnected \n",
    "    #dbDisconnect(con)\n",
    "\n",
    "#### c) SQL Queries from inside R\n",
    "In your life as a data scientist, you'll often be working with huge databases that contain tables with millions of rows. If you want to do some analyses on this data, it's possible that you only need a fraction of this data. In this case, it's a good idea to send SQL queries to your database, and only import the data you actually need into R.\n",
    "\n",
    "**dbGetQuery()** is what you need. As usual, you first pass the connection object to it. The second argument is an SQL query in the form of a character string, for example:\n",
    "\n",
    "    dbGetQuery(con, \"SELECT age FROM people WHERE gender = 'male'\")\n",
    "    \n",
    "the most important thing here that we can customize your sentence of sql and we won't have any problem with this.\n",
    "\n",
    "#### d) DBI internals\n",
    "You've used **dbGetQuery()** multiple times now. This is a virtual function from the DBI package, but is actually implemented by the RMySQL package. Behind the scenes, the following steps are performed:\n",
    "\n",
    "1. Sending the specified query with `dbSendQuery()`;\n",
    "2. Fetching the result of executing the query on the database with `dbFetch()`; You can specify the n argument inside dbFetch()\n",
    "3. Clearing the result with `dbClearResult()`  \n",
    "    \n",
    "So if we combined all these function, we will obtain the same result as **dbGetQuery()** , we do this, well **dbFetch** query allow us to specify a maximum numbers of records to retrieve per fetch, for example: \n",
    "\n",
    "    res<-dbSendQuery(con, \"SELECT age FROM people WHERE gender = 'male'\")\n",
    "    dbFetch(res)\n",
    "    dbClearResult(res)\n",
    "    \n",
    "#### d) Disconnect  \n",
    "Every time you connect to a database using dbConnect(), you're creating a new connection to the database you're referencing. RMySQL automatically specifies a maximum of open connections and closes some of the connections for you, but still: it's always polite to manually disconnect from the database afterwards. You do this with the **dbDisconnect()** function.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2) Importing data from the web\n",
    "More and more of the information that data scientists are using resides on the web. Importing this data into R requires an understanding of the protocols used on the web. In this chapter, you'll get a crash course in HTTP and learn to perform your own HTTP requests from inside R.\n",
    "\n",
    "### 2.1 HTTP\n",
    "HTTP: HyperText Transfer Protocol\n",
    "\n",
    "It´s a basically a system of rules for how data should be exchanged between computers, is the lenguage of the web \n",
    "\n",
    "#### a) Import flat files from the web\n",
    "You will see that the **utils functions** to import flat file data, such as `read.csv() and read.delim()`, are capable of automatically importing from URLs that point to flat files on the web.\n",
    "\n",
    "You must be wondering whether Hadley Wickham's alternative package, readr, is equally potent.\n",
    "\n",
    "    ##Working with Web flat file\n",
    "    library(readr)\n",
    "    # Import the csv file: pools\n",
    "    url_csv <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv\"\n",
    "\n",
    "\n",
    "    # Import the txt file: potatoes\n",
    "    url_delim <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt\"\n",
    "\n",
    "    #read information\n",
    "    pools<-read_csv(url_csv)\n",
    "    potatoes<-read_tsv(url_delim)\n",
    "\n",
    "**Note: readxl does not know how to handle Excel files that are stored on the Web **\n",
    "#### b) Secure importing\n",
    "In the previous example, you have been working with URLs that all start with http://. There is, however, a safer alternative to HTTP, namely HTTPS, which stands for HypterText Transfer Protocol Secure. Just remember this: HTTPS is relatively safe, HTTP is not.\n",
    "\n",
    "Luckily for us, you can use the standard importing functions with https:// connections since R version 3.2.2\n",
    "\n",
    "\n",
    "### 2.2 Downloading files\n",
    "When you learned about gdata, it was already mentioned that gdata can handle .xls files that are on the internet. readxl can't, at least not yet. The URL with which you'll be working is already available in the sample code. You will import it once using gdata and once with the readxl package via a workaround.\n",
    "\n",
    "    # Load the readxl and gdata package\n",
    "    library(readxl)\n",
    "    library(gdata)\n",
    "\n",
    "    # Specification of url: url_xls\n",
    "    url_xls <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls\"\n",
    "\n",
    "    # Import the .xls file with gdata: excel_gdata\n",
    "    excel_gdata<-read.xls(url_xls)\n",
    "\n",
    "    # Download file behind URL, name it local_latitude.xls\n",
    "    download.file(url_xls,\"local_latitude.xls\")\n",
    "\n",
    "    # Import the local .xls file with readxl: excel_readxl\n",
    "    excel_readxl<-read_excel(\"local_latitude.xls\")\n",
    "    \n",
    "#### a ) Downloading any file, secure or not\n",
    "In the previous example you've seen how you can read excel files on the web using the read_excel package by first downloading the file with the download.file() function.\n",
    "\n",
    "There's more: with download.file() you can download any kind of file from the web, using HTTP and HTTPS: images, executable files, but also .RData files. An RData file is very efficient format to store R data.\n",
    "\n",
    "You can load data from an RData file using the load() function, but this function does not accept a URL string as an argument. In this exercise, you'll first download the RData file securely, and then import the local data file.\n",
    "\n",
    "\n",
    "    # https URL to the wine RData file.\n",
    "    url_rdata <- \"https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData\"\n",
    "\n",
    "    # Download the wine file to your working directory\n",
    "    download.file(url_rdata,\"wine_local.RData\")\n",
    "\n",
    "    # Load the wine data into your workspace using load()\n",
    "    load(\"wine_local.RData\")\n",
    "\n",
    "    # Print out the summary of the wine data\n",
    "    summary(wine)\n",
    "    \n",
    "#### b) HTTP? httr! \n",
    "Downloading a file from the Internet means sending a GET request and receiving the file you asked for. Internally, all the previously discussed functions use a GET request to download files.\n",
    "\n",
    "httr provides a convenient function, GET() to execute this GET request. The result is a response object, that provides easy access to the status code, content-type and, of course, the actual content.\n",
    "\n",
    "You can extract the content from the request using:\n",
    "\n",
    "`the content()` function. At the time of writing, there are three ways to retrieve this content: as a raw object, as a character vector, or an R object, such as a list. If you don't tell `content()` how to retrieve the content through the `as` argument, it'll try its best to figure out which type is most appropriate based on the content-type.\n",
    "\n",
    "    # Load the httr package\n",
    "    library(httr)\n",
    "\n",
    "    # Get the url, save response to resp\n",
    "    url <- \"http://www.example.com/\"\n",
    "    resp <- GET(url)\n",
    "\n",
    "    # Print resp\n",
    "    resp\n",
    "\n",
    "    # Get the raw content of resp: raw_content\n",
    "    raw_content <- content(resp, as = \"raw\")\n",
    "\n",
    "    # Print the head of raw_content\n",
    "    head(raw_content)\n",
    "    \n",
    "Web content does not limit itself to HTML pages and files stored on remote servers such as DataCamp's Amazon S3 instances. There are many other data formats out there. A very common one is JSON. This format is very often used by so-called Web APIs, interfaces to web servers with which you as a client can communicate to get or store information in more complicated ways.\n",
    "\n",
    "You'll learn about Web APIs and JSON in the video and exercises that follow, but some experimentation never hurts, does it?\n",
    "\n",
    "    # httr is already loaded\n",
    "\n",
    "    # Get the url\n",
    "    url <- \"http://www.omdbapi.com/?apikey=ff21610b&t=Annie+Hall&y=&plot=short&r=json\"\n",
    "    resp<-GET(url)\n",
    "\n",
    "    # Print resp\n",
    "    resp\n",
    "\n",
    "    # Print content of resp as text\n",
    "    content(resp, as=\"text\")\n",
    "\n",
    "    # Print content of resp\n",
    "    content(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3) Importing data from the web II\n",
    "Importing data from the web is one thing; actually being able to extract useful information is another. Learn more about the JSON format to get one step closer to web domination.\n",
    "\n",
    "#### a) APIs & JSON\n",
    "The JSON format is very simple, concise and well-structered on top of that, it´s human-readable, but it´s easy to intepret and generate fo machine and this make it perfecto to communicate with Web APIs (Application Programming Interface) very generally put, it´s a set of routines and protocols for building software componets it´s a way in which different components interact, however we will be focus in Web API, this is an interface to get data and proccesed information from a server or to add data to a server.\n",
    "\n",
    "Now we are working with the following package:\n",
    "\n",
    "    ##JSON file\n",
    "    install.packages(\"jsonlite\")\n",
    "    library(jsonlite)\n",
    "\n",
    "#### b) From JSON to R\n",
    "In the simplest setting, `fromJSON()` can convert character strings that represent JSON data into a nicely structured R list, for example:\n",
    "\n",
    "     # Load the jsonlite package\n",
    "    library(jsonlite)\n",
    "\n",
    "    # wine_json is a JSON\n",
    "    wine_json <- '{\"name\":\"Chateau Migraine\", \"year\":1997, \"alcohol_pct\":12.4, \"color\":\"red\", \"awarded\":false}'\n",
    "\n",
    "    # Convert wine_json into a list: wine\n",
    "    wine<-fromJSON(wine_json)\n",
    "\n",
    "    # Print structure of wine\n",
    "    str(wine)\n",
    "\n",
    "#### c) Quandl API\n",
    "`fromJSON()` also works if you pass a URL as a character string or the path to a local file that contains JSON data. Let's try this out on the Quandl API, where you can fetch all sorts of financial and economical data.\n",
    "\n",
    "    # Definition of quandl_url\n",
    "    quandl_url <- \"https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?auth_token=i83asDsiWUUyfoypkgMz\"\n",
    "\n",
    "    # Import Quandl data: quandl_data\n",
    "    quandl_data<-fromJSON(quandl_url)\n",
    "\n",
    "    # Print structure of quandl_data\n",
    "    str(quandl_data)\n",
    "\n",
    "Note it´s posible that you need `install.packages('curl')`\n",
    "\n",
    "#### c) OMDb API - (The Open Movie Database)\n",
    "You also saw how to fetch all information on Rain Man from OMDb. Simply perform a ~`GET()` call, and next ask for the contents with the `content()` function. This `content()` function, which is part of the `httr` package, uses `jsonlite` behind the scenes to import the JSON data into R.\n",
    "\n",
    "However, by now you also know that jsonlite can handle URLs itself. Simply passing the request URL to `fromJSON()` will get your data into R\n",
    "\n",
    "    # The package jsonlite is already loaded\n",
    "\n",
    "    # Definition of the URLs\n",
    "    url_sw4 <- \"http://www.omdbapi.com/?apikey=ff21610b&i=tt0076759&r=json\"\n",
    "    url_sw3 <- \"http://www.omdbapi.com/?apikey=ff21610b&i=tt0121766&r=json\"\n",
    "\n",
    "    # Import two URLs with fromJSON(): sw4 and sw3\n",
    "    sw4 <- fromJSON(url_sw4)\n",
    "    sw3 <- fromJSON(url_sw3)\n",
    "\n",
    "    # Print out the Title element of both lists\n",
    "    sw4$Title\n",
    "    sw3$Title\n",
    "\n",
    "    # Is the release year of sw4 later than sw3?\n",
    "    sw4$Year>sw3$Year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON is built on two structures: objects and arrays. To help you experiment with these, two JSON strings are included in the sample code. It's up to you to change them appropriately and then call jsonlite's `fromJSON()` function on them each time\n",
    "\n",
    "    # jsonlite is already loaded\n",
    "\n",
    "    # Challenge 1\n",
    "    json1 <- '[1, 2,3 ,4,5, 6]'\n",
    "    fromJSON(json1)\n",
    "\n",
    "    # Challenge 2\n",
    "    json2 <- '{\"a\": [1, 2, 3],\"b\":[4,5,6]}'\n",
    "    fromJSON(json2)\n",
    "    \n",
    " Other Example:\n",
    " \n",
    "        #Challenge 1 matrix 2x2 \n",
    "        json1 <- '[[1, 2], [3, 4]]'\n",
    "        fromJSON(json1)\n",
    "\n",
    "        #Challenge 2 add a new observation\n",
    "        json2 <- '[{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}, {\"a\": 5, \"b\": 6}]'\n",
    "        fromJSON(json2)\n",
    "\n",
    "#### toJSON()\n",
    "Apart from converting JSON to R with `fromJSON()`, you can also use `toJSON()` to convert R data to a JSON format. In its most basic use, you simply pass this function an R object to convert to a JSON. The result is an R object of the class json, which is basically a character string representing that JSON.\n",
    "\n",
    "    # jsonlite is already loaded\n",
    "\n",
    "    # URL pointing to the .csv file\n",
    "    url_csv <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv\"\n",
    "\n",
    "    # Import the .csv file located at url_csv\n",
    "    water<-read.csv(url_csv, stringsAsFactors=FALSE)\n",
    "\n",
    "    # Convert the data file according to the requirements\n",
    "    water_json<-toJSON(water)\n",
    "\n",
    "    # Print out water_json\n",
    "    water_json\n",
    "\n",
    "#### Minify and prettify\n",
    "JSONs can come in different formats. Take these two JSONs, that are in fact exactly the same: the first one is in a minified format, the second one is in a pretty format with indentation, whitespace and new lines:\n",
    "\n",
    "    # Mini\n",
    "    {\"a\":1,\"b\":2,\"c\":{\"x\":5,\"y\":6}}\n",
    "\n",
    "    # Pretty\n",
    "    {\n",
    "      \"a\": 1,\n",
    "      \"b\": 2,\n",
    "      \"c\": {\n",
    "        \"x\": 5,\n",
    "        \"y\": 6\n",
    "      }\n",
    "    }\n",
    "\n",
    "\n",
    "Unless you're a computer, you surely prefer the second version. However, the standard form that `toJSON()` returns, is the minified version, as it is more concise. You can adapt this behavior by setting the pretty argument inside `toJSON()` to TRUE. If you already have a JSON string, you can use `prettify()` or `minify()` to make the JSON pretty or as concise as possible.\n",
    "\n",
    "    # jsonlite is already loaded\n",
    "\n",
    "    # Convert mtcars to a pretty JSON: pretty_json\n",
    "    pretty_json<-toJSON(mtcars, pretty=TRUE)\n",
    "\n",
    "    # Print pretty_json\n",
    "    pretty_json\n",
    "\n",
    "    # Minify pretty_json: mini_json\n",
    "    mini_json<-minify(pretty_json)\n",
    "\n",
    "    # Print mini_json\n",
    "    mini_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4) Importing data from statistical software packages\n",
    "Next to R, there are also other commonly used statistical software packages: SAS, STATA and SPSS. Each of them has their own file format. Learn how to use the haven and foreign packages to get them into R with remarkable ease!\n",
    "\n",
    "In the rest of this documentacion, we will see two new packages that allow us to work with this environment:\n",
    "\n",
    "1. haven, it´s more consistent, easier to use and faster to foreign \n",
    "2. foregin, support more data formats \n",
    "\n",
    "### Import SAS data with haven\n",
    "haven is an extremely easy-to-use package to import data from three software packages: SAS, STATA and SPSS. Depending on the software, you use different functions:\n",
    "\n",
    "    SAS: read_sas()\n",
    "    STATA: read_dta() (or read_stata(), which are identical)\n",
    "    SPSS: read_sav() or read_por(), depending on the file type.\n",
    "\n",
    "All these functions take one key argument: the path to your local file. In fact, you can even pass a URL; haven will then automatically download the file for you before importing it.\n",
    "\n",
    "You'll be working with data on the age, gender, income, and purchase level (0 = low, 1 = high) of 36 individuals (Source: SAS). The information is stored in a SAS file, sales.sas7bdat, which is available in your current working directory. You can also download the data.\n",
    "\n",
    "    # Load the haven package\n",
    "    library(haven)\n",
    "\n",
    "    # Import sales.sas7bdat: sales\n",
    "    sales<-read_sas(\"sales.sas7bdat\")\n",
    "\n",
    "    # Display the structure of sales\n",
    "    str(sales)\n",
    "\n",
    "### Import STATA data with haven\n",
    "Next up are STATA data files; you can use `read_dta()` for these.\n",
    "\n",
    "When inspecting the result of the `read_dta()` call, you will notice that one column will be imported as a labelled vector, an R equivalent for the common data structure in other statistical environments. In order to effectively continue working on the data in R, it's best to change this data into a standard R class. To convert a variable of the class labelled to a factor, you'll need haven's `as_factor()` function.\n",
    "\n",
    "In this exercise, you will work with data on yearly import and export numbers of sugar, both in USD and in weight. The data can be found at: http://assets.datacamp.com/production/course_1478/datasets/trade.dta\n",
    "\n",
    "    # haven is already loaded\n",
    "\n",
    "    # Import the data from the URL: sugar\n",
    "    sugar<-read_dta(\"http://assets.datacamp.com/production/course_1478/datasets/trade.dta\")\n",
    "\n",
    "    # Structure of sugar\n",
    "    str(sugar)\n",
    "\n",
    "    # Convert values in Date column to dates\n",
    "    sugar$Date<-as.Date(as_factor(sugar$Date))\n",
    "\n",
    "    # Structure of sugar again\n",
    "    str(sugar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import SPSS data with haven\n",
    "The haven package can also import data files from SPSS. Again, importing the data is pretty straightforward. Depending on the SPSS data file you're working with, you'll need either \n",
    "`read_sav() - for .sav files - or read_por() - for .por files`\n",
    "\n",
    "In this exercise, you will work with data on four of the Big Five personality traits for 434 persons (Source: University of Bath). The Big Five is a psychological concept including, originally, five dimensions of personality to classify human personality. The SPSS dataset is called person.sav and is available in your working directory.\n",
    "\n",
    "    # haven is already loaded\n",
    "\n",
    "    # Import person.sav: traits\n",
    "    traits<-read_sav(\"person.sav\")\n",
    "\n",
    "    # Summarize traits\n",
    "    summary(traits)\n",
    "\n",
    "    # Print out a subset\n",
    "    subset(traits, traits$Extroversion>40 & traits$Agreeableness>40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorize, round two\n",
    "In the last exercise you learned how to import a data file using the command read_sav(). With SPSS data files, it can also happen that some of the variables you import have the labelled class. This is done to keep all the labelling information that was originally present in the .sav and .por files. It's advised to coerce (or change) these variables to factors or other standard R classes.\n",
    "\n",
    "The data for this exercise involves information on employees and their demographic and economic attributes (Source: QRiE). The data can be found on the following URL:\n",
    "\n",
    "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav\n",
    "\n",
    "    # haven is already loaded\n",
    "\n",
    "    # Import SPSS data from the URL: work\n",
    "    work<-read_sav(\"http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav\")\n",
    "\n",
    "    # Display summary of work$GENDER\n",
    "    summary(work$GENDER)\n",
    "\n",
    "\n",
    "    # Convert work$GENDER to a factor\n",
    "    work$GENDER<-as_factor(work$GENDER)\n",
    "\n",
    "\n",
    "    # Display summary of work$GENDER again\n",
    "    summary(work$GENDER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don´t write the things that we saw in the package \"foreign\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

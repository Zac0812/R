{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Toolbox\n",
    "\n",
    "### Course Description\n",
    "Machine learning is the study and application of algorithms that learn from and make predictions on data. From search results to self-driving cars, it has manifested itself in all areas of our lives and is one of the most exciting and fast growing fields of research in the world of data science. This course teaches the big ideas in machine learning: how to build and evaluate predictive models, how to tune them for optimal performance, how to preprocess data for better results, and much more. The popular `caret` R package, which provides a consistent interface to all of R's most powerful machine learning facilities, is used throughout the course.\n",
    "\n",
    "Example: https://www.guru99.com/r-generalized-linear-model.html (muy bueno paso a paso)\n",
    "\n",
    "\n",
    "### Note how can Resizing plots in the R kernel for Jupyter notebooks\n",
    "https://blog.revolutionanalytics.com/2015/09/resizing-plots-in-the-r-kernel-for-jupyter-notebooks.html\n",
    "\n",
    "    library(repr)\n",
    "\n",
    "    # Change plot size to 4 x 3\n",
    "    options(repr.plot.width=4, repr.plot.height=3)\n",
    "    \n",
    "### Note2 Generate a table \n",
    "https://www.tablesgenerator.com/markdown_tables\n",
    "\n",
    "**Notas sobre el paquete caret**\n",
    "http://topepo.github.io/caret/model-training-and-tuning.html\n",
    "\n",
    "**Notas sobre Confusion matrix and measures\n",
    "https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62\n",
    "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n",
    "\n",
    "\n",
    "### Note 3 - DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'dplyr' was built under R version 3.5.3\"\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.5.3\"Warning message:\n",
      "\"package 'readr' was built under R version 3.5.3\"Warning message:\n",
      "\"package 'broom' was built under R version 3.5.3\"Warning message:\n",
      "\"package 'caret' was built under R version 3.5.3\"Loading required package: lattice\n",
      "Warning message:\n",
      "\"package 'openintro' was built under R version 3.5.2\"Please visit openintro.org for free statistics materials\n",
      "\n",
      "Attaching package: 'openintro'\n",
      "\n",
      "The following object is masked from 'package:caret':\n",
      "\n",
      "    dotPlot\n",
      "\n",
      "The following object is masked from 'package:lattice':\n",
      "\n",
      "    lsegments\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    diamonds\n",
      "\n",
      "The following objects are masked from 'package:datasets':\n",
      "\n",
      "    cars, trees\n",
      "\n",
      "Parsed with column specification:\n",
      "cols(\n",
      "  .default = col_double(),\n",
      "  y = col_character()\n",
      ")\n",
      "See spec(...) for full column specifications.\n"
     ]
    }
   ],
   "source": [
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(readr)\n",
    "library(tidyr)\n",
    "library(datasets)\n",
    "library(broom)\n",
    "library(caret)\n",
    "library(openintro)\n",
    "\n",
    "#Diamonds\n",
    "load(file = \"D:/Analista Pricing/6.0 Personal/R/DataSources/Diamonds.RData\")\n",
    "#Sonar\n",
    "load(file = \"D:/Analista Pricing/6.0 Personal/R/DataSources/Diamonds.RData\")\n",
    "#Wine\n",
    "wine<-readRDS(\"D:/Analista Pricing/6.0 Personal/R/DataSources/wine_100.RDS\")\n",
    "#Overfit data\n",
    "library(readr)\n",
    "path_csv<-\"https://assets.datacamp.com/production/repositories/223/datasets/0bd5f7c30d9aec3e1f1fa677a19bee3af407453a/overfit.csv\"\n",
    "overfit<-read_csv(path_csv)\n",
    "#Breast Cancer\n",
    "Cancer<-load(file = \"D:/Analista Pricing/6.0 Personal/R/DataSources/BreastCancer.RData\")\n",
    "#Blood-brain\n",
    "broodbrain<-load(file = \"D:/Analista Pricing/6.0 Personal/R/DataSources/BloodBrain.RData\")\n",
    "#Churn\n",
    "load(file = \"D:/Analista Pricing/6.0 Personal/R/DataSources/Churn.RData\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Regression models: fitting them and evaluating their performance\n",
    "In the first chapter of this course, you'll fit regression models with `train()` and evaluate their out-of-sample performance using cross-validation and root-mean-square error (RMSE).\n",
    "\n",
    "### 1.1) (video) Welcome to the Toolbox\n",
    "today `caret` package is one of the most widely used package in R for supervising learning (also known as predictive model), it´s when you have a target variable or something specifc that you whan to predict.\n",
    "\n",
    "There are two main kinds of predective models:\n",
    "1. Classification model (quialitative)\n",
    "2. Regresion model (quiantitative) \n",
    "\n",
    "Once we have a model, we use a \"metric\" to evaluate how well the model works, a metric is quantifiable and gives us an objective measure how well the model predicts on new data.\n",
    "\n",
    "for regresion problems  we focused on RMSE (root mean squared error) as our metric of choice, this is  the error that lineal model regresion typically seek to minimize but it´s not the best meausre beaucse it´s common practice to calculate RMSE on the same data we used to fit the model this typically leads to overly-optimistic thi called `overfitting`, a betther approach  it to use out-of-sample estimates of model performance.\n",
    "\n",
    "#### 1.1.1)  In-sample RMSE for linear regression\n",
    "RMSE is commonly calculated in-sample on your training set. What's a potential drawback (disadvantage) to calculating training set error?\n",
    "\n",
    "    Answer = You have no idea how well your model generalizes to new data (i.e. overfitting)\n",
    "\n",
    "#### 1.1.2) In-sample RMSE for linear regression on diamonds\n",
    "As you saw in the video, included in the course is the `diamonds` dataset, which is a classic dataset from the `ggplot2` package. The dataset contains physical attributes of diamonds as well as the price they sold for. One interesting modeling challenge is predicting diamond price based on their attributes using something like a linear regression.\n",
    "\n",
    "Recall that to fit a linear regression, you use the `lm()` function in the following format:\n",
    "\n",
    "    mod <- lm(y ~ x, my_data)\n",
    "    \n",
    "To make predictions using mod on the original data, you call the predict() function:\n",
    "\n",
    "    pred <- predict(mod, my_data)\n",
    "\n",
    "**Exercise**\n",
    "1. Fit a linear model on the diamonds dataset predicting price using all other variables as predictors (i.e. price ~ .). Save the result to model.\n",
    "2. Make predictions using model on the full original dataset and save the result to p.\n",
    "3. Compute errors using the formula errors=predicted−actual. Save the result to error.\n",
    "4. Compute RMSE using the formula you learned in the video and print it to the console.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>carat</th><th scope=col>cut</th><th scope=col>color</th><th scope=col>clarity</th><th scope=col>depth</th><th scope=col>table</th><th scope=col>price</th><th scope=col>x</th><th scope=col>y</th><th scope=col>z</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.23     </td><td>Ideal    </td><td>E        </td><td>SI2      </td><td>61.5     </td><td>55       </td><td>326      </td><td>3.95     </td><td>3.98     </td><td>2.43     </td></tr>\n",
       "\t<tr><td>0.21     </td><td>Premium  </td><td>E        </td><td>SI1      </td><td>59.8     </td><td>61       </td><td>326      </td><td>3.89     </td><td>3.84     </td><td>2.31     </td></tr>\n",
       "\t<tr><td>0.23     </td><td>Good     </td><td>E        </td><td>VS1      </td><td>56.9     </td><td>65       </td><td>327      </td><td>4.05     </td><td>4.07     </td><td>2.31     </td></tr>\n",
       "\t<tr><td>0.29     </td><td>Premium  </td><td>I        </td><td>VS2      </td><td>62.4     </td><td>58       </td><td>334      </td><td>4.20     </td><td>4.23     </td><td>2.63     </td></tr>\n",
       "\t<tr><td>0.31     </td><td>Good     </td><td>J        </td><td>SI2      </td><td>63.3     </td><td>58       </td><td>335      </td><td>4.34     </td><td>4.35     </td><td>2.75     </td></tr>\n",
       "\t<tr><td>0.24     </td><td>Very Good</td><td>J        </td><td>VVS2     </td><td>62.8     </td><td>57       </td><td>336      </td><td>3.94     </td><td>3.96     </td><td>2.48     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllll}\n",
       " carat & cut & color & clarity & depth & table & price & x & y & z\\\\\n",
       "\\hline\n",
       "\t 0.23      & Ideal     & E         & SI2       & 61.5      & 55        & 326       & 3.95      & 3.98      & 2.43     \\\\\n",
       "\t 0.21      & Premium   & E         & SI1       & 59.8      & 61        & 326       & 3.89      & 3.84      & 2.31     \\\\\n",
       "\t 0.23      & Good      & E         & VS1       & 56.9      & 65        & 327       & 4.05      & 4.07      & 2.31     \\\\\n",
       "\t 0.29      & Premium   & I         & VS2       & 62.4      & 58        & 334       & 4.20      & 4.23      & 2.63     \\\\\n",
       "\t 0.31      & Good      & J         & SI2       & 63.3      & 58        & 335       & 4.34      & 4.35      & 2.75     \\\\\n",
       "\t 0.24      & Very Good & J         & VVS2      & 62.8      & 57        & 336       & 3.94      & 3.96      & 2.48     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "carat | cut | color | clarity | depth | table | price | x | y | z | \n",
       "|---|---|---|---|---|---|\n",
       "| 0.23      | Ideal     | E         | SI2       | 61.5      | 55        | 326       | 3.95      | 3.98      | 2.43      | \n",
       "| 0.21      | Premium   | E         | SI1       | 59.8      | 61        | 326       | 3.89      | 3.84      | 2.31      | \n",
       "| 0.23      | Good      | E         | VS1       | 56.9      | 65        | 327       | 4.05      | 4.07      | 2.31      | \n",
       "| 0.29      | Premium   | I         | VS2       | 62.4      | 58        | 334       | 4.20      | 4.23      | 2.63      | \n",
       "| 0.31      | Good      | J         | SI2       | 63.3      | 58        | 335       | 4.34      | 4.35      | 2.75      | \n",
       "| 0.24      | Very Good | J         | VVS2      | 62.8      | 57        | 336       | 3.94      | 3.96      | 2.48      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  carat cut       color clarity depth table price x    y    z   \n",
       "1 0.23  Ideal     E     SI2     61.5  55    326   3.95 3.98 2.43\n",
       "2 0.21  Premium   E     SI1     59.8  61    326   3.89 3.84 2.31\n",
       "3 0.23  Good      E     VS1     56.9  65    327   4.05 4.07 2.31\n",
       "4 0.29  Premium   I     VS2     62.4  58    334   4.20 4.23 2.63\n",
       "5 0.31  Good      J     SI2     63.3  58    335   4.34 4.35 2.75\n",
       "6 0.24  Very Good J     VVS2    62.8  57    336   3.94 3.96 2.48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "  -4308    1073    2819    3933    5886   39394 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1129.84298657309"
      ],
      "text/latex": [
       "1129.84298657309"
      ],
      "text/markdown": [
       "1129.84298657309"
      ],
      "text/plain": [
       "[1] 1129.843"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data(diamonds)\n",
    "head(diamonds)\n",
    "\n",
    "# Fit lm model: model\n",
    "model <- lm(price ~ ., diamonds)\n",
    "\n",
    "# Predict on full data: p\n",
    "p <- predict(model, diamonds)\n",
    "summary(p)\n",
    "\n",
    "# Compute errors: error\n",
    "error <- p - diamonds[[\"price\"]]\n",
    "\n",
    "# Calculate RMSE\n",
    "sqrt(mean(error ^ 2))\n",
    "\n",
    "#Or can use the function Residual\n",
    "#sqrt(mean(residuals(model) ^ 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) (video) Out-of-sample error measures\n",
    "\n",
    "This course focused on predictive, rather than explanatory modeling, we want model that not overfit the training data and generalize well, our primary concern is \"do the models perform well on new data?\" so the best way to do that is test the models on new data.\n",
    "\n",
    "This simulates real worl experience, in which you fit on one dataset and then predict new data,where you do not actually know the outcome, simulating this experience with a train/test split helps you make an honest assessment fo yourself as a modeler this is one of the key insighs of machine learning: error metrics should be computed on new data because in-sample validatition (or predicting on your data) essentialy guarantees overfiting , out-of-sample validation helps you choose models that will continue perform well in the future.\n",
    "\n",
    "it´s the primary goal of the `caret` package in general and this course specifically: don´t overfit \n",
    "\n",
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>mpg</th><th scope=col>cyl</th><th scope=col>disp</th><th scope=col>hp</th><th scope=col>drat</th><th scope=col>wt</th><th scope=col>qsec</th><th scope=col>vs</th><th scope=col>am</th><th scope=col>gear</th><th scope=col>carb</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Mazda RX4</th><td>21.0 </td><td>6    </td><td>160  </td><td>110  </td><td>3.90 </td><td>2.620</td><td>16.46</td><td>0    </td><td>1    </td><td>4    </td><td>4    </td></tr>\n",
       "\t<tr><th scope=row>Mazda RX4 Wag</th><td>21.0 </td><td>6    </td><td>160  </td><td>110  </td><td>3.90 </td><td>2.875</td><td>17.02</td><td>0    </td><td>1    </td><td>4    </td><td>4    </td></tr>\n",
       "\t<tr><th scope=row>Datsun 710</th><td>22.8 </td><td>4    </td><td>108  </td><td> 93  </td><td>3.85 </td><td>2.320</td><td>18.61</td><td>1    </td><td>1    </td><td>4    </td><td>1    </td></tr>\n",
       "\t<tr><th scope=row>Hornet 4 Drive</th><td>21.4 </td><td>6    </td><td>258  </td><td>110  </td><td>3.08 </td><td>3.215</td><td>19.44</td><td>1    </td><td>0    </td><td>3    </td><td>1    </td></tr>\n",
       "\t<tr><th scope=row>Hornet Sportabout</th><td>18.7 </td><td>8    </td><td>360  </td><td>175  </td><td>3.15 </td><td>3.440</td><td>17.02</td><td>0    </td><td>0    </td><td>3    </td><td>2    </td></tr>\n",
       "\t<tr><th scope=row>Valiant</th><td>18.1 </td><td>6    </td><td>225  </td><td>105  </td><td>2.76 </td><td>3.460</td><td>20.22</td><td>1    </td><td>0    </td><td>3    </td><td>1    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllll}\n",
       "  & mpg & cyl & disp & hp & drat & wt & qsec & vs & am & gear & carb\\\\\n",
       "\\hline\n",
       "\tMazda RX4 & 21.0  & 6     & 160   & 110   & 3.90  & 2.620 & 16.46 & 0     & 1     & 4     & 4    \\\\\n",
       "\tMazda RX4 Wag & 21.0  & 6     & 160   & 110   & 3.90  & 2.875 & 17.02 & 0     & 1     & 4     & 4    \\\\\n",
       "\tDatsun 710 & 22.8  & 4     & 108   &  93   & 3.85  & 2.320 & 18.61 & 1     & 1     & 4     & 1    \\\\\n",
       "\tHornet 4 Drive & 21.4  & 6     & 258   & 110   & 3.08  & 3.215 & 19.44 & 1     & 0     & 3     & 1    \\\\\n",
       "\tHornet Sportabout & 18.7  & 8     & 360   & 175   & 3.15  & 3.440 & 17.02 & 0     & 0     & 3     & 2    \\\\\n",
       "\tValiant & 18.1  & 6     & 225   & 105   & 2.76  & 3.460 & 20.22 & 1     & 0     & 3     & 1    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | mpg | cyl | disp | hp | drat | wt | qsec | vs | am | gear | carb | \n",
       "|---|---|---|---|---|---|\n",
       "| Mazda RX4 | 21.0  | 6     | 160   | 110   | 3.90  | 2.620 | 16.46 | 0     | 1     | 4     | 4     | \n",
       "| Mazda RX4 Wag | 21.0  | 6     | 160   | 110   | 3.90  | 2.875 | 17.02 | 0     | 1     | 4     | 4     | \n",
       "| Datsun 710 | 22.8  | 4     | 108   |  93   | 3.85  | 2.320 | 18.61 | 1     | 1     | 4     | 1     | \n",
       "| Hornet 4 Drive | 21.4  | 6     | 258   | 110   | 3.08  | 3.215 | 19.44 | 1     | 0     | 3     | 1     | \n",
       "| Hornet Sportabout | 18.7  | 8     | 360   | 175   | 3.15  | 3.440 | 17.02 | 0     | 0     | 3     | 2     | \n",
       "| Valiant | 18.1  | 6     | 225   | 105   | 2.76  | 3.460 | 20.22 | 1     | 0     | 3     | 1     | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "                  mpg  cyl disp hp  drat wt    qsec  vs am gear carb\n",
       "Mazda RX4         21.0 6   160  110 3.90 2.620 16.46 0  1  4    4   \n",
       "Mazda RX4 Wag     21.0 6   160  110 3.90 2.875 17.02 0  1  4    4   \n",
       "Datsun 710        22.8 4   108   93 3.85 2.320 18.61 1  1  4    1   \n",
       "Hornet 4 Drive    21.4 6   258  110 3.08 3.215 19.44 1  0  3    1   \n",
       "Hornet Sportabout 18.7 8   360  175 3.15 3.440 17.02 0  0  3    2   \n",
       "Valiant           18.1 6   225  105 2.76 3.460 20.22 1  0  3    1   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lest walk throught a simple example of out-of-sample validation\n",
    "data(mtcars)\n",
    "head(mtcars,)\n",
    "\n",
    "#Fit a model to the mtcars data\n",
    "#we create our model on 20 register from our dataset\n",
    "model<-lm(mpg ~ hp, mtcars[1:20,])\n",
    "\n",
    "#Predict out-of-sample\n",
    "#now we make predictions on a New dataset: last 12 observations\n",
    "predicted<-predict(model, mtcars[21:32,], type = \"response\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, rather than manually splitting the dataset, we´d actually use the `createResamples` or `createFolds` in `caret`, but the manual split simplifies this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"RMSE on the test set\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "5.50723581646381"
      ],
      "text/latex": [
       "5.50723581646381"
      ],
      "text/markdown": [
       "5.50723581646381"
      ],
      "text/plain": [
       "[1] 5.507236"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"RMSE full dataset\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "3.74029708689949"
      ],
      "text/latex": [
       "3.74029708689949"
      ],
      "text/markdown": [
       "3.74029708689949"
      ],
      "text/plain": [
       "[1] 3.740297"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Evaluate error\n",
    "#finally we calculate RMSE on the test set by comparing the predictions from our model to the actual mpg value for the test set\n",
    "actual<-mtcars[21:32,\"mpg\"]\n",
    "print(\"RMSE on the test set\")\n",
    "sqrt(mean((predicted-actual)^2))\n",
    "\n",
    "#RMSE is a measure of the model´s average error and it has the same units as the test set, so this means\n",
    "#our model is off by 5 to 6 miles per gallon, on average \n",
    "\n",
    "#Compare to in-sample RMSE\n",
    "\n",
    "#fit a model to the full dataset\n",
    "model2<-lm(mpg ~ hp, mtcars)\n",
    "\n",
    "#Predict in-sample\n",
    "predicted2<-predict(model2, mtcars, type = \"response\")\n",
    "\n",
    "#Evaluate error\n",
    "print(\"RMSE full dataset\")\n",
    "actual2<-mtcars[,\"mpg\"]\n",
    "sqrt(mean((predicted2-actual2)^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compared to in-sample RMSE from a model fit on the full dataset, our model is significantly worse if we used in-sample error, we would fooled ourselves into thinking our models is much better that it actually is in reality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1) Out-of-sample RMSE for linear regression\n",
    "What is the advantage of using a train/test split rather than just validating your model in-sample on the training set? Because It gives you an estimate of how well your model performs on new data, the Tests sets are essential for making sure your models will make good predictions\n",
    "\n",
    "#### 1.1.2) Randomly order the data frame\n",
    "One way you can take a train/test split of a dataset is to order the dataset randomly, then divide it into the two sets. This ensures that the training set and test set are both random samples and that any biases in the ordering of the dataset (e.g. if it had originally been ordered by price or size) are not retained in the samples we take for training and testing your models. You can think of this like shuffling a brand new deck of playing cards before dealing hands.\n",
    "\n",
    "First, you set a random seed so that your work is reproducible and you get the same random split each time you run your script:\n",
    "\n",
    "    set.seed(42)\n",
    "\n",
    "Next, you use the `sample()` function to shuffle the row indices of the `diamonds` dataset. You can later use these indices to reorder the dataset.\n",
    "\n",
    "    rows <- sample(nrow(diamonds))\n",
    "\n",
    "Finally, you can use this random vector to reorder the diamonds dataset:\n",
    "\n",
    "    diamonds <- diamonds[rows, ]\n",
    "    \n",
    "**Exercise**\n",
    "1. Set the random seed to 42.\n",
    "2. Make a vector of row indices called rows.\n",
    "3. Randomly reorder the diamonds data frame, assigning to shuffled_diamonds.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note the data ser \"diamonds\" is avalibre in library ggplot2\n",
    "# Set seed\n",
    "set.seed(42)\n",
    "\n",
    "# Shuffle row indices: rows\n",
    "rows<-sample(nrow(diamonds))\n",
    "\n",
    "# Randomly order data\n",
    "shuffled_diamonds <- diamonds[rows, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Randomly ordering your dataset is important for many machine learning methods**\n",
    "\n",
    "#### 1.2.3) Try an 80/20 split\n",
    "Now that your dataset is randomly ordered, you can split the first 80% of it into a training set, and the last 20% into a test set. You can do this by choosing a split point approximately 80% of the way through your data:\n",
    "\n",
    "    split <- round(nrow(mydata) * .80)\n",
    "    \n",
    "You can then use this point to break off the first 80% of the dataset as a training set:\n",
    "\n",
    "    mydata[1:split, ]\n",
    "\n",
    "And then you can use that same point to determine the test set:\n",
    "\n",
    "    mydata[(split + 1):nrow(mydata), ]\n",
    "    \n",
    "**Exercise**\n",
    "1. Choose a row index to split on so that the split point is approximately 80% of the way through the diamonds dataset. Call this index split.\n",
    "2. Create a training set called train using that index.\n",
    "3. Create a test set called test using that index.\n",
    "\n",
    "*Answer*    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note we´re using our data set randomly ordering called \"shuffled_diamonds\" and not \"diamonds\" dataset the correct answear \n",
    "\n",
    "# Determine row to split on: split\n",
    "split<- round(nrow(shuffled_diamonds)*.80)\n",
    "\n",
    "# Create train\n",
    "train<- shuffled_diamonds[1:split,]\n",
    "\n",
    "# Create test\n",
    "test<-shuffled_diamonds[(split + 1):nrow(shuffled_diamonds),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4) Predict on test set\n",
    "Now that you have a randomly split `training set` and `test set`, you can use the `lm()` function as you did in the first exercise to fit a model to your training set, rather than the entire dataset. Recall that you can use the formula interface to the linear regression function *to fit a model with a specified target variable using all other variables in the dataset* as predictors:\n",
    "\n",
    "    mod <- lm(y ~ ., training_data)\n",
    "    \n",
    "You can use the `predict()` function to make predictions from that model on new data. The new dataset must have all of the columns from the training data, but they can be in a different order with different values. Here, rather than re-predicting on the training set, you can predict on the test set, which you did not use for training the model. This will allow you to determine the out-of-sample error for the model in the next exercise:\n",
    "\n",
    "    p <- predict(model, new_data)    \n",
    "\n",
    "**Exercise**\n",
    "1. Fit an lm() model called model to predict price using all other variables as covariates. Be sure to use the training set, train.\n",
    "2. Predict on the test set, test, using predict(). Store these values in a vector called p.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit lm model on train: model\n",
    "model<-lm(price ~ ., train)\n",
    "\n",
    "# Predict on test: p\n",
    "p<-predict(model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4) Calculate test set RMSE by hand\n",
    "Now that you have predictions on the test set, you can use these predictions to calculate an error metric (in this case RMSE) on the test set and see how the model performs out-of-sample, rather than in-sample as you did in the first exercise. You first do this by calculating the errors between the predicted diamond prices and the actual diamond prices by subtracting the predictions from the actual values.\n",
    "\n",
    "Once you have an error vector, calculating RMSE is as simple as squaring it, taking the mean, then taking the square root:\n",
    "\n",
    "    sqrt(mean(error^2))\n",
    "\n",
    "**Exercise**\n",
    "`test`, `model`, and `p` are loaded in your workspace.\n",
    "\n",
    "1. Calculate the error between the predictions on the test set and the actual diamond prices in the test set. Call this error.\n",
    "2. Calculate RMSE using this error vector, just printing the result to the console.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1136.59565324766"
      ],
      "text/latex": [
       "1136.59565324766"
      ],
      "text/markdown": [
       "1136.59565324766"
      ],
      "text/plain": [
       "[1] 1136.596"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute errors: error\n",
    "error<-p-test$price\n",
    "\n",
    "# Calculate RMSE\n",
    "sqrt(mean(error^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5) Comparing out-of-sample RMSE to in-sample RMSE\n",
    "Why is the test set RMSE higher than the training set RMSE?\n",
    "Because you overfit the training set and the test set contains data the model hasn't seen before, Computing the error on the training set is risky because the model may overfit the data used to train it.\n",
    "\n",
    "here we can se the RMSE of train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1128.25085917109"
      ],
      "text/latex": [
       "1128.25085917109"
      ],
      "text/markdown": [
       "1128.25085917109"
      ],
      "text/plain": [
       "[1] 1128.251"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute errors: error\n",
    "error_train<-predict(model, train) - train$price\n",
    "\n",
    "# Calculate RMSE of train set\n",
    "sqrt(mean(error_train^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) (video) Cross-validation\n",
    "\n",
    "in the last chapter we saw how can divided our Full Dataset in two sub dataset (training set and test set) and evaluated out-of sample error once, however this process it´s a little  fragile: because the presence or absence of a sigle outliers can vastly change our out-of-sample RMSE, so a better approach than a simple train/test split is using multiple test sets and averaging out-of-sample error, which gives us a more precise estimate of true out-of-sample error, one of the most common approaches for multiple test sets is know as `cross-validation` in wich we split our data into ten `folders` or traint/test splits, we create these folds in such a way that each point in our dataset ocurrs in exactly one test set.\n",
    "\n",
    "    Full DataSet >>> Fold 1\n",
    "                     Fold 2\n",
    "                     .\n",
    "                     .\n",
    "                     Fold N\n",
    "\n",
    "We assign each row to its single test set randomly , to avoid any kind of systematic biases in our data this one of the best way to estimate out-of-sample error for predictive models. \n",
    "\n",
    "One important note: after doing cross-validation, you throw all resampled models away and star over! so cross-validation is only used to estimate the out-of-sample error for you model once you know this, you re-fit your model on the full training dataset, so as to fully exploit the information in that dataset, all this make the model very expensive: it inherently takes 11 times as long as fitting a single model (11 cross-validation models plus the final model).\n",
    "\n",
    "The `train` function in `caret ` does a different kind of re-sampling known as boostrap validationt but it also capable of doing cross-validation, and the two methods in practice yield similar results, now we´ll see a example with mtcars dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: intercept=TRUE \n",
      "- Fold01: intercept=TRUE \n",
      "+ Fold02: intercept=TRUE \n",
      "- Fold02: intercept=TRUE \n",
      "+ Fold03: intercept=TRUE \n",
      "- Fold03: intercept=TRUE \n",
      "+ Fold04: intercept=TRUE \n",
      "- Fold04: intercept=TRUE \n",
      "+ Fold05: intercept=TRUE \n",
      "- Fold05: intercept=TRUE \n",
      "+ Fold06: intercept=TRUE \n",
      "- Fold06: intercept=TRUE \n",
      "+ Fold07: intercept=TRUE \n",
      "- Fold07: intercept=TRUE \n",
      "+ Fold08: intercept=TRUE \n",
      "- Fold08: intercept=TRUE \n",
      "+ Fold09: intercept=TRUE \n",
      "- Fold09: intercept=TRUE \n",
      "+ Fold10: intercept=TRUE \n",
      "- Fold10: intercept=TRUE \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear Regression \n",
       "\n",
       "32 samples\n",
       " 1 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 30, 29, 29, 28, 29, 28, ... \n",
       "Resampling results:\n",
       "\n",
       "  RMSE      Rsquared   MAE     \n",
       "  3.957996  0.9252153  3.349958\n",
       "\n",
       "Tuning parameter 'intercept' was held constant at a value of TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#first we set random seed\n",
    "set.seed(42)\n",
    "\n",
    "#fit linear regresion model\n",
    "model<- train(\n",
    "  mpg ~ hp, \n",
    "  mtcars, \n",
    "  method = \"lm\",\n",
    "  trControl = trainControl(   #Controls the parameter caret uses for cross-validation\n",
    "    method = \"cv\",\n",
    "    number = 10,\n",
    "    verboseIter = TRUE # progress log as the model is being fit \n",
    "  )\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1) Advantage of cross-validation\n",
    "What is the advantage of cross-validation over a single train/test split?\n",
    "It gives you multiple estimates of out-of-sample error, rather than a single estimate, if all of your estimates give similar outputs, you can be more certain of the model's accuracy. If your estimates give different outputs, that tells you the model does not perform consistently and suggests a problem with it\n",
    "\n",
    "#### 1.3.2 ) 10-fold cross-validation\n",
    "As you saw in the video, a better approach to validating models is to use multiple systematic test sets, rather than a single random train/test split. Fortunately, the `caret` package makes this very easy to do:\n",
    "\n",
    "    model <- train(y ~ ., my_data)\n",
    "    \n",
    "`caret` supports many types of cross-validation, and you can specify which type of cross-validation and the number of cross-validation folds with the `trainControl()` function, which you pass to the `trControl` argument in `train()`:\n",
    "\n",
    "    model <- train(\n",
    "      y ~ ., \n",
    "      my_data,\n",
    "      method = \"lm\",\n",
    "      trControl = trainControl(\n",
    "        method = \"cv\", \n",
    "        number = 10,\n",
    "        verboseIter = TRUE\n",
    "      )\n",
    "    )\n",
    " \n",
    " \n",
    "It's important to note that you pass the method for modeling to the main `train()` function and the method for cross-validation to the `trainControl()` function.    \n",
    "\n",
    "**Exercise**\n",
    "1. Fit a linear regression to model price using all other variables in the diamonds dataset as predictors. Use the train() function and 10-fold cross-validation. (Note that we've taken a subset of the full diamonds dataset to speed up this operation, but it's still named diamonds.)\n",
    "2. Print the model to the console and examine the results.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: intercept=TRUE \n",
      "- Fold01: intercept=TRUE \n",
      "+ Fold02: intercept=TRUE \n",
      "- Fold02: intercept=TRUE \n",
      "+ Fold03: intercept=TRUE \n",
      "- Fold03: intercept=TRUE \n",
      "+ Fold04: intercept=TRUE \n",
      "- Fold04: intercept=TRUE \n",
      "+ Fold05: intercept=TRUE \n",
      "- Fold05: intercept=TRUE \n",
      "+ Fold06: intercept=TRUE \n",
      "- Fold06: intercept=TRUE \n",
      "+ Fold07: intercept=TRUE \n",
      "- Fold07: intercept=TRUE \n",
      "+ Fold08: intercept=TRUE \n",
      "- Fold08: intercept=TRUE \n",
      "+ Fold09: intercept=TRUE \n",
      "- Fold09: intercept=TRUE \n",
      "+ Fold10: intercept=TRUE \n",
      "- Fold10: intercept=TRUE \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear Regression \n",
       "\n",
       "53940 samples\n",
       "    9 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 48546, 48546, 48545, 48547, 48547, 48546, ... \n",
       "Resampling results:\n",
       "\n",
       "  RMSE      Rsquared   MAE     \n",
       "  1130.929  0.9196497  740.5019\n",
       "\n",
       "Tuning parameter 'intercept' was held constant at a value of TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit lm model using 10-fold CV: model\n",
    "model <- train(\n",
    "  price ~ ., \n",
    "  diamonds,\n",
    "  method = \"lm\",\n",
    "  trControl = trainControl(\n",
    "    method = \"cv\", \n",
    "    number = 10,\n",
    "    verboseIter = TRUE\n",
    "  )\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3) 5-fold cross-validation\n",
    "In this course, you will use a wide variety of datasets to explore the full flexibility of the `caret package`. Here, you will use the famous Boston housing dataset, where the goal is to predict median home values in various Boston suburbs.\n",
    "\n",
    "You can use exactly the same code as in the previous exercise, but change the dataset used by the model:\n",
    "\n",
    "    model <- train(\n",
    "      medv ~ ., \n",
    "      Boston, # <- new!\n",
    "      method = \"lm\",\n",
    "      trControl = trainControl(\n",
    "        method = \"cv\", \n",
    "        number = 10,\n",
    "        verboseIter = TRUE\n",
    "      )\n",
    "    )\n",
    "\n",
    "Next, you can reduce the number of cross-validation folds from 10 to 5 using the `number` argument to the `trainControl()` argument:\n",
    "\n",
    "    trControl = trainControl(\n",
    "      method = \"cv\", \n",
    "      number = 5,\n",
    "      verboseIter = TRUE\n",
    "    )\n",
    "\n",
    "**Exercise**\n",
    "1. Fit an `lm()` model to the `Boston` housing dataset, such that `medv` is the response variable and all other variables are explanatory variables.\n",
    "2. Use 5-fold cross-validation rather than 10-fold cross-validation.\n",
    "3. Print the model to the console and inspect the results.\n",
    "\n",
    "*Answer*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'MASS'\n",
      "\n",
      "The following objects are masked from 'package:openintro':\n",
      "\n",
      "    housing, mammals\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    select\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1: intercept=TRUE \n",
      "- Fold1: intercept=TRUE \n",
      "+ Fold2: intercept=TRUE \n",
      "- Fold2: intercept=TRUE \n",
      "+ Fold3: intercept=TRUE \n",
      "- Fold3: intercept=TRUE \n",
      "+ Fold4: intercept=TRUE \n",
      "- Fold4: intercept=TRUE \n",
      "+ Fold5: intercept=TRUE \n",
      "- Fold5: intercept=TRUE \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear Regression \n",
       "\n",
       "506 samples\n",
       " 13 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold) \n",
       "Summary of sample sizes: 404, 405, 405, 405, 405 \n",
       "Resampling results:\n",
       "\n",
       "  RMSE      Rsquared   MAE     \n",
       "  4.816486  0.7249456  3.404306\n",
       "\n",
       "Tuning parameter 'intercept' was held constant at a value of TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Data Boston\n",
    "library(MASS)\n",
    "\n",
    "\n",
    "# Fit lm model using 5-fold CV: model\n",
    "model <- train(\n",
    "  medv ~ ., \n",
    "  Boston,\n",
    "  method = \"lm\",\n",
    "  trControl = trainControl(\n",
    "    method = \"cv\", #Cross validation \n",
    "    number = 5,\n",
    "    verboseIter = TRUE\n",
    "  )\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3) 5 x 5-fold cross-validation\n",
    "You **can do more than just one iteration of cross-validation**. Repeated cross-validation gives you a better estimate of the test-set error. You can also repeat the entire cross-validation procedure. This takes longer, but gives you many more out-of-sample datasets to look at and much more precise assessments of how well the model performs.\n",
    "\n",
    "One of the awesome things about the `train()` function in `caret` is how easy it is to run very different models or methods of cross-validation just by tweaking a few simple arguments to the function call. For example, you could repeat your entire cross-validation procedure 5 times for greater confidence in your estimates of the model's out-of-sample accuracy, e.g.:\n",
    "\n",
    "    trControl = trainControl(\n",
    "      method = \"repeatedcv\", #define the repetead\n",
    "      number = 5,\n",
    "      repeats = 5,  #We can repetead cross-validation\n",
    "      verboseIter = TRUE\n",
    "    )\n",
    "\n",
    "Suppose that method = \"repeatedcv\", number = 10 and repeats = 3,then **three** separate 10-fold cross-validations are used as the resampling scheme.\n",
    "\n",
    "**Exercise**\n",
    "1. Re-fit the linear regression model to the Boston housing dataset.\n",
    "2. Use 5 repeats of 5-fold cross-validation.\n",
    "3. Print the model to the console.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1.Rep1: intercept=TRUE \n",
      "- Fold1.Rep1: intercept=TRUE \n",
      "+ Fold2.Rep1: intercept=TRUE \n",
      "- Fold2.Rep1: intercept=TRUE \n",
      "+ Fold3.Rep1: intercept=TRUE \n",
      "- Fold3.Rep1: intercept=TRUE \n",
      "+ Fold4.Rep1: intercept=TRUE \n",
      "- Fold4.Rep1: intercept=TRUE \n",
      "+ Fold5.Rep1: intercept=TRUE \n",
      "- Fold5.Rep1: intercept=TRUE \n",
      "+ Fold1.Rep2: intercept=TRUE \n",
      "- Fold1.Rep2: intercept=TRUE \n",
      "+ Fold2.Rep2: intercept=TRUE \n",
      "- Fold2.Rep2: intercept=TRUE \n",
      "+ Fold3.Rep2: intercept=TRUE \n",
      "- Fold3.Rep2: intercept=TRUE \n",
      "+ Fold4.Rep2: intercept=TRUE \n",
      "- Fold4.Rep2: intercept=TRUE \n",
      "+ Fold5.Rep2: intercept=TRUE \n",
      "- Fold5.Rep2: intercept=TRUE \n",
      "+ Fold1.Rep3: intercept=TRUE \n",
      "- Fold1.Rep3: intercept=TRUE \n",
      "+ Fold2.Rep3: intercept=TRUE \n",
      "- Fold2.Rep3: intercept=TRUE \n",
      "+ Fold3.Rep3: intercept=TRUE \n",
      "- Fold3.Rep3: intercept=TRUE \n",
      "+ Fold4.Rep3: intercept=TRUE \n",
      "- Fold4.Rep3: intercept=TRUE \n",
      "+ Fold5.Rep3: intercept=TRUE \n",
      "- Fold5.Rep3: intercept=TRUE \n",
      "+ Fold1.Rep4: intercept=TRUE \n",
      "- Fold1.Rep4: intercept=TRUE \n",
      "+ Fold2.Rep4: intercept=TRUE \n",
      "- Fold2.Rep4: intercept=TRUE \n",
      "+ Fold3.Rep4: intercept=TRUE \n",
      "- Fold3.Rep4: intercept=TRUE \n",
      "+ Fold4.Rep4: intercept=TRUE \n",
      "- Fold4.Rep4: intercept=TRUE \n",
      "+ Fold5.Rep4: intercept=TRUE \n",
      "- Fold5.Rep4: intercept=TRUE \n",
      "+ Fold1.Rep5: intercept=TRUE \n",
      "- Fold1.Rep5: intercept=TRUE \n",
      "+ Fold2.Rep5: intercept=TRUE \n",
      "- Fold2.Rep5: intercept=TRUE \n",
      "+ Fold3.Rep5: intercept=TRUE \n",
      "- Fold3.Rep5: intercept=TRUE \n",
      "+ Fold4.Rep5: intercept=TRUE \n",
      "- Fold4.Rep5: intercept=TRUE \n",
      "+ Fold5.Rep5: intercept=TRUE \n",
      "- Fold5.Rep5: intercept=TRUE \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear Regression \n",
       "\n",
       "506 samples\n",
       " 13 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold, repeated 5 times) \n",
       "Summary of sample sizes: 404, 405, 405, 405, 405, 404, ... \n",
       "Resampling results:\n",
       "\n",
       "  RMSE      Rsquared   MAE     \n",
       "  4.837325  0.7267669  3.398271\n",
       "\n",
       "Tuning parameter 'intercept' was held constant at a value of TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# medv median value of owner-occupied homes in \\$1000s\n",
    "model <- train(\n",
    "  medv ~ ., \n",
    "  Boston,\n",
    "  method = \"lm\",\n",
    "  trControl = trainControl(\n",
    "    method = \"repeatedcv\", \n",
    "    number = 5,\n",
    "    repeats = 5, \n",
    "    verboseIter = TRUE\n",
    "  )\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4) Making predictions on new data\n",
    "Finally, the model you fit with the `train()` function has the exact same `predict()` interface as the linear regression models you fit earlier in this chapter.\n",
    "\n",
    "After fitting a model with `train()`, you can simply call `predict()` with new data, e.g:\n",
    "\n",
    "    predict(my_model, new_data)\n",
    "\n",
    "**Exercise**\n",
    "Use the predict() function to make predictions with model on the full Boston housing dataset. Print the result to the console.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result<-predict(model, Boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Classification models: fitting them and evaluating their performance\n",
    "\n",
    "In this chapter, you'll fit classification models with `train()` and evaluate their out-of-sample performance using cross-validation and area under the curve (AUC).\n",
    "\n",
    "### 2.1) (video) Logistic regression on sonar\n",
    "\n",
    "in classification models you're trying to predict a categorical target, for example predicting whether or not will a loan default? this is still a form of supervised learning, like with regression problems, as before,  we can use a train/test split to explore how well our model generalizes to new data, in this chapter we will been working with the `sonar` dataset, a classic statistics dataset whic contains some characteristics of a sonar signal for objects that are either rocks or mines the goal here is to train a classifier that can relibly distinguish rocks from mines.\n",
    "\n",
    "So let´s load the Sonar dataset and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'mlbench' was built under R version 3.5.3\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>V1</th><th scope=col>V2</th><th scope=col>V3</th><th scope=col>V4</th><th scope=col>V5</th><th scope=col>Class</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.0200</td><td>0.0371</td><td>0.0428</td><td>0.0207</td><td>0.0954</td><td>R     </td></tr>\n",
       "\t<tr><td>0.0453</td><td>0.0523</td><td>0.0843</td><td>0.0689</td><td>0.1183</td><td>R     </td></tr>\n",
       "\t<tr><td>0.0262</td><td>0.0582</td><td>0.1099</td><td>0.1083</td><td>0.0974</td><td>R     </td></tr>\n",
       "\t<tr><td>0.0100</td><td>0.0171</td><td>0.0623</td><td>0.0205</td><td>0.0205</td><td>R     </td></tr>\n",
       "\t<tr><td>0.0762</td><td>0.0666</td><td>0.0481</td><td>0.0394</td><td>0.0590</td><td>R     </td></tr>\n",
       "\t<tr><td>0.0286</td><td>0.0453</td><td>0.0277</td><td>0.0174</td><td>0.0384</td><td>R     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       " V1 & V2 & V3 & V4 & V5 & Class\\\\\n",
       "\\hline\n",
       "\t 0.0200 & 0.0371 & 0.0428 & 0.0207 & 0.0954 & R     \\\\\n",
       "\t 0.0453 & 0.0523 & 0.0843 & 0.0689 & 0.1183 & R     \\\\\n",
       "\t 0.0262 & 0.0582 & 0.1099 & 0.1083 & 0.0974 & R     \\\\\n",
       "\t 0.0100 & 0.0171 & 0.0623 & 0.0205 & 0.0205 & R     \\\\\n",
       "\t 0.0762 & 0.0666 & 0.0481 & 0.0394 & 0.0590 & R     \\\\\n",
       "\t 0.0286 & 0.0453 & 0.0277 & 0.0174 & 0.0384 & R     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "V1 | V2 | V3 | V4 | V5 | Class | \n",
       "|---|---|---|---|---|---|\n",
       "| 0.0200 | 0.0371 | 0.0428 | 0.0207 | 0.0954 | R      | \n",
       "| 0.0453 | 0.0523 | 0.0843 | 0.0689 | 0.1183 | R      | \n",
       "| 0.0262 | 0.0582 | 0.1099 | 0.1083 | 0.0974 | R      | \n",
       "| 0.0100 | 0.0171 | 0.0623 | 0.0205 | 0.0205 | R      | \n",
       "| 0.0762 | 0.0666 | 0.0481 | 0.0394 | 0.0590 | R      | \n",
       "| 0.0286 | 0.0453 | 0.0277 | 0.0174 | 0.0384 | R      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  V1     V2     V3     V4     V5     Class\n",
       "1 0.0200 0.0371 0.0428 0.0207 0.0954 R    \n",
       "2 0.0453 0.0523 0.0843 0.0689 0.1183 R    \n",
       "3 0.0262 0.0582 0.1099 0.1083 0.0974 R    \n",
       "4 0.0100 0.0171 0.0623 0.0205 0.0205 R    \n",
       "5 0.0762 0.0666 0.0481 0.0394 0.0590 R    \n",
       "6 0.0286 0.0453 0.0277 0.0174 0.0384 R    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(mlbench) #Machine Learning Benchmark Problem\n",
    "data(Sonar)\n",
    "Sonar[1:6,c(1:5,61)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the target is either \"R\" for rock and \"M\" for mine and most of predictors are numbers measuring some aspect of a Sonar signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2) Try a 60/40 split\n",
    "As you saw in the video, you'll be working with the `Sonar` dataset in this chapter, using a 60% training set and a 40% test set. We'll practice making a train/test split one more time, just to be sure you have the hang of it. Recall that you can use the `sample()` function to get a random permutation of the row indices in a dataset, to use when making train/test splits, e.g.:\n",
    "\n",
    "    n_obs <- nrow(my_data)\n",
    "    permuted_rows <- sample(n_obs)\n",
    "\n",
    "And then use those row indices to randomly reorder the dataset, e.g.:\n",
    "\n",
    "    my_data <- my_data[permuted_rows, ]\n",
    "\n",
    "Once your dataset is randomly ordered, you can split off the first 60% as a training set and the last 40% as a test set.\n",
    "\n",
    "**Exercise**\n",
    "1. Get the number of observations (rows) in Sonar, assigning to n_obs.\n",
    "2. Shuffle the row indices of Sonar and store the result in permuted_rows.\n",
    "3. Use permuted_rows to randomly reorder the rows of Sonar, saving as Sonar_shuffled.\n",
    "4. Identify the proper row to split on for a 60/40 split. Store this row number as split.\n",
    "5. Save the first 60% of Sonar_shuffled as a training set.\n",
    "6. Save the last 40% of Sonar_shuffled as the test set.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of observations\n",
    "n_obs<-nrow(Sonar)\n",
    "\n",
    "# Shuffle row indices: permuted_rows\n",
    "permuted_rows<-sample(n_obs)\n",
    "\n",
    "# Randomly order data: Sonar\n",
    "Sonar_shuffled<-Sonar[permuted_rows,]\n",
    "\n",
    "# Identify row to split on: split\n",
    "split <- round(n_obs * .60)\n",
    "\n",
    "# Create train\n",
    "train<-Sonar_shuffled[1:split,]\n",
    "\n",
    "# Create test\n",
    "test<-Sonar_shuffled[(split+1):nrow(Sonar_shuffled),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3) Fit a logistic regression model\n",
    "Once you have your random training and test sets you can fit a logistic regression model to your training set using the `glm()` function. `glm(`) is a more advanced version of `lm()` that allows for more varied types of regression models, aside from plain vanilla ordinary least squares regression.\n",
    "\n",
    "Be sure to pass the argument `family = \"binomial\"` to `glm()` to specify that you want to do logistic (rather than linear) regression. For example:\n",
    "\n",
    "    glm(Target ~ ., family = \"binomial\", dataset)\n",
    "    \n",
    "Don't worry about warnings like `glm.fit: algorithm did not converge` or `glm.fit: fitted probabilities numerically 0 or 1 occurred`. These are common on smaller datasets and usually don't cause any issues. They typically mean your dataset is perfectly separable, which can cause problems for the math behind the model, but R's `glm()` function is almost always robust enough to handle this case with no problems.\n",
    "\n",
    "Once you have a `glm()` model fit to your dataset, you can predict the outcome (e.g. rock or mine) on the `test` set using the `predict()` function with the argument `type = \"response\"`:\n",
    "\n",
    "    predict(my_model, test, type = \"response\")  \n",
    "\n",
    "**Exercise**\n",
    "\n",
    "1. Fit a logistic regression called model to predict Class using all other variables as predictors. Use the training set for Sonar.\n",
    "2. Predict on the test set using that model. Call the result p like you've done before.    \n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    }
   ],
   "source": [
    "# Fit glm model: model\n",
    "model<-glm(Class ~ .,family = \"binomial\", data = train)\n",
    "\n",
    "# Predict on test: p\n",
    "p<-predict(model, test, type = \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 (video) Confusion Matrix \n",
    "\n",
    "A really useful tool for evaluating binary classification models is known as a \"confusion matrix\" this is a matrix of the model´s predicted classes vs the actual outcomes in reality , it´s called confusion matrix because it reveals how \"confused\" the model is between 2 classes. \n",
    "\n",
    "The columns are the true classes while the rows of matrix are the predicted classes, the main diagonal of the matrix is where the model is correct \n",
    "\n",
    "| Prediction/Reference | Yes (Positive) | No (Negative)  |\n",
    "|----------------------|----------------|----------------|\n",
    "| Yes  (Positive)      | True Positive  | False Positive |\n",
    "| No   (Negative)      | False Negative | True Negative  |\n",
    "\n",
    "To generate a confusion matrix, we start by fitting a model to our training set, next we predict on the test set, and cut the predicted probabilities with a threshold to get assignments, in other words de logistic regresion model outputs the probability that an object is a mine, but we need to use these probabilities to make a binary decision: rock or mine in the simple case, we use a probability of 50% as our cutoff and assign anything under 50% as a rock otherwise is a mine, next we make a 2-way frequency table using a `table` function in R and we´ll use the `confusionMatrix` function in `caret` package to calculate our error rate.\n",
    "\n",
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n",
       "0.0000000 0.0000000 0.0000001 0.3711606 0.9998758 1.0000000 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "p_class\n",
       " M  R \n",
       "32 51 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "       \n",
       "p_class  M  R\n",
       "      M 11 21\n",
       "      R 30 21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction  M  R\n",
       "         M 11 21\n",
       "         R 30 21\n",
       "                                          \n",
       "               Accuracy : 0.3855          \n",
       "                 95% CI : (0.2807, 0.4988)\n",
       "    No Information Rate : 0.506           \n",
       "    P-Value [Acc > NIR] : 0.9897          \n",
       "                                          \n",
       "                  Kappa : -0.2323         \n",
       "                                          \n",
       " Mcnemar's Test P-Value : 0.2626          \n",
       "                                          \n",
       "            Sensitivity : 0.2683          \n",
       "            Specificity : 0.5000          \n",
       "         Pos Pred Value : 0.3438          \n",
       "         Neg Pred Value : 0.4118          \n",
       "             Prevalence : 0.4940          \n",
       "         Detection Rate : 0.1325          \n",
       "   Detection Prevalence : 0.3855          \n",
       "      Balanced Accuracy : 0.3841          \n",
       "                                          \n",
       "       'Positive' Class : M               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1) Fit Model\n",
    "model<-glm(Class ~ ., family = binomial(link = \"logit\"), train )\n",
    "p<- predict(model, test, type = \"response\")\n",
    "summary(p)\n",
    "\n",
    "#2 Turn probabilities into classes and look at their frequencies \n",
    "p_class<- ifelse(p>.5,\"M\",\"R\")\n",
    "table(p_class)\n",
    "\n",
    "#3 Make a 2-way frequency table\n",
    "table(p_class,test[[\"Class\"]])\n",
    "\n",
    "\n",
    "p_class<-as.factor(p_class)\n",
    "\n",
    "\n",
    "#use caret´s helper function to calculate additional statistics\n",
    "confusionMatrix(p_class, test[[\"Class\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1) Calculate a confusion matrix\n",
    "As you saw in the video, a confusion matrix is a very useful tool for calibrating the output of a model and examining all possible outcomes of your predictions (true positive, true negative, false positive, false negative).\n",
    "\n",
    "Before you make your confusion matrix, you need to \"cut\" your predicted probabilities at a given threshold to turn probabilities into a factor of class predictions. Combine `ifelse()` with `factor()` as follows:\n",
    "\n",
    "    pos_or_neg <- ifelse(probability_prediction > threshold, positive_class, negative_class)\n",
    "    p_class <- factor(pos_or_neg, levels = levels(test_values))\n",
    "    \n",
    "`confusionMatrix()` in `caret` improves on `table()` from base R by adding lots of useful ancillary statistics in addition to the base rates in the table. You can calculate the confusion matrix (and the associated statistics) using the predicted outcomes as well as the actual outcomes, e.g.:    \n",
    "\n",
    "    confusionMatrix(p_class, test_values)\n",
    "\n",
    "\n",
    "**Exercise**\n",
    "1. Use ifelse() to create a character vector, m_or_r that is the positive class, \"M\", when p is greater than 0.5, and the negative class, \"R\", otherwise.\n",
    "2. Convert m_or_r to be a factor, p_class, with levels the same as those of test[[\"Class\"]].\n",
    "3. Make a confusion matrix with confusionMatrix(), passing p_class and the \"Class\" column from the test dataset.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction  M  R\n",
       "         M 11 21\n",
       "         R 30 21\n",
       "                                          \n",
       "               Accuracy : 0.3855          \n",
       "                 95% CI : (0.2807, 0.4988)\n",
       "    No Information Rate : 0.506           \n",
       "    P-Value [Acc > NIR] : 0.9897          \n",
       "                                          \n",
       "                  Kappa : -0.2323         \n",
       "                                          \n",
       " Mcnemar's Test P-Value : 0.2626          \n",
       "                                          \n",
       "            Sensitivity : 0.2683          \n",
       "            Specificity : 0.5000          \n",
       "         Pos Pred Value : 0.3438          \n",
       "         Neg Pred Value : 0.4118          \n",
       "             Prevalence : 0.4940          \n",
       "         Detection Rate : 0.1325          \n",
       "   Detection Prevalence : 0.3855          \n",
       "      Balanced Accuracy : 0.3841          \n",
       "                                          \n",
       "       'Positive' Class : M               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1) Fit Model\n",
    "# Fit glm model: model\n",
    "model<-glm(Class ~ .,family = \"binomial\", data = train)\n",
    "# Predict on test: p\n",
    "p<-predict(model, test, type = \"response\")\n",
    "\n",
    "# If p exceeds threshold of 0.5, M else R: m_or_r\n",
    "m_or_r<-ifelse(p > .5,\"M\",\"R\")\n",
    "\n",
    "# Convert to factor: p_class\n",
    "p_class<-factor(m_or_r, levels = levels(test$Class))\n",
    "\n",
    "# Create confusion matrix\n",
    "confusionMatrix(p_class,test$Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it´s importante to see:\n",
    "\n",
    "The `accuracy` of this model that tell us how well this model predict, What is the test set true positive rate (or `sensitivity` or Recall or true positive rate) of this model and What is the test set true negative rate (or `specificity`) of this model.\n",
    "\n",
    "Link1: https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102 (Understanding Logistic Regression)\n",
    "Lin2: https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62 (Understanding Confusion Matrix)\n",
    "\n",
    "\n",
    "| Prediction/Reference | Yes (Positive) | No (Negative)  |\n",
    "|----------------------|----------------|----------------|\n",
    "| Yes  (Positive)      | True Positive  | False Positive |\n",
    "| No   (Negative)      | False Negative | True Negative  |\n",
    "\n",
    "where \n",
    "\n",
    "1. **Accuracy** = `TP+TN/Total`  (Out of all the classes, how much we predicted correctly)\n",
    "2. Precision (Positive predictive value) = `TP / TP + FP`  (Out of all the positive classes we have predicted correctly, how many are actually positive)\n",
    "3. **Sensitivity** (Recall or True positive rate) = `TP/ TP + FN` or `TP/Positive (P)` (Out of all the positive classes, how much we predicted correctly. It should be high as possible.)\n",
    "4. Specificity (True negative rate) = `TN/ TN + FP` or `TN/Negative (N)`\n",
    "\n",
    "and F-measure = `2*Recall*Precision / Recall + Precision`\n",
    "\n",
    "\n",
    "### 2.3)  (video) Class probabilities and predictions\n",
    "in the previous exercise we worked through an example confusion matrix using 50% as classification cutoff threshold, however we´re not limited to using this threshold we can use 10% or 90 % but choosing a threshold is an exercise in balancing the true positive rate (or percent of mines we catch) with false positive rate (or percen of non-mines we incorrectly flag as mines) but unfortunately there is not a metodology to choose one, you usually have to use a confusion matrix on your test set to find a good threshold.\n",
    "\n",
    "#### 2.3.1) Probabilities and classes\n",
    "What's the relationship between the predicted probabilities and the predicted classes?\n",
    "Predicted classes are based off of predicted probabilities plus a classification threshol\n",
    "\n",
    "#### 2.3.2) Try another threshold\n",
    "In the previous exercises, you used a threshold of 0.50 to cut your predicted probabilities to make class predictions (rock vs mine). However, this classification threshold does not always align with the goals for a given modeling problem.\n",
    "\n",
    "For example, pretend you want to identify the objects you are really certain are mines. In this case, you might want to use a probability threshold of 0.90 to get fewer predicted mines, but with greater confidence in each prediction.\n",
    "\n",
    "The code pattern for cutting probabilities into predicted classes, then calculating a confusion matrix, was shown in Exercise 7 of this chapter.\n",
    "\n",
    "**Exercise**\n",
    "1. Use `ifelse()` to create a character vector, `m_or_r` that is the positive class, `\"M\"`, when p is greater than 0.9, and the negative class, `\"R\"`, otherwise.\n",
    "2. Convert `m_or_r` to be a factor, `p_class`, with levels the same as those of `test[[\"Class\"]]`.\n",
    "3. Make a confusion matrix with `confusionMatrix()`, passing `p_class` and the `\"Class\"` column from the test dataset.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction  M  R\n",
       "         M  8 21\n",
       "         R 33 21\n",
       "                                         \n",
       "               Accuracy : 0.3494         \n",
       "                 95% CI : (0.248, 0.4619)\n",
       "    No Information Rate : 0.506          \n",
       "    P-Value [Acc > NIR] : 0.9986         \n",
       "                                         \n",
       "                  Kappa : -0.3059        \n",
       "                                         \n",
       " Mcnemar's Test P-Value : 0.1344         \n",
       "                                         \n",
       "            Sensitivity : 0.19512        \n",
       "            Specificity : 0.50000        \n",
       "         Pos Pred Value : 0.27586        \n",
       "         Neg Pred Value : 0.38889        \n",
       "             Prevalence : 0.49398        \n",
       "         Detection Rate : 0.09639        \n",
       "   Detection Prevalence : 0.34940        \n",
       "      Balanced Accuracy : 0.34756        \n",
       "                                         \n",
       "       'Positive' Class : M              \n",
       "                                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If p exceeds threshold of 0.9, M else R: m_or_r\n",
    "m_or_r<-ifelse(p>.9,\"M\",\"R\")\n",
    "\n",
    "# Convert to factor: p_class\n",
    "p_class<-factor(m_or_r, levels = levels(Sonar$Class))\n",
    "\n",
    "# Create confusion matrix\n",
    "confusionMatrix(p_class, test$Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3) From probabilites to confusion matrix\n",
    "Conversely, say you want to be really certain that your model correctly identifies all the mines as mines. In this case, you might use a prediction threshold of 0.10, instead of 0.90.\n",
    "\n",
    "The code pattern for cutting probabilities into predicted classes, then calculating a confusion matrix, was shown in Exercise 7 of this chapter.\n",
    "\n",
    "**Exercise**\n",
    "1. Use ifelse() to create a character vector, m_or_r that is the positive class, \"M\", when p is greater than 0.1, and the negative class, \"R\", otherwise.\n",
    "2. Convert m_or_r to be a factor, p_class, with levels the same as those of test[[\"Class\"]].\n",
    "3. Make a confusion matrix with confusionMatrix(), passing p_class and the \"Class\" column from the test dataset.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction  M  R\n",
       "         M 11 21\n",
       "         R 30 21\n",
       "                                          \n",
       "               Accuracy : 0.3855          \n",
       "                 95% CI : (0.2807, 0.4988)\n",
       "    No Information Rate : 0.506           \n",
       "    P-Value [Acc > NIR] : 0.9897          \n",
       "                                          \n",
       "                  Kappa : -0.2323         \n",
       "                                          \n",
       " Mcnemar's Test P-Value : 0.2626          \n",
       "                                          \n",
       "            Sensitivity : 0.2683          \n",
       "            Specificity : 0.5000          \n",
       "         Pos Pred Value : 0.3438          \n",
       "         Neg Pred Value : 0.4118          \n",
       "             Prevalence : 0.4940          \n",
       "         Detection Rate : 0.1325          \n",
       "   Detection Prevalence : 0.3855          \n",
       "      Balanced Accuracy : 0.3841          \n",
       "                                          \n",
       "       'Positive' Class : M               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(repr)\n",
    "# Change plot size to 4 x 3\n",
    "    options(repr.plot.width=4, repr.plot.height=3)\n",
    "\n",
    "# If p exceeds threshold of 0.1, M else R: m_or_r\n",
    "m_or_r<- ifelse(p>.1,\"M\",\"R\")\n",
    "\n",
    "# Convert to factor: p_class\n",
    "p_class<-factor(m_or_r, levels = levels(test$Class))\n",
    "\n",
    "# Create confusion matrix\n",
    "confusionMatrix(p_class, test$Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) (video) Introducing the ROC curve\n",
    "\n",
    "link: https://www.youtube.com/watch?v=4jRBRDbJemM (Youtube/ ROC and AUC, Clearly Explained!)\n",
    "\n",
    "Manually evaluating classification thresholds is hard work! in order to do this correctly, we´d have to manually dozens or hundreds of confusion matrices, and then visually inspect them unitil we find one we like, this seem un-scientific, as it requiries a lot manually work is heuristic-based  and could easly overlook a particular important threshold we need a systematic approach to evaluating classification threslholds. \n",
    "\n",
    "One common approach to do this problem is to let the computer iteratively evaluate every possible classification threshold and then calculate the true-positive rate and false-positive rate for each of them, we can then plot the true positive/false positive rate at every possible threshold and visualize the trade-off between the 2 extreme models (predict all mines vs predicts all rocks) or 100% true positive rate vs 0 % false positive rate the resulting curve is called a ROC curve or Receiver Operating Characteristic curve e.g\n",
    "\n",
    "Note: in the graph the X axis is the false positive rate, the Y axis is the true positive rate \n",
    "\n",
    "**Remember**\n",
    "\n",
    "| Prediction/Reference | Yes (Positive) | No (Negative)  |\n",
    "|----------------------|----------------|----------------|\n",
    "| Yes  (Positive)      | True Positive  | False Positive |\n",
    "| No   (Negative)      | False Negative | True Negative  |\n",
    "\n",
    "1. **Accuracy** = `TP+TN/Total`  (Out of all the classes, how much we predicted correctly)\n",
    "2. Precision (Positive predictive value) = `TP / TP + FP`  (Out of all the positive classes we have predicted correctly, how many are actually positive)\n",
    "3. **Sensitivity** (Recall or True Positive Rate or \"TPR\" ) = `TP/ TP + FN` or `TP/Positive (P)` (Out of all the positive classes, how much we predicted correctly. It should be high as possible.)\n",
    "4. **Specificity** (True Negative Rate) = `TN/ TN + FP` or `TN/Negative (N)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'caTools' was built under R version 3.5.3\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>M vs. R</th><td>0.6567944</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "\tM vs. R & 0.6567944\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| M vs. R | 0.6567944 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "        [,1]     \n",
       "M vs. R 0.6567944"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAOVBMVEUAAABAQEBNTU1mZmZo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\ndujlOuBlenUGPYNGB5hkigAjFwFGrv8BM42QLEupvXsAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"ROC Curves\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create a ROC curve\n",
    "library(caTools)\n",
    "colAUC(p, test$Class, plotROC = TRUE)\n",
    "\n",
    "#Note False Negative Rate = 1-Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1) Plot an ROC curve\n",
    "As you saw in the video, an ROC curve is a really useful shortcut for summarizing the performance of a classifier over all possible thresholds. This saves you a lot of tedious work computing class predictions for many different thresholds and examining the confusion matrix for each.\n",
    "\n",
    "My favorite package for computing ROC curves is `caTools`, which contains a function called `colAUC()`. This function is very user-friendly and can actually calculate ROC curves for multiple predictors at once. In this case, you only need to calculate the ROC curve for one predictor, e.g.\n",
    "\n",
    "    colAUC(predicted_probabilities, actual, plotROC = TRUE)\n",
    "\n",
    "The function will return a score called AUC (more on that later) and the `plotROC = TRUE` argument will return the plot of the ROC curve for visual inspection.\n",
    "\n",
    "**Exercise**\n",
    "`model`, `test`, and `train` from the last exercise using the sonar data are loaded in your workspace.\n",
    "\n",
    "1. Predict probabilities (i.e. type = \"response\") on the test set, then store the result as p.\n",
    "2. Make an ROC curve using the predicted test set probabilities.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>M vs. R</th><td>0.6567944</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "\tM vs. R & 0.6567944\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| M vs. R | 0.6567944 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "        [,1]     \n",
       "M vs. R 0.6567944"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAOVBMVEUAAABAQEBNTU1mZmZo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\ndujlOuBlenUGPYNGB5hkigAjFwFGrv8BM42QLEupvXsAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"ROC Curves\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict on test: p\n",
    "p<-predict(model, test, type = \"response\")\n",
    "\n",
    "# Make ROC curve\n",
    "colAUC(p, test$Class, plotROC = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 (video) Area under the curve (AUC)\n",
    "\n",
    "Just looking at a ROC curves starts to give us a good idea of how to evaluate whether or not our predict model is any good one interesting observation is that models with random predictions tend to produce curves that closely follow the diagonal line. On the other hand, models with a classification threshold that allows for perfect separation of classes produce a \"box\" with a single point at (1,0) to represent a model where it is possible to archive a 100% true positive rate and 0% false positive rate, wouldn´t that be nice? continuing with this example, if we calculate the area under each of these 2 ROC curves, an interesting property emerges: the area under the curve for a perfect model is exactly 1, as our plot represents a 1 by 1square, and the average area under the curve for a random model is point-5 as our plot represents a diagonal line. We can use this insight to formalize a measure of model accurancy know as \"AUC\" OR \"Area Under the Curve\".\n",
    "\n",
    "1. Single-number summary of model accuracy \n",
    "2. summarizes performance across all threshold \n",
    "3. Rank different models within the same dataset.\n",
    "\n",
    "It ranges from 0 to 1, where point-5 is the UAC of a random model and 1-point-0 is the UAC of a perfect model and 0 model always wrong , as a very rough rule of thumb, UAC can be thought of as letter grade , where point-9 is an \"A\", point-8 is an \"B\" and so on. I´m generally happy with a model that has an UAC of point-8 or higher, and models in the point-7 range are often useful.\n",
    "\n",
    "#### 2.5.1 ) Customizing trainControl\n",
    "As you saw in the video, area under the ROC curve is a very useful, single-number summary of a model's ability to discriminate the positive from the negative class (e.g. mines from rocks). An AUC of 0.5 is no better than random guessing, an AUC of 1.0 is a perfectly predictive model, and an AUC of 0.0 is perfectly anti-predictive (which rarely happens).\n",
    "\n",
    "This is often a much more useful metric than simply ranking models by their accuracy at a set threshold, as different models might require different calibration steps (looking at a confusion matrix at each step) to find the optimal classification threshold for that model.\n",
    "\n",
    "You can use the `trainControl()` function in `caret` to use AUC (instead of acccuracy), to tune the parameters of your models. The `twoClassSummary()` convenience function allows you to do this easily.\n",
    "\n",
    "When using `twoClassSummary()`, be sure to always include the argument `classProbs = TRUE` or your model will throw an error! (You cannot calculate AUC with just class predictions. You need to have class probabilities as well.)\n",
    "\n",
    "**Exercise**\n",
    "1. Customize the trainControl object to use twoClassSummary rather than defaultSummary.\n",
    "2. Use 10-fold cross-validation.\n",
    "3. Be sure to tell trainControl() to return class probabilities.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainControl object: myControl\n",
    "myControl <- trainControl(\n",
    "  method = \"cv\", #cross validation \n",
    "  number = 10,\n",
    "  summaryFunction = twoClassSummary, #Before# defaultSummary,\n",
    "  classProbs = TRUE, # IMPORTANT!\n",
    "  verboseIter = TRUE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2) Using custom trainControl\n",
    "Now that you have a custom `trainControl` object, it's easy to fit `caret` models that use AUC rather than accuracy to tune and evaluate the model. You can just pass your custom `trainControl` object to the `train()` function via the `trControl` argument, e.g.:\n",
    "\n",
    "    train(<standard arguments here>, trControl = myControl)\n",
    "    \n",
    "This syntax gives you a convenient way to store a lot of custom modeling parameters and then use them across multiple different calls to `train()`. You will make extensive use of this trick in Chapter 5.\n",
    "\n",
    "**Exercise**\n",
    "1. Use `train()` to predict `Class` from all other variables in the Sonar data (that is, Class ~ .). It should be a `glm` model (that is, set method to \"glm\") using your custom `trainControl` object, `myControl`. Save the result to model.\n",
    "2. Print the model to the console and examine its output.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x, y, weights = w, ...):\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold01: parameter=none \n",
      "+ Fold02: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold02: parameter=none \n",
      "+ Fold03: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold03: parameter=none \n",
      "+ Fold04: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold04: parameter=none \n",
      "+ Fold05: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold05: parameter=none \n",
      "+ Fold06: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold06: parameter=none \n",
      "+ Fold07: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold07: parameter=none \n",
      "+ Fold08: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold08: parameter=none \n",
      "+ Fold09: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold09: parameter=none \n",
      "+ Fold10: parameter=none \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold10: parameter=none \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "208 samples\n",
       " 60 predictor\n",
       "  2 classes: 'M', 'R' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 188, 186, 187, 187, 187, 187, ... \n",
       "Resampling results:\n",
       "\n",
       "  ROC        Sens       Spec     \n",
       "  0.7397096  0.7280303  0.6822222\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train glm with custom trainControl: model\n",
    "model<- train(\n",
    "Class~.,\n",
    "Sonar,\n",
    "method = \"glm\",\n",
    "trControl = myControl\n",
    ")\n",
    "\n",
    "\n",
    "# Print model to console\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Great work! Note that fitting a glm with caret often produces warnings about convergence or probabilities. These warnings can almost always be safely ignored, as you can use the glm's predictions to validate whether the model is accurate enough for your task.\n",
    "\n",
    "## 3) Tuning model parameters to improve performance\n",
    "In this chapter, you will use the `train()` function to tweak model parameters through cross-validation and grid search.\n",
    "\n",
    "### 3.1) (video) Random forests and wine\n",
    "Now that we have explored simple, linear model for classification and regression, lets move on  to something more interesting, random forest are a popular type of machine learning model, they are very useful especially for beginners because they´re quite robust againtst over-fitting.\n",
    "\n",
    "Random forest typically yield very accurante , non-linear models with no extra work on the part of the data scientist, this make them very useful on many real-word problems.\n",
    "\n",
    "The drawback to random forest is that, unlike linear models, the have \"hyperparameters\" to tune , unlike regular parameters, for instance slope or incercept in a linear model. Hyperparameters cannot be directly estimated from the training data they must be manually specified by the data scientist as inputs to the predict model, however theese hyperparameters can impact how the model fits he data, and the optimal values for theese parameters vary dataset to dataset. In practice the default value of the hyperparameters for random forest are often fine, but ocasionally need adjustment fortunately, we have the caret package to help us. \n",
    "\n",
    "Random forest starts with a simple decision tree model that is very fast but usually not very acurrante, RF (randome forest) improve the accuracy of a single model by fitting many decision trees, each fit to a different boostrap sample of the original dataset. This called boostrap aggregation or baggin , and is a well-know techinique for improving the performance of predictive models. \n",
    "\n",
    "Random forests take bagging one step further by randomly re-sampling the columns of the dataset at each split this additional level of sampling often helps yield even more accurate models.\n",
    "\n",
    "Let´s fit a random forest using caret\n",
    "\n",
    "after that we fit a model using the train function, and pass the \"ranger\" argument to fit a random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "208 samples\n",
       " 60 predictor\n",
       "  2 classes: 'M', 'R' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Bootstrapped (25 reps) \n",
       "Summary of sample sizes: 208, 208, 208, 208, 208, 208, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  splitrule   Accuracy   Kappa    \n",
       "   2    gini        0.8205321  0.6373345\n",
       "   2    extratrees  0.8334256  0.6646559\n",
       "  31    gini        0.8011446  0.5990418\n",
       "  31    extratrees  0.8385782  0.6743135\n",
       "  60    gini        0.7791183  0.5549046\n",
       "  60    extratrees  0.8331836  0.6637987\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 1\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were mtry = 31, splitrule = extratrees\n",
       " and min.node.size = 1."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAOVBMVEUAAAAAgP9NTU1oaGh8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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#laod some data\n",
    "library(caret)\n",
    "library(mlbench)\n",
    "\n",
    "data(Sonar)\n",
    "set.seed(42)\n",
    "\n",
    "model<-train(Class ~ ., data = Sonar, method = \"ranger\")\n",
    "model\n",
    "plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1) Random forests vs. linear models\n",
    "What's the primary advantage of random forests over linear models?\n",
    "\n",
    "A random forest is a more flexible model than a linear model, but just as easy to fit, Random forests are very powerful non-linear models, but are also very easy to fit.\n",
    "\n",
    "#### Fit a random forest\n",
    "As you saw in the video, random forest models are much more flexible than linear models, and can model complicated nonlinear effects as well as automatically capture interactions between variables. They tend to give very good results on real world data, so let's try one out on the wine quality dataset, where the goal is to predict the human-evaluated quality of a batch of wine, given some of the machine-measured chemical and physical properties of that batch.\n",
    "\n",
    "Fitting a random forest model is exactly the same as fitting a generalized linear regression model, as you did in the previous chapter. You simply change the `method` argument in the `train` function to be `\"ranger\"`. The `ranger` package is a rewrite of R's classic `randomForest` package and fits models much faster, but gives almost exactly the same results. We suggest that all beginners use the `ranger` package for random forest modeling.\n",
    "\n",
    "**Exercise**\n",
    "1. Train a random forest called model on the wine quality dataset, wine, such that quality is the response variable and all other variables are explanatory variables.\n",
    "    - Use method = \"ranger\".\n",
    "    - Use a tuneLength of 1.\n",
    "    - Use 5 CV folds.\n",
    "2. Print model to the console.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1: mtry=3, min.node.size=5, splitrule=variance \n",
      "- Fold1: mtry=3, min.node.size=5, splitrule=variance \n",
      "+ Fold1: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "- Fold1: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "+ Fold2: mtry=3, min.node.size=5, splitrule=variance \n",
      "- Fold2: mtry=3, min.node.size=5, splitrule=variance \n",
      "+ Fold2: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "- Fold2: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "+ Fold3: mtry=3, min.node.size=5, splitrule=variance \n",
      "- Fold3: mtry=3, min.node.size=5, splitrule=variance \n",
      "+ Fold3: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "- Fold3: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "+ Fold4: mtry=3, min.node.size=5, splitrule=variance \n",
      "- Fold4: mtry=3, min.node.size=5, splitrule=variance \n",
      "+ Fold4: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "- Fold4: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "+ Fold5: mtry=3, min.node.size=5, splitrule=variance \n",
      "- Fold5: mtry=3, min.node.size=5, splitrule=variance \n",
      "+ Fold5: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "- Fold5: mtry=3, min.node.size=5, splitrule=extratrees \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting mtry = 3, splitrule = variance, min.node.size = 5 on full training set\n"
     ]
    }
   ],
   "source": [
    "# Fit random forest: model\n",
    "model <- train(\n",
    "  quality ~ .,\n",
    "  tuneLength = 1,\n",
    "  data = wine, \n",
    "  method = \"ranger\",\n",
    "  trControl = trainControl(\n",
    "    method = \"cv\", \n",
    "    number = 5, \n",
    "    verboseIter = TRUE\n",
    "  )\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) (video) Explore a winder model space\n",
    "\n",
    "One of the big difference between a random forest and linear regression model  we've been exploring up to now is that random forest require \"tuning\" in other word random forests have `hyperameters` that control how the model is fit unlike `parameters` of a model (for example the split points in the random forest or coefficients in linear regresion, hyperamaters must be selected by hand before fitting the mode.\n",
    "\n",
    "The most importat of these hyperamaters is the `mtry` or number of randomly selected variables used at each split point in the individual desicion tress that make up the random forest.This number is tunable: you could look as a few as 2 or as many 100 variables per split forest that used 2 variables would tend to be more random forest that used 100 variables  would tend to be less random, unfortunately, due to their nature, it´s hard to know the best value of these hyperameters without trying them out on your trainig data for somedata set 2 variable random foresst  are best and other data set 100 variable random forest are best, one again `caret` save us a lot boring  manually work, automating the process of hyperamaters selection.\n",
    "\n",
    "To start we can play with the `tuneLenght` argument to the train function this argument is used to tell train to explore more models along its default tuning grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAOVBMVEUAAAAAgP9NTU1oaGh8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\nBq7/Wzitf6ruMR8AAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#first we loand the Sonar data from mlbench package \n",
    "library(caret)\n",
    "library(mlbench)\n",
    "data(Sonar)\n",
    "\n",
    "#this will take longer than the default model, which uses a tunelenght of 3\n",
    "model<-train(\n",
    "    Class ~ .,\n",
    "    data = Sonar,\n",
    "    method = \"ranger\", #this uses the ranger package in R to fit a random forest wich is much faster than the more widely know randomForest package \n",
    "    tuneLength = 10\n",
    ")\n",
    "\n",
    "\n",
    "#finally we can plot to visually inspect the model´s accurancy for different values of mtry \n",
    "plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1) Advantage of a longer tune length\n",
    "What's the advantage of a longer tuneLength?\n",
    "You explore more potential models and can potentially find a better model.\n",
    "\n",
    "Note: Number of variables available for splitting at each tree node. In the random forests literature, this is referred to as the mtry parameter. ... randomForest - For classification models, the default is the square root of the number of predictor variables (rounded down). For regression models, it is the number of predictor variables divided by 3 (rounded down).\n",
    "\n",
    "#### 3.2.2) Try a longer tune length\n",
    "Recall from the video that random forest models have a primary tuning parameter of `mtry`, which controls how many variables are exposed to the splitting search routine at each split . For example, suppose that a tree has a total of 10 splits and mtry = 2. This means that there are 10 samples of 2 predictors each time a split is evaluated.\n",
    "\n",
    "Use a larger tuning grid this time, but stick to the defaults provided by the `train()` function. Try a `tuneLength` of 3, rather than 1, to explore some more potential models, and `plot` the resulting model using the plot function.\n",
    "\n",
    "**Exercise**\n",
    "- Train a random forest model, model, using the wine dataset on the quality variable with all other variables as explanatory variables. (This will take a few seconds to run, so be patient!)\n",
    "- Use method = \"ranger\".\n",
    "- Change the tuneLength to 3.\n",
    "- Use 5 CV folds.\n",
    "- Print model to the console.\n",
    "- Plot the model after fitting it.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1: mtry= 2, min.node.size=5, splitrule=variance \n",
      "- Fold1: mtry= 2, min.node.size=5, splitrule=variance \n",
      "+ Fold1: mtry= 7, min.node.size=5, splitrule=variance \n",
      "- Fold1: mtry= 7, min.node.size=5, splitrule=variance \n",
      "+ Fold1: mtry=12, min.node.size=5, splitrule=variance \n",
      "- Fold1: mtry=12, min.node.size=5, splitrule=variance \n",
      "+ Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "- Fold1: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "+ Fold1: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "- Fold1: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "+ Fold1: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "- Fold1: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "+ Fold2: mtry= 2, min.node.size=5, splitrule=variance \n",
      "- Fold2: mtry= 2, min.node.size=5, splitrule=variance \n",
      "+ Fold2: mtry= 7, min.node.size=5, splitrule=variance \n",
      "- Fold2: mtry= 7, min.node.size=5, splitrule=variance \n",
      "+ Fold2: mtry=12, min.node.size=5, splitrule=variance \n",
      "- Fold2: mtry=12, min.node.size=5, splitrule=variance \n",
      "+ Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "- Fold2: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "+ Fold2: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "- Fold2: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "+ Fold2: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "- Fold2: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "+ Fold3: mtry= 2, min.node.size=5, splitrule=variance \n",
      "- Fold3: mtry= 2, min.node.size=5, splitrule=variance \n",
      "+ Fold3: mtry= 7, min.node.size=5, splitrule=variance \n",
      "- Fold3: mtry= 7, min.node.size=5, splitrule=variance \n",
      "+ Fold3: mtry=12, min.node.size=5, splitrule=variance \n",
      "- Fold3: mtry=12, min.node.size=5, splitrule=variance \n",
      "+ Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "- Fold3: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "+ Fold3: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "- Fold3: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "+ Fold3: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "- Fold3: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "+ Fold4: mtry= 2, min.node.size=5, splitrule=variance \n",
      "- Fold4: mtry= 2, min.node.size=5, splitrule=variance \n",
      "+ Fold4: mtry= 7, min.node.size=5, splitrule=variance \n",
      "- Fold4: mtry= 7, min.node.size=5, splitrule=variance \n",
      "+ Fold4: mtry=12, min.node.size=5, splitrule=variance \n",
      "- Fold4: mtry=12, min.node.size=5, splitrule=variance \n",
      "+ Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "- Fold4: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "+ Fold4: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "- Fold4: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "+ Fold4: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "- Fold4: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "+ Fold5: mtry= 2, min.node.size=5, splitrule=variance \n",
      "- Fold5: mtry= 2, min.node.size=5, splitrule=variance \n",
      "+ Fold5: mtry= 7, min.node.size=5, splitrule=variance \n",
      "- Fold5: mtry= 7, min.node.size=5, splitrule=variance \n",
      "+ Fold5: mtry=12, min.node.size=5, splitrule=variance \n",
      "- Fold5: mtry=12, min.node.size=5, splitrule=variance \n",
      "+ Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "- Fold5: mtry= 2, min.node.size=5, splitrule=extratrees \n",
      "+ Fold5: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "- Fold5: mtry= 7, min.node.size=5, splitrule=extratrees \n",
      "+ Fold5: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "- Fold5: mtry=12, min.node.size=5, splitrule=extratrees \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting mtry = 7, splitrule = variance, min.node.size = 5 on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "100 samples\n",
       " 12 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold) \n",
       "Summary of sample sizes: 79, 80, 80, 80, 81 \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  splitrule   RMSE       Rsquared   MAE      \n",
       "   2    variance    0.6573597  0.3022398  0.5021202\n",
       "   2    extratrees  0.6852534  0.2378773  0.5096205\n",
       "   7    variance    0.6543511  0.2848738  0.4983836\n",
       "   7    extratrees  0.6655256  0.2691106  0.5040878\n",
       "  12    variance    0.6573930  0.2873497  0.5047146\n",
       "  12    extratrees  0.6695405  0.2510089  0.5094095\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were mtry = 7, splitrule = variance\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAOVBMVEUAAAAAgP9NTU1oaGh8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\ngQsBAxcCBi4EDFwIGLj+A7KCtFrHQKkFAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit random forest: model\n",
    "model <- train(\n",
    "  quality ~ . ,\n",
    "  tuneLength = 3,\n",
    "  data = wine, \n",
    "  method = \"ranger\",\n",
    "  trControl = trainControl(\n",
    "    method = \"cv\", \n",
    "    number = 5, \n",
    "    verboseIter = TRUE\n",
    "  )\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model\n",
    "\n",
    "# Plot model\n",
    "plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) (video) Custom tuning grids\n",
    "in the last video we learned how use tuneLength argument to customize caret model, however we´are not limited to the defaults train chooses for us, we can pass our own, fully-customized grid as data frames to the `tuneGrid` argument in `train` function . it give us complete control over the models that are explored during grid search the major drawback of this method is that it requires the most knowledge of the how the model works.\n",
    "\n",
    "let´s make a custom tuning grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error: The tuning parameter grid should have columns mtry, splitrule, min.node.size\n",
     "output_type": "error",
     "traceback": [
      "Error: The tuning parameter grid should have columns mtry, splitrule, min.node.size\nTraceback:\n",
      "1. train(Class ~ ., data = Sonar, method = \"ranger\", tuneGrid = myGrid)",
      "2. train.formula(Class ~ ., data = Sonar, method = \"ranger\", tuneGrid = myGrid)",
      "3. train(x, y, weights = w, ...)",
      "4. train.default(x, y, weights = w, ...)",
      "5. stop(paste(\"The tuning parameter grid should have columns\", paste(tuneNames, \n .     collapse = \", \", sep = \"\")), call. = FALSE)"
     ]
    }
   ],
   "source": [
    "#first we loand the Sonar data from mlbench package \n",
    "library(caret)\n",
    "library(mlbench)\n",
    "data(Sonar)\n",
    "\n",
    "\n",
    "# first we need to make a dataframe\n",
    "myGrid<- data.frame(mtry = c(2, 3, 4, 5, 10, 20))\n",
    "\n",
    "#this example only receives an mtry parameter because the random forest only needs this\n",
    "set.seed(42)\n",
    "\n",
    "model<-train(\n",
    "    Class ~ .,\n",
    "    data = Sonar,\n",
    "    method = \"ranger\", #this uses the ranger package in R to fit a random forest wich is much faster than the more widely know randomForest package \n",
    "    tuneGrid = myGrid\n",
    ")\n",
    "\n",
    "#plot the model\n",
    "plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1) Advantages of a custom tuning grid\n",
    "Why use a custom tuneGrid?\n",
    "It gives you more fine-grained control over the tuning parameters that are explored.\n",
    "\n",
    "#### 3.3.2) Fit a random forest with custom tuning\n",
    "Now that you've explored the default tuning grids provided by the `train()` function, let's customize your models a bit more.\n",
    "\n",
    "You can provide any number of values for `mtry`, from 2 up to the number of columns in the dataset. In practice, there are diminishing returns for much larger values of mtry, so you will use a custom tuning grid that explores 2 simple models (mtry = 2 and mtry = 3) as well as one more complicated model (mtry = 7).\n",
    "\n",
    "**Exercise**\n",
    "1. Define a custom tuning grid.\n",
    " - Set the number of variables to possibly split at each node, .mtry, to a vector of 2, 3, and 7.\n",
    " - Set the rule to split on, .splitrule, to \"variance\".\n",
    " - Set the minimum node size, .min.node.size, to 5.\n",
    "\n",
    "2. Train another random forest model, model, using the wine dataset on the quality variable with all other variables as explanatory variables.\n",
    " - Use method = \"ranger\".\n",
    " - Use the custom tuneGrid.\n",
    " - Use 5 CV folds.\n",
    " - Print model to the console.\n",
    " - Plot the model after fitting it using plot().\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1: mtry=2, splitrule=variance, min.node.size=5 \n",
      "- Fold1: mtry=2, splitrule=variance, min.node.size=5 \n",
      "+ Fold1: mtry=3, splitrule=variance, min.node.size=5 \n",
      "- Fold1: mtry=3, splitrule=variance, min.node.size=5 \n",
      "+ Fold1: mtry=7, splitrule=variance, min.node.size=5 \n",
      "- Fold1: mtry=7, splitrule=variance, min.node.size=5 \n",
      "+ Fold2: mtry=2, splitrule=variance, min.node.size=5 \n",
      "- Fold2: mtry=2, splitrule=variance, min.node.size=5 \n",
      "+ Fold2: mtry=3, splitrule=variance, min.node.size=5 \n",
      "- Fold2: mtry=3, splitrule=variance, min.node.size=5 \n",
      "+ Fold2: mtry=7, splitrule=variance, min.node.size=5 \n",
      "- Fold2: mtry=7, splitrule=variance, min.node.size=5 \n",
      "+ Fold3: mtry=2, splitrule=variance, min.node.size=5 \n",
      "- Fold3: mtry=2, splitrule=variance, min.node.size=5 \n",
      "+ Fold3: mtry=3, splitrule=variance, min.node.size=5 \n",
      "- Fold3: mtry=3, splitrule=variance, min.node.size=5 \n",
      "+ Fold3: mtry=7, splitrule=variance, min.node.size=5 \n",
      "- Fold3: mtry=7, splitrule=variance, min.node.size=5 \n",
      "+ Fold4: mtry=2, splitrule=variance, min.node.size=5 \n",
      "- Fold4: mtry=2, splitrule=variance, min.node.size=5 \n",
      "+ Fold4: mtry=3, splitrule=variance, min.node.size=5 \n",
      "- Fold4: mtry=3, splitrule=variance, min.node.size=5 \n",
      "+ Fold4: mtry=7, splitrule=variance, min.node.size=5 \n",
      "- Fold4: mtry=7, splitrule=variance, min.node.size=5 \n",
      "+ Fold5: mtry=2, splitrule=variance, min.node.size=5 \n",
      "- Fold5: mtry=2, splitrule=variance, min.node.size=5 \n",
      "+ Fold5: mtry=3, splitrule=variance, min.node.size=5 \n",
      "- Fold5: mtry=3, splitrule=variance, min.node.size=5 \n",
      "+ Fold5: mtry=7, splitrule=variance, min.node.size=5 \n",
      "- Fold5: mtry=7, splitrule=variance, min.node.size=5 \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting mtry = 7, splitrule = variance, min.node.size = 5 on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "100 samples\n",
       " 12 predictor\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (5 fold) \n",
       "Summary of sample sizes: 81, 79, 80, 80, 80 \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  RMSE       Rsquared   MAE      \n",
       "  2     0.6466191  0.3362868  0.4788620\n",
       "  3     0.6412933  0.3376880  0.4730280\n",
       "  7     0.6337477  0.3427215  0.4692297\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of variance\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final values used for the model were mtry = 7, splitrule = variance\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAANlBMVEUAAAAAgP9NTU1oaGh8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\nAyYuBkxcDJi4GDBx/Qfc1YuU/VfgDAAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1 From previous step\n",
    "tuneGrid <- data.frame(\n",
    "  .mtry = c(2, 3, 7),\n",
    "  .splitrule = \"variance\",\n",
    "  .min.node.size = 5\n",
    ")\n",
    "\n",
    "#2 Fit random forest: model\n",
    "model <- train(\n",
    "  quality ~ .,\n",
    "  tuneGrid = tuneGrid,\n",
    "  data = wine, \n",
    "  method = \"ranger\",\n",
    "  trControl = trainControl(\n",
    "    method = \"cv\", \n",
    "    number = 5, \n",
    "    verboseIter = TRUE\n",
    "  )\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model\n",
    "\n",
    "# Plot model\n",
    "plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4) (video) Introducing glmnet\n",
    "Now we´ll see one of the best predict models called `glmnet` models are an extension of  generalized linear model (or the `glm` function in R), however, they have built-in variables selection that useful on many real-world datasets in particular, it helps in linear regresion model better handle collinearity (or correlation among predictos in a model) and also help prevent them in being over-condifedent in results derived from smal samples sizes, there are two pimary forms in glmnet models:\n",
    "\n",
    "- lasso regression,which penalizes the number of number of non-zeros coefficients.\n",
    "- ridge regression, which penalizes the absolute magnitud of the coefficients (large coefficients).\n",
    "\n",
    "These penalties are calculated during the model fit, and are used by the optimizer to adjutst the linea regression coefficients in other words a `glmnet` attempts to find a parsimonious model, with either few non-zeros coefficients or small absolute magnitude coefficients, that  best fit the input dataset. this is an extremely useful model, and pairs particulary well with random forests models, as it yield different results.\n",
    "\n",
    "`glmnet` models are a combination of 2 types of models: lasso regression and ridge regression, this gives `glmnet` models many parameters to tune:\n",
    "\n",
    " - the alpha paramter ranges from 0 to , where 0 is pure ridge regression and 1 is pure lasso regression, and any value between is a a mix of the two\n",
    " - lambda on the other hand, ranges from 0 to positive infinity, and controls the size of the penalty, higher values of lambda will yield simplier models and high enough values of lambda will yield intercept-only models that just predict the mean of the response variable in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>y</th><th scope=col>X1</th><th scope=col>X2</th><th scope=col>X3</th><th scope=col>X4</th><th scope=col>X5</th><th scope=col>X6</th><th scope=col>X7</th><th scope=col>X8</th><th scope=col>X9</th><th scope=col>...</th><th scope=col>X191</th><th scope=col>X192</th><th scope=col>X193</th><th scope=col>X194</th><th scope=col>X195</th><th scope=col>X196</th><th scope=col>X197</th><th scope=col>X198</th><th scope=col>X199</th><th scope=col>X200</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>class2      </td><td>0.9148060   </td><td>0.33423133  </td><td>0.1365052   </td><td>0.24492099  </td><td>0.84829322  </td><td>0.73592037  </td><td>0.05391100  </td><td>0.1651787   </td><td>0.9899656   </td><td>...         </td><td>0.5406514   </td><td>0.7222597   </td><td>0.002576062 </td><td>0.25223405  </td><td>0.5568160   </td><td>0.08803946  </td><td>0.7399642   </td><td>0.3738620   </td><td>0.6715633   </td><td>0.0001012264</td></tr>\n",
       "\t<tr><td>class2      </td><td>0.9370754   </td><td>0.18843433  </td><td>0.1771364   </td><td>0.08763591  </td><td>0.06274633  </td><td>0.75178575  </td><td>0.95509577  </td><td>0.7277811   </td><td>0.4384936   </td><td>...         </td><td>0.1411782   </td><td>0.1310911   </td><td>0.411775569 </td><td>0.52042257  </td><td>0.4986688   </td><td>0.67626884  </td><td>0.9724303   </td><td>0.8393751   </td><td>0.7468173   </td><td>0.8653565315</td></tr>\n",
       "\t<tr><td>class2      </td><td>0.2861395   </td><td>0.26971618  </td><td>0.5195605   </td><td>0.39110850  </td><td>0.81984509  </td><td>0.33261448  </td><td>0.02560094  </td><td>0.2061579   </td><td>0.6999032   </td><td>...         </td><td>0.2106741   </td><td>0.5895710   </td><td>0.891389510 </td><td>0.77904746  </td><td>0.3912340   </td><td>0.09020876  </td><td>0.6599458   </td><td>0.8371015   </td><td>0.9785843   </td><td>0.3703812116</td></tr>\n",
       "\t<tr><td>class1      </td><td>0.8304476   </td><td>0.53074408  </td><td>0.8111208   </td><td>0.18256143  </td><td>0.53936029  </td><td>0.05754862  </td><td>0.92076314  </td><td>0.5864655   </td><td>0.8890770   </td><td>...         </td><td>0.8211162   </td><td>0.5913061   </td><td>0.767070756 </td><td>0.08815411  </td><td>0.8240591   </td><td>0.41487972  </td><td>0.3337511   </td><td>0.5536796   </td><td>0.7041028   </td><td>0.6651317959</td></tr>\n",
       "\t<tr><td>class2      </td><td>0.6417455   </td><td>0.02145023  </td><td>0.1153620   </td><td>0.13362478  </td><td>0.49902010  </td><td>0.67441545  </td><td>0.36666474  </td><td>0.9135460   </td><td>0.8341595   </td><td>...         </td><td>0.8930576   </td><td>0.9879969   </td><td>0.598619170 </td><td>0.60512811  </td><td>0.5665962   </td><td>0.01701286  </td><td>0.2368524   </td><td>0.7016140   </td><td>0.7000800   </td><td>0.3351982590</td></tr>\n",
       "\t<tr><td>class2      </td><td>0.5190959   </td><td>0.79876031  </td><td>0.8934218   </td><td>0.25746291  </td><td>0.02222732  </td><td>0.04157017  </td><td>0.69839255  </td><td>0.2069666   </td><td>0.7344215   </td><td>...         </td><td>0.1698084   </td><td>0.1546299   </td><td>0.127565006 </td><td>0.08847341  </td><td>0.6644925   </td><td>0.96472272  </td><td>0.3598008   </td><td>0.9455406   </td><td>0.2009973   </td><td>0.2989820768</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       " y & X1 & X2 & X3 & X4 & X5 & X6 & X7 & X8 & X9 & ... & X191 & X192 & X193 & X194 & X195 & X196 & X197 & X198 & X199 & X200\\\\\n",
       "\\hline\n",
       "\t class2       & 0.9148060    & 0.33423133   & 0.1365052    & 0.24492099   & 0.84829322   & 0.73592037   & 0.05391100   & 0.1651787    & 0.9899656    & ...          & 0.5406514    & 0.7222597    & 0.002576062  & 0.25223405   & 0.5568160    & 0.08803946   & 0.7399642    & 0.3738620    & 0.6715633    & 0.0001012264\\\\\n",
       "\t class2       & 0.9370754    & 0.18843433   & 0.1771364    & 0.08763591   & 0.06274633   & 0.75178575   & 0.95509577   & 0.7277811    & 0.4384936    & ...          & 0.1411782    & 0.1310911    & 0.411775569  & 0.52042257   & 0.4986688    & 0.67626884   & 0.9724303    & 0.8393751    & 0.7468173    & 0.8653565315\\\\\n",
       "\t class2       & 0.2861395    & 0.26971618   & 0.5195605    & 0.39110850   & 0.81984509   & 0.33261448   & 0.02560094   & 0.2061579    & 0.6999032    & ...          & 0.2106741    & 0.5895710    & 0.891389510  & 0.77904746   & 0.3912340    & 0.09020876   & 0.6599458    & 0.8371015    & 0.9785843    & 0.3703812116\\\\\n",
       "\t class1       & 0.8304476    & 0.53074408   & 0.8111208    & 0.18256143   & 0.53936029   & 0.05754862   & 0.92076314   & 0.5864655    & 0.8890770    & ...          & 0.8211162    & 0.5913061    & 0.767070756  & 0.08815411   & 0.8240591    & 0.41487972   & 0.3337511    & 0.5536796    & 0.7041028    & 0.6651317959\\\\\n",
       "\t class2       & 0.6417455    & 0.02145023   & 0.1153620    & 0.13362478   & 0.49902010   & 0.67441545   & 0.36666474   & 0.9135460    & 0.8341595    & ...          & 0.8930576    & 0.9879969    & 0.598619170  & 0.60512811   & 0.5665962    & 0.01701286   & 0.2368524    & 0.7016140    & 0.7000800    & 0.3351982590\\\\\n",
       "\t class2       & 0.5190959    & 0.79876031   & 0.8934218    & 0.25746291   & 0.02222732   & 0.04157017   & 0.69839255   & 0.2069666    & 0.7344215    & ...          & 0.1698084    & 0.1546299    & 0.127565006  & 0.08847341   & 0.6644925    & 0.96472272   & 0.3598008    & 0.9455406    & 0.2009973    & 0.2989820768\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "y | X1 | X2 | X3 | X4 | X5 | X6 | X7 | X8 | X9 | ... | X191 | X192 | X193 | X194 | X195 | X196 | X197 | X198 | X199 | X200 | \n",
       "|---|---|---|---|---|---|\n",
       "| class2       | 0.9148060    | 0.33423133   | 0.1365052    | 0.24492099   | 0.84829322   | 0.73592037   | 0.05391100   | 0.1651787    | 0.9899656    | ...          | 0.5406514    | 0.7222597    | 0.002576062  | 0.25223405   | 0.5568160    | 0.08803946   | 0.7399642    | 0.3738620    | 0.6715633    | 0.0001012264 | \n",
       "| class2       | 0.9370754    | 0.18843433   | 0.1771364    | 0.08763591   | 0.06274633   | 0.75178575   | 0.95509577   | 0.7277811    | 0.4384936    | ...          | 0.1411782    | 0.1310911    | 0.411775569  | 0.52042257   | 0.4986688    | 0.67626884   | 0.9724303    | 0.8393751    | 0.7468173    | 0.8653565315 | \n",
       "| class2       | 0.2861395    | 0.26971618   | 0.5195605    | 0.39110850   | 0.81984509   | 0.33261448   | 0.02560094   | 0.2061579    | 0.6999032    | ...          | 0.2106741    | 0.5895710    | 0.891389510  | 0.77904746   | 0.3912340    | 0.09020876   | 0.6599458    | 0.8371015    | 0.9785843    | 0.3703812116 | \n",
       "| class1       | 0.8304476    | 0.53074408   | 0.8111208    | 0.18256143   | 0.53936029   | 0.05754862   | 0.92076314   | 0.5864655    | 0.8890770    | ...          | 0.8211162    | 0.5913061    | 0.767070756  | 0.08815411   | 0.8240591    | 0.41487972   | 0.3337511    | 0.5536796    | 0.7041028    | 0.6651317959 | \n",
       "| class2       | 0.6417455    | 0.02145023   | 0.1153620    | 0.13362478   | 0.49902010   | 0.67441545   | 0.36666474   | 0.9135460    | 0.8341595    | ...          | 0.8930576    | 0.9879969    | 0.598619170  | 0.60512811   | 0.5665962    | 0.01701286   | 0.2368524    | 0.7016140    | 0.7000800    | 0.3351982590 | \n",
       "| class2       | 0.5190959    | 0.79876031   | 0.8934218    | 0.25746291   | 0.02222732   | 0.04157017   | 0.69839255   | 0.2069666    | 0.7344215    | ...          | 0.1698084    | 0.1546299    | 0.127565006  | 0.08847341   | 0.6644925    | 0.96472272   | 0.3598008    | 0.9455406    | 0.2009973    | 0.2989820768 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  y      X1        X2         X3        X4         X5         X6        \n",
       "1 class2 0.9148060 0.33423133 0.1365052 0.24492099 0.84829322 0.73592037\n",
       "2 class2 0.9370754 0.18843433 0.1771364 0.08763591 0.06274633 0.75178575\n",
       "3 class2 0.2861395 0.26971618 0.5195605 0.39110850 0.81984509 0.33261448\n",
       "4 class1 0.8304476 0.53074408 0.8111208 0.18256143 0.53936029 0.05754862\n",
       "5 class2 0.6417455 0.02145023 0.1153620 0.13362478 0.49902010 0.67441545\n",
       "6 class2 0.5190959 0.79876031 0.8934218 0.25746291 0.02222732 0.04157017\n",
       "  X7         X8        X9        ... X191      X192      X193        X194      \n",
       "1 0.05391100 0.1651787 0.9899656 ... 0.5406514 0.7222597 0.002576062 0.25223405\n",
       "2 0.95509577 0.7277811 0.4384936 ... 0.1411782 0.1310911 0.411775569 0.52042257\n",
       "3 0.02560094 0.2061579 0.6999032 ... 0.2106741 0.5895710 0.891389510 0.77904746\n",
       "4 0.92076314 0.5864655 0.8890770 ... 0.8211162 0.5913061 0.767070756 0.08815411\n",
       "5 0.36666474 0.9135460 0.8341595 ... 0.8930576 0.9879969 0.598619170 0.60512811\n",
       "6 0.69839255 0.2069666 0.7344215 ... 0.1698084 0.1546299 0.127565006 0.08847341\n",
       "  X195      X196       X197      X198      X199      X200        \n",
       "1 0.5568160 0.08803946 0.7399642 0.3738620 0.6715633 0.0001012264\n",
       "2 0.4986688 0.67626884 0.9724303 0.8393751 0.7468173 0.8653565315\n",
       "3 0.3912340 0.09020876 0.6599458 0.8371015 0.9785843 0.3703812116\n",
       "4 0.8240591 0.41487972 0.3337511 0.5536796 0.7041028 0.6651317959\n",
       "5 0.5665962 0.01701286 0.2368524 0.7016140 0.7000800 0.3351982590\n",
       "6 0.6644925 0.96472272 0.3598008 0.9455406 0.2009973 0.2989820768"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x, y, weights = w, ...):\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: alpha=0.10, lambda=0.01013 \n",
      "- Fold01: alpha=0.10, lambda=0.01013 \n",
      "+ Fold01: alpha=0.55, lambda=0.01013 \n",
      "- Fold01: alpha=0.55, lambda=0.01013 \n",
      "+ Fold01: alpha=1.00, lambda=0.01013 \n",
      "- Fold01: alpha=1.00, lambda=0.01013 \n",
      "+ Fold02: alpha=0.10, lambda=0.01013 \n",
      "- Fold02: alpha=0.10, lambda=0.01013 \n",
      "+ Fold02: alpha=0.55, lambda=0.01013 \n",
      "- Fold02: alpha=0.55, lambda=0.01013 \n",
      "+ Fold02: alpha=1.00, lambda=0.01013 \n",
      "- Fold02: alpha=1.00, lambda=0.01013 \n",
      "+ Fold03: alpha=0.10, lambda=0.01013 \n",
      "- Fold03: alpha=0.10, lambda=0.01013 \n",
      "+ Fold03: alpha=0.55, lambda=0.01013 \n",
      "- Fold03: alpha=0.55, lambda=0.01013 \n",
      "+ Fold03: alpha=1.00, lambda=0.01013 \n",
      "- Fold03: alpha=1.00, lambda=0.01013 \n",
      "+ Fold04: alpha=0.10, lambda=0.01013 \n",
      "- Fold04: alpha=0.10, lambda=0.01013 \n",
      "+ Fold04: alpha=0.55, lambda=0.01013 \n",
      "- Fold04: alpha=0.55, lambda=0.01013 \n",
      "+ Fold04: alpha=1.00, lambda=0.01013 \n",
      "- Fold04: alpha=1.00, lambda=0.01013 \n",
      "+ Fold05: alpha=0.10, lambda=0.01013 \n",
      "- Fold05: alpha=0.10, lambda=0.01013 \n",
      "+ Fold05: alpha=0.55, lambda=0.01013 \n",
      "- Fold05: alpha=0.55, lambda=0.01013 \n",
      "+ Fold05: alpha=1.00, lambda=0.01013 \n",
      "- Fold05: alpha=1.00, lambda=0.01013 \n",
      "+ Fold06: alpha=0.10, lambda=0.01013 \n",
      "- Fold06: alpha=0.10, lambda=0.01013 \n",
      "+ Fold06: alpha=0.55, lambda=0.01013 \n",
      "- Fold06: alpha=0.55, lambda=0.01013 \n",
      "+ Fold06: alpha=1.00, lambda=0.01013 \n",
      "- Fold06: alpha=1.00, lambda=0.01013 \n",
      "+ Fold07: alpha=0.10, lambda=0.01013 \n",
      "- Fold07: alpha=0.10, lambda=0.01013 \n",
      "+ Fold07: alpha=0.55, lambda=0.01013 \n",
      "- Fold07: alpha=0.55, lambda=0.01013 \n",
      "+ Fold07: alpha=1.00, lambda=0.01013 \n",
      "- Fold07: alpha=1.00, lambda=0.01013 \n",
      "+ Fold08: alpha=0.10, lambda=0.01013 \n",
      "- Fold08: alpha=0.10, lambda=0.01013 \n",
      "+ Fold08: alpha=0.55, lambda=0.01013 \n",
      "- Fold08: alpha=0.55, lambda=0.01013 \n",
      "+ Fold08: alpha=1.00, lambda=0.01013 \n",
      "- Fold08: alpha=1.00, lambda=0.01013 \n",
      "+ Fold09: alpha=0.10, lambda=0.01013 \n",
      "- Fold09: alpha=0.10, lambda=0.01013 \n",
      "+ Fold09: alpha=0.55, lambda=0.01013 \n",
      "- Fold09: alpha=0.55, lambda=0.01013 \n",
      "+ Fold09: alpha=1.00, lambda=0.01013 \n",
      "- Fold09: alpha=1.00, lambda=0.01013 \n",
      "+ Fold10: alpha=0.10, lambda=0.01013 \n",
      "- Fold10: alpha=0.10, lambda=0.01013 \n",
      "+ Fold10: alpha=0.55, lambda=0.01013 \n",
      "- Fold10: alpha=0.55, lambda=0.01013 \n",
      "+ Fold10: alpha=1.00, lambda=0.01013 \n",
      "- Fold10: alpha=1.00, lambda=0.01013 \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting alpha = 0.1, lambda = 0.0101 on full training set\n"
     ]
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAPFBMVEUAAAAAZAAAgP9NTU1o\naGh8fHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHm5ubp6enw8PD/AP////+nD2O2AAAA\nCXBIWXMAABJ0AAASdAHeZh94AAAXnElEQVR4nO2dh5arIBCGubHEmLImvv+7XmlKMypFRzL/\nOWsMyjjLF4oISHpU1iJHO4BKKwScuRBw5kLAmQsBZy4EnLkQcOZCwJkLAWcuBJy5EHDmQsCZ\nCwFnLgScuRBw5kLAmQsBZy4EnLkQcOZCwJkLAWcuBJy5EHDmQsCZCwFnLgScuRBw5kLAmQsw\nYMJVPdedvD500OPrUc2B4tqtcWBOj5DIEQQfMCFrCG8DXJJvR00HigDC5dEJfPT1v0ikf0Oq\n9SdvM73qrHdFmi22fa6UUEdf/4tk2qxKo3SA+zcpttj2uVJCHX39LzIAtyUpWh7SFEOmosFk\nLGrZ3qMeytOGB71LUosjXMph8Z3HHuyWLY/T1aS4zThg2dZC+huL2RCR26Wz4kqK9zL2XoIP\nWBTRNW9x0d2K7l1twDdOkiOsyfgjEICnwyrgarQ74KK7N9MBloNt21oI+/KoZMDorARcK1ep\nSUiZv1WgAQu9hi8PUr1pdfigu8WrfxU2YELufX9nYYSerhSQ1yFxtcMy3l0Yu4s4LSkVB+i2\nq/hvwrath7RiW2jOchtaAIu9m+ADrijfIQvQZHnTwq2m6TSkmV1E9/JbL5reMrSaGmoGYGms\nknGUSnNsRb9dtq2QoYgfjY7OcnNawKrbvmgCDXjYlMVDfBmrUoHABbh73CqFoPyQfI3Ds8ZG\nB9T7YNO205y0ZDhre7+boAN+8nyxDnAlT9EBV+Ta947Dy4AVbyzbTnMIeINk6VZPX5RwB+Dr\n0Bx+dCbgrhBtGuvwFsC2bac5s75Y+NXsIPCAX7yRxetKJrMOfk6NrOGbCXjkax9W6+B6gYXT\ntm1ONarYsAL2E3jAIguz1u7QRq21VnRJWto4HQE/+5dZB4989cOyPaS1opWrGrtO20bItFWc\n5VfSAlKll1vwAb95FuY1HusWrsYKraUftQTciPCnCniq/pTDJaF3M0q9XfULgG3bVoiynZzl\nV1ICEPAomRQNr4XbIa1Eg7YpSMUK5v5WDA2osXi+0mdPSnGrA1YOP8sRcN8WY0+WetXeYGHZ\ntkLU7egsv5ISgIDXatUzCNQJAbMupHe9Z3/fiXVCwKITOOARzy/phID7dmiylJh/1+mMgFEb\nhIAzFwLOXAg4cyHgzIWAMxcCzlwIOHMdB7gpSNG83QHasZa4YoyhdEBqw0fBjY8VmJ7T/yZ2\n34qFMVqvxppClWjqCfQ5x8O0C1mHecgfoJXOAO3YSx8iY4byp3Z0XJzkK/ow38X4v4ndrpge\nOU7RXgq/KVSJpp7Afbj11iXA6igPn+JB+9MRoB3jT/bNGGPoi1zfNDtfpZ2HNFlPD+bE7pU9\nn2jouUq01zQM3TBWyzEl4wktG/56ZQ+o9Us49flsS5QUOgpwwwax3Kdh5kqAemxI0vEBuyu0\nVsbPUL0LQeM+ldVyVxlro0RrJx90YzKackLFfj2deJClXMIlivd4xEcBrtlQFiVzKAHqMTE9\noZ8LFRq/8RHIFIL8CUy7okBVnkNxwK3hGz97jKacIH8ilXEJpz7j5kgdBZgYOc8e3sg/Xuap\nRijXWz79f8mnxBXpprpb7N5EEX3TotXkcRVzjDRjYzTlBM0V5RIufbSPwzR6ePGU73VXArZO\nNUK5WjlqUWbgG7nLM5TdlrayilaPxqcNKeNDuLEpmnJCyUoRMVposuvUd8AfT81fb0bQc7B1\nqgtwJyvel2gfvaaRUsquGCpw06PxASLNWA7z0Jc61mo84UbqtxhK+VocIolFdB8J8LuQ2a8R\nObnkt03GbkuL6KEN3JrR6Bd5/yVClWjqCcU4jNM6wdJvN7IKE7ASUDhRukOpqvFuWpxzZZzF\nQMtxdyhg+RSw0oym2uOhajT1hOHnUdzoruMEW8fjPboV3Zmt6G5qRY/HtFa0GTqElJVcQ0M2\nysc+LaLu6mWAEs0RqkbTTuCXKV0nwNRR/t3ElJHGEWAcGytQVyif+Skkb2hmAPP8zVdkmKIV\nLF/zH84YqkcbT+C7LVs7AAF/1eqerBGlO7RTh0fXsotJPWPabQjtZm7oT0SJxr6/We3dGWOt\nx2jyBNYL9ixpq8u8BFQd5mE53ZzIGnIMUHb7KRWdoVc1I5VEfXhhF6/TNBUl2pv3UDemsTGa\ncoLYrR2XgKrDPORPdrgPxAhQdvspFZ2hWkk507s17Y4W1GjUrpy84gKsnNB3w2+gflh2AQu+\nh6ggIeDMhYAzFwLOXAg4cyHgzIWAMxcCzlwIOHMh4MyFgDMXAs5cCDhzIeDMhYAzFwLOXAg4\nc1mAnw0d2FI1+75ZAJVKBuB7OQ5bKY9+6x4qhjTAXUWq9sVGhz9vw37QWxlRIKQCfhBtSYWu\nIZiJTy8VcG2+sel97VEnF7aiMxcCzlwIOHNZgG+lMbwfdWqZGG+nmTaHWiUTY2GtOYM6tUzA\nmHEzk8mzJubNMOrUMgF3RXX8Y4bwUiTYAgAX4tzh2EU0gEYWgLQB4AICTmkBgAtpAIMQgLQB\n4AICTmkBgAupAN/piI767jh3NwFIGwAuJAIs37585MtbAaQNABfSAG5JwdYbO7RHC0DaAHAh\nDeBSrCX2IqV98l4CkDYAXEh1m2TuHCAAaQPAhdQ5OM4LmAkqrjYDML7HroO9foV/wZcNtgDA\nBaeFYMCLrWjzR0Scof4OUQFIXQAuJALc3+k7Cmbvg4kRRywSOXtpBBzXQgTAK05XVvlU2DpN\nIeC4FnYGTL5lXj+HqACkLgAX4gPmy95/ba/NAd5SB//7t+QUgNQF4AIAwDL3ErtmHvVn6N8/\n/ofy0uYbppAi2mhfrauD/42beQHIPgBcOLwOnrYbAP/TPmYEIHUBuJAG8JjzC2dPloZ2LCo2\nAh4qYQTsaSEa4Jn3Ltoo/Yro7w0tAKkLwIX4gB9ap6f7aZJ1V7S1o4Oi5X/zkAGkLgAXEuTg\nUuU7M3pWNt/UlvS2rsoJ7BxjAKkLwIXEdXAkLZlzMgaQugBcOKQVvVkr7NuMAaQuABcSA37W\nc0fi2NdkMAaQugBcSAS48X60vM7+nNRGF4DUBeBCGsAT3zgr7GxySDIGkLoAXEgDuCD3viJd\nV5E4c9C2OsQYA0hdAC6ka0XTN/W+Ig2M9ijoF3pBVgkBz0WggB90PNbedbCqv2DGCHguQj0U\n0R0p++ehgPvvPV3rLAQpW8APCpYNvIuzyl3QiA5/xgh4NsKNhlz5C68jKHTIjidjBOwdYQ/7\n+n/mwxgBe0fYw771n22ukBGwK0LgJIlF+6vlTJtNjBGwKwJswP0Wxgh4NkLN5iY9i0hLRUcF\n3K9mjIDnIjTj7MI4zejYgPt1jBHwXITY84MTAO5XNLoQ8FyEIvb8YJ9Iq9LmK2MEPBehIQV9\njPQoyG0mgnP66Oyl0wHuvzFGwLMR5PzgmQEd5vjJpSI9KeB+ljECno/A5wfPPO43R0CTaRrL\nMYB7d4WMgD0jGIDVWQ2HAaYyGSNgzwgrAX+ZXZhM/35izmLQ7ELv6aNbZjaske+Pf8rHmINd\nEbYC1tmuBvz5LDkVkDaCMQL2jEBcW7Pl9dU+xbuEOCxtaKMLAXtG0NDKnL6c5RV9xs28wtPm\n+EFdEAGveJpkZ9aNHR0D2stliXCUtAlj/KuA7ebUVsAD3uFvB8B9EOMsAa+L4Jw+OmvJWUSn\nrYM1C76MfxdwoP0Lb2R9Rxw3bbwG4OYPONHsQlr/fi7DxzfE8dNmM+N8AaeeXXhhG7qdR5wk\nbbYxzhZw8tmFlO3w9xVxqtTdwDhbwDvMLmRsv+bihKm7tkLOFvCeswtnESdO3V0GdUEGvN/s\nQo7YCk6fuouMswW89+xC1q1lIt4ldTXGENeBSQN4/9mFDsR7pa5kLBdnO8CFrRbCb5MOmF1o\nId4xdVmjy7ECbpaA49wYzduf1+Wi9XzsnLoiI0NbySnBw4ai6fzdWbb/VSrivQHzjJw/YLpU\nZRU5G29waEK8d+rKFXAVxlkC7rumGBg3L2+XFuwvSSLeHfDYyBoh5wl40HNoX5Gyffu5tGx/\nQRTxEamr3zIN37IF3Is3RF8jFdWbHbpcPp/jU9eqk/d3IeXjwvdtqI4Pm3x2uSyPu1xSlEF3\nB6/klPZ58OPI6aPhiKONqvSHDBnwsTmY/mehiKMOm/WDDBfw9zrYPX306yC9zaL/WRji6OOi\nt0MGCvix0Io2R1WmfPtoAOIkA9+3tbwgAn7S++Di232wOS468dtHvRGnm9mwGjJAwCt6sgyU\nRIcdwyEq5T9bGEG9woKflkYMHDVkILAv+rbUwbEScMzpo8NtMUT9O2a6atD00RWjsOxB71sH\nvq+R8dP1KKfT5mCpr5UywBw8Bc6a0QATZT8lYA/E+wBmmoN8fsB2bo7hEJXjP9uIeEfAVEmW\nCTkesFoXzMWIBZhPiAizEOzCV+3zRHnXInomJMwhqpm02YD4AMBU6V/+lBqw3aWhV8ThDlHN\nps1qxAcBppItr5MCdk4f3TLDf42+pM3nsgrxgYCZwp82zvgQ7WlSLMUGTDPxCsRHA6YWgiGn\nAdyWfd+Vs68PDra/Rt9Tdw1iCIDpJghyuoHvdGTWQa+2Y1pK3WXEUABTeY8OSQK4Ivf+Rcr+\nftyr7dak7hJiSICZfCAnAUwzMFvtHfSC4P0SYnCAqbZCTga4ppO/oQPmiOcYgwRMtQVyoiL6\n9aCjdYAX0VyfcTa5r4VgF3wsrK2UUzWyCF3s/ZAXREutTt3PHGLQgJlWQE50m1SwiYXlfaul\nlfbXaEPqziCGD5hqAfJvdnRYciI+B2CqL5ARsJAD8XkAU81A/tWeLIcsxOcCTGW3vJzQf6Mn\nyyED8fkAMymQ6QQ4B+Jf6clySEN8UsBUAvI/aiEB4NP0ZDlER/WIvo8TA6bikP/MhUP63+rJ\nssUHblHEJwdMyTLG8QGfqifLlkR8esDJiuhz9WQ5NOXiIB0POFEj62w9WQ592CIQgYgPB5zq\nNmk5wtz00Uj2qULThi8CEYQYAOBjerLMUZUpp48GKLigzhgwm/9dz5TQ5ijoxNNHvfUXijhf\nwPL9we5GtIGS9GABy2WKPRFnC7glBW0+Pwq6ZvTc6YuAY04f9ZeYeHoZ7pqyUdD0UaqS8Pn9\ntLtyETDpIefgfpqytj0bZ5uDF17ZrqE02lcAAfsjzhbwlIOdyygR5xYwYF/E2QLeUAdPdQFo\nwH6IswW8rRXdnyAHU21HnC/g/l5/uQ+2uzRgdnRYFrYizhjwUgTn20dhdVU6Lcg1IHaafwoV\ncB3pZRxz9lcpTdp8JsTLjLMFHOk5/6z9VUqVutNKLouIswVckmiLvTvtr1K61F2NOFvA77qK\nNGDWbX+VUqbuSsTZAt7c17nR/iqlTd1ViBGwp/1VSp26NuL4c1+gAo4tkIBNxPJvVxf8LCDg\ntRampRHFXdNPAO6urAf6XTo7on0EFrCWiW3CeQLuClLTzwchRaS3GMIFrHRfMsaXHwBckiu/\nC35W7uf92wUZ8IiYZ2CNcZaAH3TEuxB9E3gMwQYsENM3vFxGynu7sMlCEOCr0ovVnXPqioeF\nATHFq1XJlzwBk9kv/oIPWBbU2orjVp2c2IXVFoIAF78KmCPWCPOpESGQAQK+KhPOHrw9Haxz\nAKa5+KO9NUAa8IcMEPBrujkabph+o5HFJIvoj5RuwA8yQMB9Q4obHVT5uhWR2lgnAfwx3vrx\nUVBzbYcMEXB/G580XGcjqJXz+OUEQ3a+ynqpy994YCK9reUFEnDfNWzq2W22H0sbXjd+gT/o\nLtjASHotZJiA151+opkN8Q1YpXcqF44HPO7/FGAh2jXyFTVAwLU5HOtt1cQWSqATwNMDppKV\nspM0QMAP0qiIu8ZeiMUAPNfIgjF9dB9dlOmpI+lkVwubPtpVpGpfFPL7eRv27aaWu4j+1Rw8\nymper62oN/sQWgffy/EHUrrWUXLUtuQ362BL7ifKCxX1Zh/CG1lPdqNUNe7Bswj4q1TIpoXt\npA9vRY9fELAi2fKatbCWtPOE5IPu1NqW/FJHxzat6vP6SnoI/XMcST+qUptdmE1XZQoX1nds\nuipq9sDjCMB72M8DMN1sfUahof7rbcIIOJKFiC54jQ4RkBFwKguxXdgMGYvotBZSuLAJ8mGN\nrD3sZwqYagPkFLdJvzR15TgX1lbKOHUloYXkLqyAjFNXElrYxYUFyDh1JaGF3Vz4AhmnriS0\nsKsLM5Bx6kpCC7u7YLe8nNCDAP/q1BU4LiiQ6QQ4B+LAIvo3p67AckFAZq9viwz4Z6eugHOB\nQ/4zFw7pQ2+TfnXqCkAXLnzlkMiA10xdCbK/UkenLgQXUhTR/YqpK4H21+nw1AXgQopGVgoh\nYG/Fv01KIQQc10Io4HfLFvS/RVtTGAHHtRAI+FGINlYR6e2yCDiyhTDAD0LYgPdXQyLdBiPg\nyBaCAL+n134PqOOU0gg4roUgwA1plP2bfbKHEHBcC0GASzLd/nZBT/wJKr22A07xuNBL4ZcO\ntgDAhTi3sAgYrAvxAccrogMFIG0AuBAfcIpGlpcApA0AF+IDTnGb5CUAaQPAhfiA+zshDXse\n3BB7/ZUdBSBtALiQADDNuEJH8oWQNgBcSAG4f9/ow4Yq3sMGLwFIGwAuJAEMQwDSBoALCDil\nBQAuJAb8ijNsFnWwNMDPaqh/aTN6wHtkTxYqnlSMT96AfvUdbWlFftc76hipgCsKtSEVvVmy\nVp5FnVP28wVCClK/DnIHFVsuwGXUl7yjDpUL8EGuoFIIAWcuBJy5dMAeY35QsAUKMDFeurXd\nBTPO9kGIzvd+7WtBczsUBKR8SnrFH+2Ll4Xeo7qJ7IKXBc1tTwu6P0BElK3+xcsC2wsxEO6C\nlwXNbU8LlkMg5PhnwgCTGHg2KgJg0iPgdRaCAXvUf7oLvgXsrwD2qUGDUlcH7IPH/I0FNrIQ\n8LwF4mEhdjMAc7Aq658JKh8PaiLFaGT9CuDtrmlofe7mEXBK2U2kQAvbjSDgpNJqLO/GSUgz\n3HbBq5oIs6DF8bXgMAVAskQlYwkbYEH79DMQ1FXpb0HLuDl1VaISCAFnLgScuRBw5kLAmQsB\nZy4EnLkQcOZCwJkLAWcuBJy5EHDmQsCZCwFnLgScuRBw5kLAmQsBZy4EnLkQcOY6LeCibvn6\n9F1bF32/7X0EfERfcfV7B+ehK/Fu1WkBE/kK3Csfd+gBmEwvxN6i8lRpdipnVRFSFmynKH1G\n19Ltu/Jazu9ci1ucyllVhDSErU4/fHoC7t+k8LqyR6TDdCpnVRHyIO3w2ZK7LKIrQldwew5F\nN2FD57uaFPzVIk0x5FV1YQSifrYlKVr+9V2Smp9f8dJbOSbMyQH5j3oo4kUJoNgfIwDRiQG/\nGYuadBJwxzJkUbwFYPYqVUqYvtWcXG3APAfXDFnFgmu2CCs7v3gbx4Q5AfjGa3FGWLE/RQCi\nEwPmrZ2B0djIaof0v9H3pnLA1XsIKembKIpX/yoswB2rgx/0vKE6fogo9N0kw8fVdYyZk8uJ\n3dlbTHrNvhIBiM4MuBmKZFkg801FWpatOeCn2KtZej80wKIV/aZHKVRWHPAoQ8hT5G7rGDes\neaHZVyIA0ZkB30WGnQAPpTV7e9vEQSFiAeb3wcq6YNaJ1jEVcPe4VeodGiFaBCCC48lGsTq3\nGvJspwCWL29bBqwa8gNcjSQRcArRNCwIK0h9cnDv2ncANo5N5q6kbB+dDTjm/xhD4BxaK5qU\nQ0OIdmdNqVwPdXDVm4BddfBkqJ5aRGPenOpg45huuO/sOhhO84rr1ICHRqxsM4vvzVAptyaH\nuVY0150e7VvekGIhLW0Ks8LeOiZusHve6HpVZitaiQBEpwYsSmQJ+F2w+2BRKysZrTIrRq0k\nrcZuaRk83Qebx+i2JDRzN8LmU7c/RQCiUwMeYI4PkobNVfRkVSZg1jP1nANMO5/ItVODB3p1\n5zpGt8+SXfU63Bk/HzyvKvbHCEB0WsDblbp/CVL/1aRfAMwq6ned7E1Qqe0H6RcAi25jnydH\nIOwH6RcA9+3Q9CkT5q/U9kP0E4B/WQg4cyHgzIWAMxcCzlwIOHMh4MyFgDMXAs5cCDhzIeDM\nhYAzFwLOXAg4cyHgzIWAM9d/PgMOd7eeolgAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(overfit)\n",
    "\n",
    "#make a custome trainControl\n",
    "myControl<- trainControl(\n",
    "method = \"cv\",\n",
    "    number = 10,\n",
    "    summaryFunction = twoClassSummary,\n",
    "    classProbs = TRUE, #super important!\n",
    "    verboseIter = TRUE\n",
    ")\n",
    "\n",
    "# fit a model\n",
    "\n",
    "set.seed(42)\n",
    "model <- train (\n",
    "    y ~ .,\n",
    "    overfit,\n",
    "    method = \"glmnet\",\n",
    "    trControl = myControl\n",
    ")\n",
    "\n",
    "plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1) Advantage of glmnet\n",
    "What's the advantage of glmnet over regular glm models?\n",
    "models place constraints on your coefficients, which helps prevent overfitting\n",
    "\n",
    "#### 3.4.2) Make a custom trainControl\n",
    "The wine quality dataset was a regression problem, but now you are looking at a classification problem. This is a simulated dataset based on the \"don't overfit\" competition on Kaggle a number of years ago.\n",
    "\n",
    "Classification problems are a little more complicated than regression problems because you have to provide a custom `summaryFunction` to the `train()` function to use the `AUC` metric to rank your models. Start by making a custom `trainControl`, as you did in the previous chapter. Be sure to set `classProbs = TRUE`, otherwise the `twoClassSummary` for `summaryFunction` will break.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Make a custom trainControl called myControl for classification using the trainControl function.\n",
    "\n",
    "- Use 10 CV folds.\n",
    "- Use twoClassSummary for the summaryFunction.\n",
    "- Be sure to set classProbs = TRUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom trainControl: myControl\n",
    "myControl <- trainControl(\n",
    "  method = \"cv\", \n",
    "  number = 10,\n",
    "  summaryFunction = twoClassSummary,\n",
    "  classProbs = TRUE, # IMPORTANT!\n",
    "  verboseIter = TRUE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3) Fit glmnet with custom trainControl\n",
    "Now that you have a custom `trainControl` object, fit a `glmnet` model to the \"don't overfit\" dataset. Recall from the video that `glmnet` is an extension of the generalized linear regression model (or `glm`) that places constraints on the magnitude of the coefficients to prevent overfitting. This is more commonly known as \"penalized\" regression modeling and is a very useful technique on datasets with many predictors and few values.\n",
    "\n",
    "`glmnet` is capable of fitting two different kinds of penalized models, controlled by the alpha parameter:\n",
    "\n",
    "    Ridge regression (or alpha = 0)\n",
    "    Lasso regression (or alpha = 1)\n",
    "\n",
    "You'll now fit a `glmnet` model to the \"don't overfit\" dataset using the defaults provided by the caret package.\n",
    "\n",
    "**Exercise**\n",
    "- Train a `glmnet` model called `model` on the overfit data. Use the custom `trainControl` from the previous exercise (myControl). The variable `y` is the response variable and all other variables are explanatory variables.\n",
    "- Print the model to the console.\n",
    "- Use the `max()` function to find the maximum of the ROC statistic contained somewhere in `model[[\"results\"]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x, y, weights = w, ...):\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: alpha=0.10, lambda=0.01013 \n",
      "- Fold01: alpha=0.10, lambda=0.01013 \n",
      "+ Fold01: alpha=0.55, lambda=0.01013 \n",
      "- Fold01: alpha=0.55, lambda=0.01013 \n",
      "+ Fold01: alpha=1.00, lambda=0.01013 \n",
      "- Fold01: alpha=1.00, lambda=0.01013 \n",
      "+ Fold02: alpha=0.10, lambda=0.01013 \n",
      "- Fold02: alpha=0.10, lambda=0.01013 \n",
      "+ Fold02: alpha=0.55, lambda=0.01013 \n",
      "- Fold02: alpha=0.55, lambda=0.01013 \n",
      "+ Fold02: alpha=1.00, lambda=0.01013 \n",
      "- Fold02: alpha=1.00, lambda=0.01013 \n",
      "+ Fold03: alpha=0.10, lambda=0.01013 \n",
      "- Fold03: alpha=0.10, lambda=0.01013 \n",
      "+ Fold03: alpha=0.55, lambda=0.01013 \n",
      "- Fold03: alpha=0.55, lambda=0.01013 \n",
      "+ Fold03: alpha=1.00, lambda=0.01013 \n",
      "- Fold03: alpha=1.00, lambda=0.01013 \n",
      "+ Fold04: alpha=0.10, lambda=0.01013 \n",
      "- Fold04: alpha=0.10, lambda=0.01013 \n",
      "+ Fold04: alpha=0.55, lambda=0.01013 \n",
      "- Fold04: alpha=0.55, lambda=0.01013 \n",
      "+ Fold04: alpha=1.00, lambda=0.01013 \n",
      "- Fold04: alpha=1.00, lambda=0.01013 \n",
      "+ Fold05: alpha=0.10, lambda=0.01013 \n",
      "- Fold05: alpha=0.10, lambda=0.01013 \n",
      "+ Fold05: alpha=0.55, lambda=0.01013 \n",
      "- Fold05: alpha=0.55, lambda=0.01013 \n",
      "+ Fold05: alpha=1.00, lambda=0.01013 \n",
      "- Fold05: alpha=1.00, lambda=0.01013 \n",
      "+ Fold06: alpha=0.10, lambda=0.01013 \n",
      "- Fold06: alpha=0.10, lambda=0.01013 \n",
      "+ Fold06: alpha=0.55, lambda=0.01013 \n",
      "- Fold06: alpha=0.55, lambda=0.01013 \n",
      "+ Fold06: alpha=1.00, lambda=0.01013 \n",
      "- Fold06: alpha=1.00, lambda=0.01013 \n",
      "+ Fold07: alpha=0.10, lambda=0.01013 \n",
      "- Fold07: alpha=0.10, lambda=0.01013 \n",
      "+ Fold07: alpha=0.55, lambda=0.01013 \n",
      "- Fold07: alpha=0.55, lambda=0.01013 \n",
      "+ Fold07: alpha=1.00, lambda=0.01013 \n",
      "- Fold07: alpha=1.00, lambda=0.01013 \n",
      "+ Fold08: alpha=0.10, lambda=0.01013 \n",
      "- Fold08: alpha=0.10, lambda=0.01013 \n",
      "+ Fold08: alpha=0.55, lambda=0.01013 \n",
      "- Fold08: alpha=0.55, lambda=0.01013 \n",
      "+ Fold08: alpha=1.00, lambda=0.01013 \n",
      "- Fold08: alpha=1.00, lambda=0.01013 \n",
      "+ Fold09: alpha=0.10, lambda=0.01013 \n",
      "- Fold09: alpha=0.10, lambda=0.01013 \n",
      "+ Fold09: alpha=0.55, lambda=0.01013 \n",
      "- Fold09: alpha=0.55, lambda=0.01013 \n",
      "+ Fold09: alpha=1.00, lambda=0.01013 \n",
      "- Fold09: alpha=1.00, lambda=0.01013 \n",
      "+ Fold10: alpha=0.10, lambda=0.01013 \n",
      "- Fold10: alpha=0.10, lambda=0.01013 \n",
      "+ Fold10: alpha=0.55, lambda=0.01013 \n",
      "- Fold10: alpha=0.55, lambda=0.01013 \n",
      "+ Fold10: alpha=1.00, lambda=0.01013 \n",
      "- Fold10: alpha=1.00, lambda=0.01013 \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting alpha = 0.55, lambda = 0.0101 on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "glmnet \n",
       "\n",
       "250 samples\n",
       "200 predictors\n",
       "  2 classes: 'class1', 'class2' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 226, 225, 224, 225, 225, 225, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  alpha  lambda        ROC        Sens  Spec     \n",
       "  0.10   0.0001012745  0.4172101  0.00  0.9742754\n",
       "  0.10   0.0010127448  0.4299819  0.00  0.9786232\n",
       "  0.10   0.0101274483  0.4361413  0.00  0.9956522\n",
       "  0.55   0.0001012745  0.4148551  0.05  0.9445652\n",
       "  0.55   0.0010127448  0.4191123  0.05  0.9617754\n",
       "  0.55   0.0101274483  0.4596014  0.00  0.9873188\n",
       "  1.00   0.0001012745  0.3730072  0.05  0.9315217\n",
       "  1.00   0.0010127448  0.3663043  0.00  0.9487319\n",
       "  1.00   0.0101274483  0.4227355  0.00  0.9914855\n",
       "\n",
       "ROC was used to select the optimal model using the largest value.\n",
       "The final values used for the model were alpha = 0.55 and lambda = 0.01012745."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.459601449275362"
      ],
      "text/latex": [
       "0.459601449275362"
      ],
      "text/markdown": [
       "0.459601449275362"
      ],
      "text/plain": [
       "[1] 0.4596014"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Fit glmnet model: model\n",
    "model <- train(\n",
    "  y ~ ., \n",
    "  overfit,\n",
    "  method = \"glmnet\",\n",
    "  trControl = myControl\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model\n",
    "\n",
    "# Print maximum ROC statistic\n",
    "max(model[[\"results\"]][[\"ROC\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5) (video) glmnet with custom tuning grid\n",
    "\n",
    "\n",
    "Random forest models are relatively easy to tune since has really one parameter important `mtry` glmnet models in other hand have two tuning parameter:\n",
    "- alpha or the mixed parameter between `lasso` and `ridge` regresion\n",
    "- lambda or the strength of the penalty on coefficients.\n",
    "\n",
    "However there is trick to `glmnet` models for a single value at alpha `glmnet` fit all values in lambda simultaneously this is called submodel-trick because we can fit a number differents models simultaneously and explore results each sub-model after the fact.\n",
    "\n",
    "We can also explore this trick to get faster running great searches while still exploring final great tuning grid.\n",
    "\n",
    "with `glmnet` models are usually like to explore two values of alpha 0 and 1 with a weight range of lambda, `caret` will use this sub-model trick to colect entire tuning grid down to tune two models fits which run pretty fast even 10 folder of cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x, y, weights = w, ...):\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: alpha=0, lambda=1 \n",
      "- Fold01: alpha=0, lambda=1 \n",
      "+ Fold01: alpha=1, lambda=1 \n",
      "- Fold01: alpha=1, lambda=1 \n",
      "+ Fold02: alpha=0, lambda=1 \n",
      "- Fold02: alpha=0, lambda=1 \n",
      "+ Fold02: alpha=1, lambda=1 \n",
      "- Fold02: alpha=1, lambda=1 \n",
      "+ Fold03: alpha=0, lambda=1 \n",
      "- Fold03: alpha=0, lambda=1 \n",
      "+ Fold03: alpha=1, lambda=1 \n",
      "- Fold03: alpha=1, lambda=1 \n",
      "+ Fold04: alpha=0, lambda=1 \n",
      "- Fold04: alpha=0, lambda=1 \n",
      "+ Fold04: alpha=1, lambda=1 \n",
      "- Fold04: alpha=1, lambda=1 \n",
      "+ Fold05: alpha=0, lambda=1 \n",
      "- Fold05: alpha=0, lambda=1 \n",
      "+ Fold05: alpha=1, lambda=1 \n",
      "- Fold05: alpha=1, lambda=1 \n",
      "+ Fold06: alpha=0, lambda=1 \n",
      "- Fold06: alpha=0, lambda=1 \n",
      "+ Fold06: alpha=1, lambda=1 \n",
      "- Fold06: alpha=1, lambda=1 \n",
      "+ Fold07: alpha=0, lambda=1 \n",
      "- Fold07: alpha=0, lambda=1 \n",
      "+ Fold07: alpha=1, lambda=1 \n",
      "- Fold07: alpha=1, lambda=1 \n",
      "+ Fold08: alpha=0, lambda=1 \n",
      "- Fold08: alpha=0, lambda=1 \n",
      "+ Fold08: alpha=1, lambda=1 \n",
      "- Fold08: alpha=1, lambda=1 \n",
      "+ Fold09: alpha=0, lambda=1 \n",
      "- Fold09: alpha=0, lambda=1 \n",
      "+ Fold09: alpha=1, lambda=1 \n",
      "- Fold09: alpha=1, lambda=1 \n",
      "+ Fold10: alpha=0, lambda=1 \n",
      "- Fold10: alpha=0, lambda=1 \n",
      "+ Fold10: alpha=1, lambda=1 \n",
      "- Fold10: alpha=1, lambda=1 \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting alpha = 1, lambda = 1 on full training set\n"
     ]
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAOVBMVEUAAAAAgP9NTU1oaGh8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\n8+MwgGSQUAAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAP1BMVEUAAAAAAP8AzQAA//9N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\nhwUThwUThwUThwUThwUT53/bC01J1APGJwAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we start making a custome tuning grid \n",
    "myGrid <- expand.grid(\n",
    "alpha = 0:1,\n",
    "    lambda = seq(.0001, 1, length = 10)\n",
    ")\n",
    "\n",
    "#after that we fit our model\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "model<- train(\n",
    "y ~ .,\n",
    "    overfit,\n",
    "    method = \"glmnet\",\n",
    "    tuneGrid = myGrid,\n",
    "    trControl = myControl\n",
    ")\n",
    "\n",
    "plot(model)\n",
    "plot(model$finalModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1) Exercise\n",
    "Why use a custom tuning grid for a glmnet model?\n",
    "With a custom grid you can deeply explore machine learning models in caret.\n",
    "\n",
    "#### glmnet with custom trainControl and tuning\n",
    "As you saw in the video, the `glmnet` model actually fits many models at once (one of the great things about the package). You can exploit this by passing a large number of `lambda` values, which control the amount of penalization in the model. `train()` is smart enough to only fit one model per `alpha` value and pass all of the `lambda` values at once for simultaneous fitting.\n",
    "\n",
    "My favorite tuning grid for `glmnet` models is:\n",
    "\n",
    "    expand.grid(\n",
    "      alpha = 0:1,\n",
    "      lambda = seq(0.0001, 1, length = 100)\n",
    "    )\n",
    "    \n",
    "This grid explores a large number of `lambda` values (100, in fact), from a very small one to a very large one. (You could increase the maximum `lambda` to 10, but in this exercise 1 is a good upper bound.)\n",
    "\n",
    "If you want to explore fewer models, you can use a shorter lambda sequence. For example, `lambda = seq(0.0001, 1, length = 10)` would fit 10 models per value of alpha.\n",
    "\n",
    "You also look at the two forms of penalized models with this `tuneGrid`: `ridge` regression and `lasso` regression. `alpha = 0` is pure ridge regression, and `alpha = 1` is pure lasso regression. You can fit a mixture of the two models (i.e. an elastic net) using an alpha between 0 and 1. For example, alpha = 0.05 would be 95% ridge regression and 5% lasso regression.\n",
    "\n",
    "In this problem you'll just explore the 2 extremes – pure ridge and pure lasso regression – for the purpose of illustrating their differences.\n",
    "\n",
    "**Exercise**\n",
    "- Train a `glmnet` model on the `overfit` data such that `y` is the response variable and all other variables are explanatory variables. Make sure to use your custom `trainControl` from the previous exercise (`myControl`). Also, use a custom `tuneGrid` to explore `alpha = 0:1` and 20 values of `lambda` between 0.0001 and 1 per value of alpha.\n",
    "- Print `model` to the console.\n",
    "- Print the `max()` of the `ROC` statistic in `model[[\"results\"]]`. You can access it using `model[[\"results\"]][[\"ROC\"]]`.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x, y, weights = w, ...):\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: alpha=0, lambda=1 \n",
      "- Fold01: alpha=0, lambda=1 \n",
      "+ Fold01: alpha=1, lambda=1 \n",
      "- Fold01: alpha=1, lambda=1 \n",
      "+ Fold02: alpha=0, lambda=1 \n",
      "- Fold02: alpha=0, lambda=1 \n",
      "+ Fold02: alpha=1, lambda=1 \n",
      "- Fold02: alpha=1, lambda=1 \n",
      "+ Fold03: alpha=0, lambda=1 \n",
      "- Fold03: alpha=0, lambda=1 \n",
      "+ Fold03: alpha=1, lambda=1 \n",
      "- Fold03: alpha=1, lambda=1 \n",
      "+ Fold04: alpha=0, lambda=1 \n",
      "- Fold04: alpha=0, lambda=1 \n",
      "+ Fold04: alpha=1, lambda=1 \n",
      "- Fold04: alpha=1, lambda=1 \n",
      "+ Fold05: alpha=0, lambda=1 \n",
      "- Fold05: alpha=0, lambda=1 \n",
      "+ Fold05: alpha=1, lambda=1 \n",
      "- Fold05: alpha=1, lambda=1 \n",
      "+ Fold06: alpha=0, lambda=1 \n",
      "- Fold06: alpha=0, lambda=1 \n",
      "+ Fold06: alpha=1, lambda=1 \n",
      "- Fold06: alpha=1, lambda=1 \n",
      "+ Fold07: alpha=0, lambda=1 \n",
      "- Fold07: alpha=0, lambda=1 \n",
      "+ Fold07: alpha=1, lambda=1 \n",
      "- Fold07: alpha=1, lambda=1 \n",
      "+ Fold08: alpha=0, lambda=1 \n",
      "- Fold08: alpha=0, lambda=1 \n",
      "+ Fold08: alpha=1, lambda=1 \n",
      "- Fold08: alpha=1, lambda=1 \n",
      "+ Fold09: alpha=0, lambda=1 \n",
      "- Fold09: alpha=0, lambda=1 \n",
      "+ Fold09: alpha=1, lambda=1 \n",
      "- Fold09: alpha=1, lambda=1 \n",
      "+ Fold10: alpha=0, lambda=1 \n",
      "- Fold10: alpha=0, lambda=1 \n",
      "+ Fold10: alpha=1, lambda=1 \n",
      "- Fold10: alpha=1, lambda=1 \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting alpha = 1, lambda = 0.0527 on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "glmnet \n",
       "\n",
       "250 samples\n",
       "200 predictors\n",
       "  2 classes: 'class1', 'class2' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 225, 226, 225, 225, 226, 225, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  alpha  lambda      ROC        Sens  Spec     \n",
       "  0      0.00010000  0.4199275  0     0.9871377\n",
       "  0      0.05272632  0.4412138  0     0.9956522\n",
       "  0      0.10535263  0.4560688  0     1.0000000\n",
       "  0      0.15797895  0.4580616  0     1.0000000\n",
       "  0      0.21060526  0.4622283  0     1.0000000\n",
       "  0      0.26323158  0.4623188  0     1.0000000\n",
       "  0      0.31585789  0.4666667  0     1.0000000\n",
       "  0      0.36848421  0.4708333  0     1.0000000\n",
       "  0      0.42111053  0.4708333  0     1.0000000\n",
       "  0      0.47373684  0.4751812  0     1.0000000\n",
       "  0      0.52636316  0.4773551  0     1.0000000\n",
       "  0      0.57898947  0.4773551  0     1.0000000\n",
       "  0      0.63161579  0.4794384  0     1.0000000\n",
       "  0      0.68424211  0.4903080  0     1.0000000\n",
       "  0      0.73686842  0.4903080  0     1.0000000\n",
       "  0      0.78949474  0.4924819  0     1.0000000\n",
       "  0      0.84212105  0.4924819  0     1.0000000\n",
       "  0      0.89474737  0.4924819  0     1.0000000\n",
       "  0      0.94737368  0.4924819  0     1.0000000\n",
       "  0      1.00000000  0.4924819  0     1.0000000\n",
       "  1      0.00010000  0.4460145  0     0.9442029\n",
       "  1      0.05272632  0.5210145  0     1.0000000\n",
       "  1      0.10535263  0.5000000  0     1.0000000\n",
       "  1      0.15797895  0.5000000  0     1.0000000\n",
       "  1      0.21060526  0.5000000  0     1.0000000\n",
       "  1      0.26323158  0.5000000  0     1.0000000\n",
       "  1      0.31585789  0.5000000  0     1.0000000\n",
       "  1      0.36848421  0.5000000  0     1.0000000\n",
       "  1      0.42111053  0.5000000  0     1.0000000\n",
       "  1      0.47373684  0.5000000  0     1.0000000\n",
       "  1      0.52636316  0.5000000  0     1.0000000\n",
       "  1      0.57898947  0.5000000  0     1.0000000\n",
       "  1      0.63161579  0.5000000  0     1.0000000\n",
       "  1      0.68424211  0.5000000  0     1.0000000\n",
       "  1      0.73686842  0.5000000  0     1.0000000\n",
       "  1      0.78949474  0.5000000  0     1.0000000\n",
       "  1      0.84212105  0.5000000  0     1.0000000\n",
       "  1      0.89474737  0.5000000  0     1.0000000\n",
       "  1      0.94737368  0.5000000  0     1.0000000\n",
       "  1      1.00000000  0.5000000  0     1.0000000\n",
       "\n",
       "ROC was used to select the optimal model using the largest value.\n",
       "The final values used for the model were alpha = 1 and lambda = 0.05272632."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.521014492753623"
      ],
      "text/latex": [
       "0.521014492753623"
      ],
      "text/markdown": [
       "0.521014492753623"
      ],
      "text/plain": [
       "[1] 0.5210145"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myControl <- trainControl(\n",
    "  method = \"cv\", \n",
    "  number = 10,\n",
    "  summaryFunction = twoClassSummary,\n",
    "  classProbs = TRUE, # IMPORTANT!\n",
    "  verboseIter = TRUE\n",
    ")\n",
    "\n",
    "\n",
    "#Anwears\n",
    "# Train glmnet with custom trainControl and tuning: model\n",
    "model <- train(\n",
    "  y ~ ., \n",
    "  overfit,\n",
    "  tuneGrid = expand.grid(\n",
    "  alpha = 0:1,\n",
    "  lambda = seq(.0001, 1, length = 20)\n",
    "    ),\n",
    "  method = \"glmnet\",\n",
    "  trControl = myControl\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model\n",
    "\n",
    "# Print maximum ROC statistic\n",
    "max(model[[\"results\"]][[\"ROC\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Preprocessing your data\n",
    "In this chapter, you will practice using `train()` to preprocess data before fitting models, improving your ability to making accurate predictions.\n",
    "\n",
    "### 4.1) (video) Median imputation\n",
    "real world data have missing values this is a problem for most statistical or machine learning algorithm they usually require numbers to work and don´t know what to do with missing data a common approaches is throw out rows with missing values but in generally not a good idea , it can lead to biases in your dataset and generete over-confident models, it can also, in extrame cases lead to you throwing out all of your data, a much better strategy is to use the `median` to guess what a missing value would be, if it weren´t missing  this is very good idea if your data are `missing at random` and lets you model data that include rows with missing values.\n",
    "\n",
    "so let´s make an example: hear we´ll take our dataset called `mtcars` and we´ll changes some values with `NA` values after that we split our data set and try to fit our model, but unfortunately it´s show us an error, the simple solution to this problem i to pass `medianImpute` the `preProcess` argument for train, which tells caret to impute the missing values in X with their medians.\n",
    "\n",
    "this way it´s no necessary clean our data or reprocess the information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "32 samples\n",
       " 3 predictor\n",
       "\n",
       "Pre-processing: median imputation (3) \n",
       "Resampling: Bootstrapped (25 reps) \n",
       "Summary of sample sizes: 32, 32, 32, 32, 32, 32, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  RMSE      Rsquared   MAE     \n",
       "  2     2.629862  0.8234310  2.200327\n",
       "  3     2.615883  0.8267228  2.179490\n",
       "\n",
       "RMSE was used to select the optimal model using the smallest value.\n",
       "The final value used for the model was mtry = 3."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Generete some data with missing values \n",
    "data(mtcars)\n",
    "set.seed(42)\n",
    "mtcars2<-mtcars\n",
    "mtcars2[sample(1:nrow(mtcars2),10),\"hp\"]<-NA\n",
    "\n",
    "#split target from predictors \n",
    "Y<-mtcars2$mpg\n",
    "X<-mtcars2[,2:4]\n",
    "\n",
    "#try to fit a caret model\n",
    "library(caret)\n",
    "model<-train(X, Y, preProcess = \"medianImpute\")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1) Apply median imputation\n",
    "In this chapter, you'll be using a version of the Wisconsin Breast Cancer dataset. This dataset presents a classic binary classification problem: 50% of the samples are benign, 50% are malignant, and the challenge is to identify which are which.\n",
    "\n",
    "This dataset is interesting because many of the predictors contain missing values and most rows of the dataset have at least one missing value. This presents a modeling challenge, because most machine learning algorithms cannot handle missing values out of the box. For example, your first instinct might be to fit a logistic regression model to this data, but prior to doing this you need a strategy for handling the NAs.\n",
    "\n",
    "Fortunately, the `train()` function in `caret` contains an argument called `preProcess`, which allows you to specify that `median imputation` should be used to fill in the missing values. In previous chapters, you created models with the `train()` function using formulas such as `y ~ .`. An alternative way is to specify the `x` and `y` arguments to `train()`, where `x` is an object with samples in rows and features in columns and `y` is a numeric or factor vector containing the outcomes. Said differently, `x` is a matrix or data frame that contains the whole dataset you'd use for the data argument to the `lm()` call, for example, but excludes the response variable column; `y` is a vector that contains just the response variable column.\n",
    "\n",
    "For this exercise, the argument `x` to `train()` is loaded in your workspace as `breast_cancer_x` and `y` as `breast_cancer_y`.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "- Use the train() function to fit a glm model called median_model to the breast cancer dataset. Use preProcess = \"medianImpute\" to handle the missing values.\n",
    "- Print median_model to the console.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x = breast_cancer_x, y = breast_cancer_y, method = \"glm\", :\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: parameter=none \n",
      "- Fold01: parameter=none \n",
      "+ Fold02: parameter=none \n",
      "- Fold02: parameter=none \n",
      "+ Fold03: parameter=none \n",
      "- Fold03: parameter=none \n",
      "+ Fold04: parameter=none \n",
      "- Fold04: parameter=none \n",
      "+ Fold05: parameter=none \n",
      "- Fold05: parameter=none \n",
      "+ Fold06: parameter=none \n",
      "- Fold06: parameter=none \n",
      "+ Fold07: parameter=none \n",
      "- Fold07: parameter=none \n",
      "+ Fold08: parameter=none \n",
      "- Fold08: parameter=none \n",
      "+ Fold09: parameter=none \n",
      "- Fold09: parameter=none \n",
      "+ Fold10: parameter=none \n",
      "- Fold10: parameter=none \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "699 samples\n",
       "  9 predictor\n",
       "  2 classes: 'benign', 'malignant' \n",
       "\n",
       "Pre-processing: median imputation (9) \n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 629, 628, 630, 629, 629, 630, ... \n",
       "Resampling results:\n",
       "\n",
       "  ROC        Sens       Spec \n",
       "  0.9920564  0.9695169  0.942\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myControl <- trainControl(\n",
    "  method = \"cv\", \n",
    "  number = 10,\n",
    "  summaryFunction = twoClassSummary,\n",
    "  classProbs = TRUE, # IMPORTANT!\n",
    "  verboseIter = TRUE\n",
    ")\n",
    "\n",
    "#avalible in cancer\n",
    "#breast_cancer_x\n",
    "#breast_cancer_y\n",
    "\n",
    "\n",
    "# Apply median imputation: median_model\n",
    "median_model <- train(\n",
    "  x = breast_cancer_x, \n",
    "  y = breast_cancer_y,\n",
    "  method = \"glm\",\n",
    "  trControl = myControl,\n",
    "  preProcess = \"medianImpute\"\n",
    ")\n",
    "\n",
    "# Print median_model to console\n",
    "median_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 (video) KNN imputation\n",
    "There´are some problems with median imputation, it's very fast but it can produce incorrect results if the input data set has a systematic bias and is missing not-at-random, in other words, if there is  a pattern in data that leads to missing values, median imputation can miss this, it´s therefore useful explore other strategies for missing imputation, particulary for linear models. \n",
    "\n",
    "Tree based models such as random forest tend to be more robust to the missing-not-at random case, one useful type of missing values imputation is k-nearst neighbors (KNN impuration) this´s a strategy for imputation missing values based on other \"similar\" non-missing rows, this is a strategy  for imputation missing value based in other similar no missing row, this method tries to overcome the missing-not-at-random problem by inferring what the missing value would be, based on observations that are similar in other, non-missing variables, fortunately, the train function has a built-in method to do this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 6.717277e-15\n",
      "[1] 6.084156e-14\n"
     ]
    }
   ],
   "source": [
    "# let´s make a data set that has some missing-not-at random data\n",
    "data(mtcars)\n",
    "set.seed(42)\n",
    "\n",
    "mtcars3<-mtcars\n",
    "# we pretend that smaler cars (those with as lower displacement) don´t report ther horsepower\n",
    "mtcars3[mtcars3$disp < 140, \"hp\"]<-NA\n",
    "Y<-mtcars3$disp\n",
    "X<-mtcars3[,2:4]\n",
    "\n",
    "#use median imputation, it would be incorrect because only have information about horesepower of the medium and large cars\n",
    "#that way if I use a median imputation i will be bias my small cars\n",
    "model<-train(X, Y, method = \"glm\", preProcess = \"medianImpute\")\n",
    "print(min(model$results$RMSE))\n",
    "\n",
    "#use KNN imputation\n",
    "model<-train(X, Y, method = \"glm\", preProcess = \"knnImpute\")\n",
    "print(min(model$results$RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1) Question\n",
    "Will KNN imputation always be better than median imputation? No, you should try both options and keep the one that gives more accurate models.\n",
    "\n",
    "#### 3.2.2) Use KNN imputation\n",
    "In the previous exercise, you used `median imputation` to fill in missing values in the `breast cancer` dataset, but that is not the only possible method for dealing with missing data.\n",
    "\n",
    "An alternative to `median imputation` is `k-nearest neighbors`, or `KNN`, imputation. This is a more advanced form of imputation where missing values are replaced with values from other rows that are similar to the current row. While this is a lot more complicated to implement in practice than simple median imputation, it is very easy to explore in `caret` using the `preProcess` argument to `train()`. You can simply use `preProcess = \"knnImpute\"` to change the method of imputation used prior to model fitting.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "breast_cancer_x and breast_cancer_y are loaded in your workspace.\n",
    "\n",
    "- Use the train() function to fit a glm model called knn_model to the breast cancer dataset.\n",
    "- Use KNN imputation to handle missing values.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x = breast_cancer_x, y = breast_cancer_y, method = \"glm\", :\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: parameter=none \n",
      "- Fold01: parameter=none \n",
      "+ Fold02: parameter=none \n",
      "- Fold02: parameter=none \n",
      "+ Fold03: parameter=none \n",
      "- Fold03: parameter=none \n",
      "+ Fold04: parameter=none \n",
      "- Fold04: parameter=none \n",
      "+ Fold05: parameter=none \n",
      "- Fold05: parameter=none \n",
      "+ Fold06: parameter=none \n",
      "- Fold06: parameter=none \n",
      "+ Fold07: parameter=none \n",
      "- Fold07: parameter=none \n",
      "+ Fold08: parameter=none \n",
      "- Fold08: parameter=none \n",
      "+ Fold09: parameter=none \n",
      "- Fold09: parameter=none \n",
      "+ Fold10: parameter=none \n",
      "- Fold10: parameter=none \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "699 samples\n",
       "  9 predictor\n",
       "  2 classes: 'benign', 'malignant' \n",
       "\n",
       "Pre-processing: nearest neighbor imputation (9), centered (9), scaled (9) \n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 629, 629, 629, 629, 629, 629, ... \n",
       "Resampling results:\n",
       "\n",
       "  ROC        Sens      Spec     \n",
       "  0.9896606  0.976087  0.9378333\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply KNN imputation: knn_model\n",
    "knn_model <- train(\n",
    "  x = breast_cancer_x, \n",
    "  y = breast_cancer_y,\n",
    "  method = \"glm\",\n",
    "  trControl = myControl,\n",
    "  preProcess = \"knnImpute\"\n",
    ")\n",
    "\n",
    "# Print knn_model to console\n",
    "knn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 (video) Multiple preprocessing methods\n",
    "the preprocess argument to train can do a lot more than missing values imputation,it exposes a very wide range of pre-processing steps that can have a large impact on the results of your models. you can also chain together multiple pre-processing steps, for example you can use `median imputation`, then `center` and `scale` your data, then fit a `glm` model this is a common \"recipe\" for pre-preprocessing data prior to fitting a linear model, note that there is an \"orden of operations \" to the pre-processing steps (to see more detail you can type `?preProcess`) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 3.332758\n"
     ]
    }
   ],
   "source": [
    "# we load mtcars and type some missing at random data\n",
    "data(mtcars)\n",
    "set.seed(42)\n",
    "mtcars[sample(1:nrow(mtcars),10),\"hp\"]<-NA\n",
    "Y<-mtcars$mpg\n",
    "X<-mtcars[,2:4] #missing at random\n",
    "\n",
    "# Now let´s use our linear model recipe: median imputation, the center, scale and fit glm\n",
    "set.seed(42)\n",
    "model<-train(X, Y,\n",
    "            method = \"glm\",\n",
    "            preProcess = c(\"medianImpute\", \"center\", \"scale\")\n",
    "            )\n",
    "print(min(model$results$RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing cheat sheet\n",
    "- start with median imputation\n",
    "- try KNN imputation if data missing not at random\n",
    "- for linear model (lm, glm, glmnet) always center and scale.\n",
    "-it´s worth trying PCA and spatial sign transformation \n",
    "-tree-based models such as random forest or GBMs typically don´t need much pre-processing \n",
    "\n",
    "#### 3.3.1) Combining preprocessing methods\n",
    "The `preProcess` argument to `train()` doesn't just limit you to imputing missing values. It also includes a wide variety of other `preProcess` techniques to make your life as a data scientist much easier. You can read a full list of them by typing   `?preProcess` and reading the help page for this function.\n",
    "\n",
    "One set of preprocessing functions that is particularly useful for fitting regression models is standardization: **centering** and **scaling**. You first **center** by subtracting the mean of each column from each value in that column, then you **scale** by dividing by the standard deviation.\n",
    "\n",
    "**Standardization** transforms your data such that for each column, the mean is 0 and the standard deviation is 1. This makes it easier for regression models to find a good solution.\n",
    "\n",
    "**Exercise**\n",
    "- `breast_cancer_x` and `breast_cancer_y` are loaded in your workspace. Fit a logistic regression model using median imputation called `model` to the breast cancer data, then print it to the console.\n",
    "- Update the model to include two more pre-processing steps: centering and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x = breast_cancer_x, y = breast_cancer_y, method = \"glm\", :\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: parameter=none \n",
      "- Fold01: parameter=none \n",
      "+ Fold02: parameter=none \n",
      "- Fold02: parameter=none \n",
      "+ Fold03: parameter=none \n",
      "- Fold03: parameter=none \n",
      "+ Fold04: parameter=none \n",
      "- Fold04: parameter=none \n",
      "+ Fold05: parameter=none \n",
      "- Fold05: parameter=none \n",
      "+ Fold06: parameter=none \n",
      "- Fold06: parameter=none \n",
      "+ Fold07: parameter=none \n",
      "- Fold07: parameter=none \n",
      "+ Fold08: parameter=none \n",
      "- Fold08: parameter=none \n",
      "+ Fold09: parameter=none \n",
      "- Fold09: parameter=none \n",
      "+ Fold10: parameter=none \n",
      "- Fold10: parameter=none \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "699 samples\n",
       "  9 predictor\n",
       "  2 classes: 'benign', 'malignant' \n",
       "\n",
       "Pre-processing: median imputation (9) \n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 629, 629, 629, 629, 628, 629, ... \n",
       "Resampling results:\n",
       "\n",
       "  ROC        Sens       Spec     \n",
       "  0.9920978  0.9694686  0.9503333\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x = breast_cancer_x, y = breast_cancer_y, method = \"glm\", :\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01: parameter=none \n",
      "- Fold01: parameter=none \n",
      "+ Fold02: parameter=none \n",
      "- Fold02: parameter=none \n",
      "+ Fold03: parameter=none \n",
      "- Fold03: parameter=none \n",
      "+ Fold04: parameter=none \n",
      "- Fold04: parameter=none \n",
      "+ Fold05: parameter=none \n",
      "- Fold05: parameter=none \n",
      "+ Fold06: parameter=none \n",
      "- Fold06: parameter=none \n",
      "+ Fold07: parameter=none \n",
      "- Fold07: parameter=none \n",
      "+ Fold08: parameter=none \n",
      "- Fold08: parameter=none \n",
      "+ Fold09: parameter=none \n",
      "- Fold09: parameter=none \n",
      "+ Fold10: parameter=none \n",
      "- Fold10: parameter=none \n",
      "Aggregating results\n",
      "Fitting final model on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "699 samples\n",
       "  9 predictor\n",
       "  2 classes: 'benign', 'malignant' \n",
       "\n",
       "Pre-processing: median imputation (9), centered (9), scaled (9) \n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 629, 629, 629, 629, 629, 629, ... \n",
       "Resampling results:\n",
       "\n",
       "  ROC        Sens       Spec     \n",
       "  0.9915775  0.9672947  0.9418333\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myControl <- trainControl(\n",
    "  method = \"cv\", \n",
    "  number = 10,\n",
    "  summaryFunction = twoClassSummary,\n",
    "  classProbs = TRUE, # IMPORTANT!\n",
    "  verboseIter = TRUE\n",
    ")\n",
    "\n",
    "#avalible in cancer\n",
    "#breast_cancer_x\n",
    "#breast_cancer_y\n",
    "\n",
    "#1)\n",
    "# Fit glm with median imputation\n",
    "model <- train(\n",
    "  x = breast_cancer_x, \n",
    "  y = breast_cancer_y,\n",
    "  method = \"glm\",\n",
    "  trControl = myControl,\n",
    "  preProcess = \"medianImpute\"\n",
    ")\n",
    "\n",
    "# Print model\n",
    "model\n",
    "\n",
    "\n",
    "#2)\n",
    "# Update model with standardization\n",
    "model <- train(\n",
    "  x = breast_cancer_x, \n",
    "  y = breast_cancer_y,\n",
    "  method = \"glm\",\n",
    "  trControl = myControl,\n",
    "  preProcess = c(\"medianImpute\", \"center\", \"scale\")\n",
    ")\n",
    "\n",
    "# Print updated model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 (video) Handling low-information predictors\n",
    "in the real word the data that we´are using for predict modelsing is often messy some variables in our dataset might not contain much information for example variables that are constant or very close to constant don´t contain much useful information and it´s can sometimes be useful remove them prior to modeling, nearly constant variables are pariculary tricky, because it easy for one fold of cross-validation to end up with a constant column.\n",
    "\n",
    "cosntant columns can mess up a lot of models and should be avoided furthermore nearly constant columns cantain little information wich means that these variables tend not to have an impact on the result of your model in general we remove extremly low variance variables from my dataset prior to modeling this speeds up my model and makes the run with fewer bugs and generally doent´s have a large impact on their accurancy.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample01: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample02: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample03: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample04: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample05: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample06: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample07: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample08: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample09: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample10: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample11: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample12: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample13: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample14: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample15: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample16: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample17: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample18: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample19: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample20: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample21: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample22: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample23: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample24: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: bad\"Warning message:\n",
      "\"model fit failed for Resample25: parameter=none Error in prcomp.default(x[, method$pca, drop = FALSE], scale = TRUE, retx = FALSE) : \n",
      "  cannot rescale a constant/zero column to unit variance\n",
      "\"Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "\"There were missing values in resampled performance measures.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something is wrong; all the RMSE metric values are missing:\n",
      "      RMSE        Rsquared        MAE     \n",
      " Min.   : NA   Min.   : NA   Min.   : NA  \n",
      " 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  \n",
      " Median : NA   Median : NA   Median : NA  \n",
      " Mean   :NaN   Mean   :NaN   Mean   :NaN  \n",
      " 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  \n",
      " Max.   : NA   Max.   : NA   Max.   : NA  \n",
      " NA's   :1     NA's   :1     NA's   :1    \n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error: Stopping\n",
     "output_type": "error",
     "traceback": [
      "Error: Stopping\nTraceback:\n",
      "1. train(X, Y, method = \"glm\", preProcess = c(\"medianImpute\", \"center\", \n .     \"scale\", \"pca\"))",
      "2. train.default(X, Y, method = \"glm\", preProcess = c(\"medianImpute\", \n .     \"center\", \"scale\", \"pca\"))",
      "3. stop(\"Stopping\", call. = FALSE)"
     ]
    }
   ],
   "source": [
    "# we load mtcars and type some missing at random data\n",
    "data(mtcars)\n",
    "set.seed(42)\n",
    "mtcars[sample(1:nrow(mtcars),10),\"hp\"]<-NA\n",
    "Y<-mtcars$mpg\n",
    "X<-mtcars[,2:4] #missing at random\n",
    "\n",
    "\n",
    "# add constant-valued columns to mtcars\n",
    "X$bad<-1\n",
    "\n",
    "\n",
    "# Now let´s use our linear model recipe: median imputation, the center, scale and fit glm\n",
    "set.seed(42)\n",
    "model<-train(X, Y,\n",
    "            method = \"glm\",\n",
    "            preProcess = c(\"medianImpute\", \"center\", \"scale\",\"pca\")\n",
    "            )\n",
    "#model\n",
    "#print(min(model$results$RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the example we add a constant column and try to modeling but unfortunately we obtain an horribly wrong, but it´s hard to tell what, because all of the metrics are missing, this error is due to the constant-valued column, which has a standar deviation of 0 therefore, when we try to scale the column by dividing by stantar deviation we end up with a whole bunch of missing values, which throw off the subsequent stages of modeling .\n",
    "\n",
    "Fortunately `caret` packges salve us again because we can add `zv` to  preprocesing argument to remove cosntatn-valued columns or `nzv`  to remove nearly constant columns.\n",
    "\n",
    "we can fix the previous code adding `zv` like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 3.25045\n"
     ]
    }
   ],
   "source": [
    "# we load mtcars and type some missing at random data\n",
    "data(mtcars)\n",
    "set.seed(42)\n",
    "mtcars[sample(1:nrow(mtcars),10),\"hp\"]<-NA\n",
    "Y<-mtcars$mpg\n",
    "X<-mtcars[,2:4] #missing at random\n",
    "\n",
    "\n",
    "# add constant-valued columns to mtcars\n",
    "X$bad<-1\n",
    "\n",
    "\n",
    "# Now let´s use our linear model recipe: median imputation, the center, scale and fit glm\n",
    "set.seed(42)\n",
    "model<-train(X, Y,\n",
    "            method = \"glm\",\n",
    "            preProcess = c(\"zv\",\"medianImpute\", \"center\", \"scale\",\"pca\")\n",
    "            )\n",
    "\n",
    "print(min(model$results$RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1)  ,  Low variance variables are unlikely to have a large impact on our models.\n",
    "What's the best reason to remove near zero variance predictors from your data before building a model?\n",
    "To reduce model-fitting time without reducing model accuracy, Low variance variables are unlikely to have a large impact on our models.\n",
    "\n",
    "#### 4.4.2) Remove near zero variance predictors\n",
    "you'll be using the blood-brain dataset. This is a biochemical dataset in which the task is to predict the following value for a set of biochemical compounds:\n",
    "\n",
    "    log((concentration of compound in brain) /\n",
    "          (concentration of compound in blood))\n",
    "          \n",
    "This gives a quantitative metric of the compound's ability to cross the blood-brain barrier, and is useful for understanding the biological properties of that barrier.\n",
    "\n",
    "One interesting aspect of this dataset is that it contains many variables and many of these variables have extemely low variances. This means that there is very little information in these variables because they mostly consist of a single value (e.g. zero).\n",
    "\n",
    "Fortunately, `caret` contains a utility function called `nearZeroVar()` for removing such variables to save time during modeling.\n",
    "\n",
    "`nearZeroVar()` takes in data `x`, then looks at the ratio of the most common value to the second most common value, `freqCut`, and the percentage of distinct values out of the number of total samples, `uniqueCut`. By default, `caret` uses `freqCut = 19` and `uniqueCut` = 10, which is fairly conservative. I like to be a little more aggressive and use `freqCut = 2` and `uniqueCut = 20` when calling `nearZeroVar()`.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "bloodbrain_x and bloodbrain_y are loaded in your workspace.\n",
    "\n",
    "- Identify the near zero variance predictors by running nearZeroVar() on the blood-brain dataset. Store the result as an object called remove_cols. Use freqCut = 2 and uniqueCut = 20 in the call to nearZeroVar().\n",
    "- Use names() to create a vector containing all column names of bloodbrain_x. Call this all_cols.\n",
    "- Make a new data frame called bloodbrain_x_small with the near-zero variance variables removed. Use setdiff() to isolate the column names that you wish to keep (i.e. that you don't want to remove.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "myControl <- trainControl(\n",
    "  method = \"cv\", \n",
    "  number = 10,\n",
    "  summaryFunction = twoClassSummary,\n",
    "  classProbs = TRUE, # IMPORTANT!\n",
    "  verboseIter = TRUE\n",
    ")\n",
    "\n",
    "#avalible in broodbrain\n",
    "#bloodbrain_x\n",
    "#bloodbrain_y\n",
    "\n",
    "\n",
    "# Identify near zero variance predictors: remove_cols\n",
    "remove_cols <- nearZeroVar(bloodbrain_x, names = TRUE, \n",
    "                           freqCut = 2, uniqueCut = 20)\n",
    "\n",
    "# Get all column names from bloodbrain_x: all_cols\n",
    "all_cols<-names(bloodbrain_x)\n",
    "\n",
    "# Remove from data: bloodbrain_x_small\n",
    "bloodbrain_x_small <- bloodbrain_x[ , setdiff(all_cols, remove_cols)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3) preProcess() and nearZeroVar()\n",
    "Can you use the preProcess argument in caret to remove near-zero variance predictors? Or do you have to do this by hand, prior to modeling, using the nearZeroVar() function? Yes! Set the preProcess argument equal to \"nzv\"\n",
    "\n",
    "#### 4.4.4) Fit model on reduced blood-brain data\n",
    "Now that you've reduced your dataset, you can fit a `glm` model to it using the `train()` function. This model will run faster than using the full dataset and will yield very similar predictive accuracy.\n",
    "\n",
    "Furthermore, zero variance variables can cause problems with cross-validation (e.g. if one fold ends up with only a single unique value for that variable), so removing them prior to modeling means you are less likely to get errors during the fitting process.\n",
    "\n",
    "**Exercise**\n",
    "bloodbrain_x, bloodbrain_y, remove, and bloodbrain_x_small are loaded in your workspace.\n",
    "\n",
    "- Fit a glm model using the train() function and the reduced blood-brain dataset you created in the previous exercise.\n",
    "- Print the result to the console.\n",
    "\n",
    "*Anwear*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\"Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "\"prediction from a rank-deficient fit may be misleading\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "208 samples\n",
       "112 predictors\n",
       "\n",
       "No pre-processing\n",
       "Resampling: Bootstrapped (25 reps) \n",
       "Summary of sample sizes: 208, 208, 208, 208, 208, 208, ... \n",
       "Resampling results:\n",
       "\n",
       "  RMSE      Rsquared   MAE     \n",
       "  1.737597  0.1226598  1.136949\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit model on reduced data: model\n",
    "model <- train(\n",
    "  x = bloodbrain_x_small, \n",
    "  y = bloodbrain_y, \n",
    "  method = \"glm\"\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 (video) Principle components analysis (PCA)\n",
    "Principal components analysis (PCA) is one my favorites pre-process steps for linear regression models you'll noticed many that I used it as an many of previous examples. PCA is incredibly useful because it combines all the low-variance and correlated variables in your dataset  into a single set of high-variance, perpendicular predictors . as we saw before, low variance variables can be problematic for cross-validation but can also contain useful information it´s better to find a systematic way to use that information, rather than throw it away, furthermore, perpendicular regresion are useful because they are perfectly uncorrelated.\n",
    "\n",
    "Linear regression models have trouble with correlation between variables (also know as collinearity) and PCA elegantly removes this issue from the equiation. \n",
    "\n",
    "PCA searches for hight-variance linear combinations of the input data that are perpendicular to each other, the first componen of PCA is the highest-variance component, and is the highest variance axis of the orginal dataset, the second PCA component has the second highets variance, and so on.\n",
    "\n",
    "#### 4.5.1) Using PCA as an alternative to nearZeroVar()\n",
    "An alternative to removing low-variance predictors is to run PCA on your dataset. This is sometimes preferable because it does not throw out all of your data: many different low variance predictors may end up combined into one high variance PCA variable, which might have a positive impact on your model's accuracy.\n",
    "\n",
    "This is an especially good trick for linear models: the `pca` option in the `preProcess` argument will center and scale your data, combine low variance variables, and ensure that all of your predictors are orthogonal. This creates an ideal dataset for linear regression modeling, and can often improve the accuracy of your models.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "bloodbrain_x and bloodbrain_y are loaded in your workspace.\n",
    "\n",
    "- Fit a glm model to the full blood-brain dataset using the \"pca\" option to preProcess.\n",
    "- Print the model to the console and inspect the result.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generalized Linear Model \n",
       "\n",
       "208 samples\n",
       "132 predictors\n",
       "\n",
       "Pre-processing: principal component signal extraction (132), centered\n",
       " (132), scaled (132) \n",
       "Resampling: Bootstrapped (25 reps) \n",
       "Summary of sample sizes: 208, 208, 208, 208, 208, 208, ... \n",
       "Resampling results:\n",
       "\n",
       "  RMSE       Rsquared   MAE      \n",
       "  0.6003685  0.4320452  0.4603794\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit glm model using PCA: model\n",
    "model <- train(\n",
    "  x = bloodbrain_x, \n",
    "  y = bloodbrain_y,\n",
    "  method = \"glm\", \n",
    "  preProcess = (\"pca\")\n",
    ")\n",
    "\n",
    "# Print model to console\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Selecting models: a case study in churn prediction\n",
    "in the final chapter of this course, you'll learn how to use resamples() to compare multiple models and select (or ensemble) the best one(s).\n",
    "\n",
    "### 5.1) (video) Reusing a trainControl\n",
    "We are working with a more realistic dataset `customer churn` at a telecom company, we'll fit different kind of models and choose the best.\n",
    "\n",
    "in order to compare apple with apple , we define the training and test folds and make sure each model uses exactly the same split for each folds we can do this by pre-defining a `trainControl` object which explicity specifies which rows are used for model building and which are used as holdouts (this `trainControl` can then be used across multiple models) \n",
    "\n",
    "before to start we need load the `customer churn` data from the C50 package in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'C50' was built under R version 3.5.3\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "      yes        no \n",
       "0.1449145 0.8550855 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "      yes        no \n",
       "0.1441441 0.8558559 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ERROR",
     "evalue": "Error in trainControl(summaryFunction = twoClassSummary, classProbs = TRUE, : unused argument (savePridictions = TRUE)\n",
     "output_type": "error",
     "traceback": [
      "Error in trainControl(summaryFunction = twoClassSummary, classProbs = TRUE, : unused argument (savePridictions = TRUE)\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(C50)\n",
    "\n",
    "# first we load our data\n",
    "data(churn)\n",
    "table(churnTrain$churn)/ nrow(churnTrain) \n",
    "\n",
    "#second make train/test index fro cross validation \n",
    "set.seed(42)\n",
    "myFolds<- createFolds(churnTrain$churn, k = 5)\n",
    "\n",
    "#compare class distribution\n",
    "i<-myFolds$Fold1\n",
    "table(churnTrain$churn[i])/ length(i) \n",
    "\n",
    "#now we use theese folds to create a trainControl object, which we can re-use to fit multiple models.\n",
    "myContro<- trainControl(\n",
    "    summaryFunction = twoClassSummary,\n",
    "    classProbs = TRUE,\n",
    "    verboseIter = TRUE,\n",
    "    savePridictions = TRUE,\n",
    "    index = myFolds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1) Why reuse a trainControl?\n",
    "- So you can use the same summaryFunction and tuning parameters for multiple models.\n",
    "- So you don't have to repeat code when fitting multiple models.\n",
    "- So you can compare models on the exact same training and test data.\n",
    "\n",
    "#### 5.1.2) Make custom train/test indices\n",
    "As you saw in the video, for this chapter you will focus on a real-world dataset that brings together all of the concepts discussed in the previous chapters.\n",
    "\n",
    "The churn dataset contains data on a variety of telecom customers and the modeling challenge is to predict which customers will cancel their service (or churn).\n",
    "\n",
    "In this chapter, you will be exploring two different types of predictive models: `glmnet` and `rf`, so the first order of business is to create a reusable `trainControl` object you can use to reliably compare them.\n",
    "\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "`churn_x` and `churn_y` are loaded in your workspace.\n",
    "\n",
    "- Use `createFolds()` to create 5 CV folds on `churn_y`, your target variable for this exercise.\n",
    "- Pass them to `trainControl()` to create a reusable `trainControl` for comparing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom indices: myFolds\n",
    "myFolds <- createFolds(churn_y, k = 5)\n",
    "\n",
    "# Create reusable trainControl object: myControl\n",
    "myControl <- trainControl(\n",
    "  summaryFunction = twoClassSummary,\n",
    "  classProbs = TRUE, # IMPORTANT!\n",
    "  verboseIter = TRUE,\n",
    "  savePredictions = TRUE,\n",
    "  index = myFolds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2) (video) Reintroducing glmnet\n",
    "\n",
    "recall the `glmnet` model that we learned about early, it's a linear regression model with  build-in variables selection and is a great baseline model for any predictive modeling problem , it is a useful baseline because :\n",
    "- it's fast\n",
    "- uses variable selection to ignore noisy variables\n",
    "-  and also provides linear regresion coeficcients that you can use to understand patters in your data.\n",
    "\n",
    "It yields models that just as interpetable as models from the `lm` or `glm` function in R. A business analyst could use these coefficients to understand key drivers of churn, but even if you only care about predictions, glment is a solid baseline that fits quickly and often provide very accurate models.\n",
    "\n",
    "let´s fit one to the churn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1: alpha=0, lambda=1 \n",
      "- Fold1: alpha=0, lambda=1 \n",
      "+ Fold1: alpha=1, lambda=1 \n",
      "- Fold1: alpha=1, lambda=1 \n",
      "+ Fold2: alpha=0, lambda=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold2: alpha=0, lambda=1 \n",
      "+ Fold2: alpha=1, lambda=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold2: alpha=1, lambda=1 \n",
      "+ Fold3: alpha=0, lambda=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold3: alpha=0, lambda=1 \n",
      "+ Fold3: alpha=1, lambda=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold3: alpha=1, lambda=1 \n",
      "+ Fold4: alpha=0, lambda=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold4: alpha=0, lambda=1 \n",
      "+ Fold4: alpha=1, lambda=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold4: alpha=1, lambda=1 \n",
      "+ Fold5: alpha=0, lambda=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold5: alpha=0, lambda=1 \n",
      "+ Fold5: alpha=1, lambda=1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold5: alpha=1, lambda=1 \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting alpha = 0, lambda = 1 on full training set\n"
     ]
    }
   ],
   "source": [
    "set.seed(42)\n",
    "model_glmnet<-train(\n",
    "    churn ~ .,\n",
    "    churnTrain,\n",
    "    metric = \"ROC\",\n",
    "    method = \"glmnet\",\n",
    "    tuneGrid = expand.grid(\n",
    "    alpha = 0:1,\n",
    "    lambda = 0:10/10),\n",
    "    trControl = myControl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAOVBMVEUAAAAAgP9NTU1oaGh8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\nYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAP1BMVEUAAAAAAP8AzQAA//9N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\nP0ubHGXKqvDHAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(repr)\n",
    "# Change plot size to 4 x 3\n",
    "    options(repr.plot.width=4, repr.plot.height=3)\n",
    "\n",
    "plot(model_glmnet)\n",
    "\n",
    "plot(model_glmnet$finalModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1) Fit the baseline model\n",
    "Now that you have a reusable `trainControl` object called `myControl`, you can start fitting different predictive models to your churn dataset and evaluate their predictive accuracy.\n",
    "\n",
    "You'll start with one of my favorite models, `glmnet`, which penalizes linear and logistic regression models on the size and number of coefficients to help prevent overfitting.\n",
    "\n",
    "**Exercise**\n",
    "- Fit a `glmnet` model to the churn dataset called `model_glmnet`. Make sure to use `myControl`, which you created in the first exercise and is available in your workspace, as the `trainControl` object.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1: alpha=0.10, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold1: alpha=0.10, lambda=0.01821 \n",
      "+ Fold1: alpha=0.55, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold1: alpha=0.55, lambda=0.01821 \n",
      "+ Fold1: alpha=1.00, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold1: alpha=1.00, lambda=0.01821 \n",
      "+ Fold2: alpha=0.10, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold2: alpha=0.10, lambda=0.01821 \n",
      "+ Fold2: alpha=0.55, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold2: alpha=0.55, lambda=0.01821 \n",
      "+ Fold2: alpha=1.00, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold2: alpha=1.00, lambda=0.01821 \n",
      "+ Fold3: alpha=0.10, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold3: alpha=0.10, lambda=0.01821 \n",
      "+ Fold3: alpha=0.55, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold3: alpha=0.55, lambda=0.01821 \n",
      "+ Fold3: alpha=1.00, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold3: alpha=1.00, lambda=0.01821 \n",
      "+ Fold4: alpha=0.10, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold4: alpha=0.10, lambda=0.01821 \n",
      "+ Fold4: alpha=0.55, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold4: alpha=0.55, lambda=0.01821 \n",
      "+ Fold4: alpha=1.00, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold4: alpha=1.00, lambda=0.01821 \n",
      "+ Fold5: alpha=0.10, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold5: alpha=0.10, lambda=0.01821 \n",
      "+ Fold5: alpha=0.55, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold5: alpha=0.55, lambda=0.01821 \n",
      "+ Fold5: alpha=1.00, lambda=0.01821 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs, :\n",
      "\"one multinomial or binomial class has fewer than 8  observations; dangerous ground\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold5: alpha=1.00, lambda=0.01821 \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting alpha = 1, lambda = 0.0182 on full training set\n"
     ]
    }
   ],
   "source": [
    "# Create reusable trainControl object: myControl\n",
    "myControl <- trainControl(\n",
    "  summaryFunction = twoClassSummary,\n",
    "  classProbs = TRUE, # IMPORTANT!\n",
    "  verboseIter = TRUE,\n",
    "  savePredictions = TRUE,\n",
    "  index = myFolds\n",
    ")\n",
    "\n",
    "# *Answer*\n",
    "# Fit glmnet model: model_glmnet\n",
    "model_glmnet <- train(\n",
    "  x = churn_x, \n",
    "  y = churn_y,\n",
    "  metric = \"ROC\",\n",
    "  method = \"glmnet\",\n",
    "  trControl = myControl\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3) (video) Reintroducing random forest\n",
    "Next let's try a random forest model on the churn dataset, after 'glmnet', random forest is always the second model that recommended to fit at any predictive model problem.\n",
    "\n",
    "- it´is slower than 'glmnet' model\n",
    "- could be less interpetable\n",
    "- but in a lot of situation can yield much more accurent models with a little parameters tuning.\n",
    "- Require little pre-processing, not need log tranform or otherwise normalize your predictors,and they handle the missing not at random case pretty well even with median imputation\n",
    "- they also automatically capture threshold effects and variable interactions by default, both of which occurr often in real-world data.\n",
    "\n",
    "Now, we´ll see an example, this model is even easier to fit than glmnet, the default caret values for the tuning parameters are great, so we don't need a custum tunning grid. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1: mtry= 2, min.node.size=1, splitrule=gini \n",
      "- Fold1: mtry= 2, min.node.size=1, splitrule=gini \n",
      "+ Fold1: mtry=35, min.node.size=1, splitrule=gini \n",
      "- Fold1: mtry=35, min.node.size=1, splitrule=gini \n",
      "+ Fold1: mtry=69, min.node.size=1, splitrule=gini \n",
      "- Fold1: mtry=69, min.node.size=1, splitrule=gini \n",
      "+ Fold1: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "- Fold1: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "+ Fold1: mtry=35, min.node.size=1, splitrule=extratrees \n",
      "- Fold1: mtry=35, min.node.size=1, splitrule=extratrees \n",
      "+ Fold1: mtry=69, min.node.size=1, splitrule=extratrees \n",
      "- Fold1: mtry=69, min.node.size=1, splitrule=extratrees \n",
      "+ Fold2: mtry= 2, min.node.size=1, splitrule=gini \n",
      "- Fold2: mtry= 2, min.node.size=1, splitrule=gini \n",
      "+ Fold2: mtry=35, min.node.size=1, splitrule=gini \n",
      "- Fold2: mtry=35, min.node.size=1, splitrule=gini \n",
      "+ Fold2: mtry=69, min.node.size=1, splitrule=gini \n",
      "- Fold2: mtry=69, min.node.size=1, splitrule=gini \n",
      "+ Fold2: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "- Fold2: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "+ Fold2: mtry=35, min.node.size=1, splitrule=extratrees \n",
      "- Fold2: mtry=35, min.node.size=1, splitrule=extratrees \n",
      "+ Fold2: mtry=69, min.node.size=1, splitrule=extratrees \n",
      "- Fold2: mtry=69, min.node.size=1, splitrule=extratrees \n",
      "+ Fold3: mtry= 2, min.node.size=1, splitrule=gini \n",
      "- Fold3: mtry= 2, min.node.size=1, splitrule=gini \n",
      "+ Fold3: mtry=35, min.node.size=1, splitrule=gini \n",
      "- Fold3: mtry=35, min.node.size=1, splitrule=gini \n",
      "+ Fold3: mtry=69, min.node.size=1, splitrule=gini \n",
      "- Fold3: mtry=69, min.node.size=1, splitrule=gini \n",
      "+ Fold3: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "- Fold3: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "+ Fold3: mtry=35, min.node.size=1, splitrule=extratrees \n",
      "- Fold3: mtry=35, min.node.size=1, splitrule=extratrees \n",
      "+ Fold3: mtry=69, min.node.size=1, splitrule=extratrees \n",
      "- Fold3: mtry=69, min.node.size=1, splitrule=extratrees \n",
      "+ Fold4: mtry= 2, min.node.size=1, splitrule=gini \n",
      "- Fold4: mtry= 2, min.node.size=1, splitrule=gini \n",
      "+ Fold4: mtry=35, min.node.size=1, splitrule=gini \n",
      "- Fold4: mtry=35, min.node.size=1, splitrule=gini \n",
      "+ Fold4: mtry=69, min.node.size=1, splitrule=gini \n",
      "- Fold4: mtry=69, min.node.size=1, splitrule=gini \n",
      "+ Fold4: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "- Fold4: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "+ Fold4: mtry=35, min.node.size=1, splitrule=extratrees \n",
      "- Fold4: mtry=35, min.node.size=1, splitrule=extratrees \n",
      "+ Fold4: mtry=69, min.node.size=1, splitrule=extratrees \n",
      "- Fold4: mtry=69, min.node.size=1, splitrule=extratrees \n",
      "+ Fold5: mtry= 2, min.node.size=1, splitrule=gini \n",
      "- Fold5: mtry= 2, min.node.size=1, splitrule=gini \n",
      "+ Fold5: mtry=35, min.node.size=1, splitrule=gini \n",
      "- Fold5: mtry=35, min.node.size=1, splitrule=gini \n",
      "+ Fold5: mtry=69, min.node.size=1, splitrule=gini \n",
      "- Fold5: mtry=69, min.node.size=1, splitrule=gini \n",
      "+ Fold5: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "- Fold5: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "+ Fold5: mtry=35, min.node.size=1, splitrule=extratrees \n",
      "- Fold5: mtry=35, min.node.size=1, splitrule=extratrees \n",
      "+ Fold5: mtry=69, min.node.size=1, splitrule=extratrees \n",
      "- Fold5: mtry=69, min.node.size=1, splitrule=extratrees \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting mtry = 35, splitrule = extratrees, min.node.size = 1 on full training set\n"
     ]
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAOVBMVEUAAAAAgP9NTU1oaGh8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\n8pjRZIYAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(caret)\n",
    "library(C50)\n",
    "\n",
    "# first we load our data\n",
    "data(churn)\n",
    "\n",
    "myFolds <- createFolds(churn_y, k = 5)\n",
    "# Create reusable trainControl object: myControl\n",
    "myControl <- trainControl(\n",
    "  summaryFunction = twoClassSummary,\n",
    "  classProbs = TRUE, # IMPORTANT!\n",
    "  verboseIter = TRUE,\n",
    "  savePredictions = TRUE,\n",
    "  index = myFolds\n",
    ")\n",
    "\n",
    "\n",
    "set.seed(42)\n",
    "churnTrain$churn<-factor(churnTrain$churn, levels = c(\"no\", \"yes\"))\n",
    "\n",
    "model_rf<-train(\n",
    "churn ~ .,\n",
    "    churnTrain,\n",
    "    metric = \"ROC\",\n",
    "    method = \"ranger\",\n",
    "    trControl  = myControl\n",
    "    \n",
    ")\n",
    "\n",
    "plot(model_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1) Random forest with custom trainControl\n",
    "Another one of my favorite models is the random forest, which combines an ensemble of non-linear decision trees into a highly flexible (and usually quite accurate) model.\n",
    "\n",
    "Rather than using the classic randomForest package, you'll be using the `ranger` package, which is a re-implementation of `randomForest` that produces almost the exact same results, but is faster, more stable, and uses less memory. I highly recommend it as a starting point for random forest modeling in R.\n",
    "\n",
    "**Exercise**\n",
    "`churn_x` and `churn_y` are loaded in your workspace.\n",
    "\n",
    "Fit a random forest model to the `churn` dataset. Be sure to use `myControl` as the trainControl like you've done before and implement the \"ranger\" method.\n",
    "\n",
    "*Answer*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold1: mtry= 2, min.node.size=1, splitrule=gini \n",
      "- Fold1: mtry= 2, min.node.size=1, splitrule=gini \n",
      "+ Fold1: mtry=36, min.node.size=1, splitrule=gini \n",
      "- Fold1: mtry=36, min.node.size=1, splitrule=gini \n",
      "+ Fold1: mtry=70, min.node.size=1, splitrule=gini \n",
      "- Fold1: mtry=70, min.node.size=1, splitrule=gini \n",
      "+ Fold1: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "- Fold1: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "+ Fold1: mtry=36, min.node.size=1, splitrule=extratrees \n",
      "- Fold1: mtry=36, min.node.size=1, splitrule=extratrees \n",
      "+ Fold1: mtry=70, min.node.size=1, splitrule=extratrees \n",
      "- Fold1: mtry=70, min.node.size=1, splitrule=extratrees \n",
      "+ Fold2: mtry= 2, min.node.size=1, splitrule=gini \n",
      "- Fold2: mtry= 2, min.node.size=1, splitrule=gini \n",
      "+ Fold2: mtry=36, min.node.size=1, splitrule=gini \n",
      "- Fold2: mtry=36, min.node.size=1, splitrule=gini \n",
      "+ Fold2: mtry=70, min.node.size=1, splitrule=gini \n",
      "- Fold2: mtry=70, min.node.size=1, splitrule=gini \n",
      "+ Fold2: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "- Fold2: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "+ Fold2: mtry=36, min.node.size=1, splitrule=extratrees \n",
      "- Fold2: mtry=36, min.node.size=1, splitrule=extratrees \n",
      "+ Fold2: mtry=70, min.node.size=1, splitrule=extratrees \n",
      "- Fold2: mtry=70, min.node.size=1, splitrule=extratrees \n",
      "+ Fold3: mtry= 2, min.node.size=1, splitrule=gini \n",
      "- Fold3: mtry= 2, min.node.size=1, splitrule=gini \n",
      "+ Fold3: mtry=36, min.node.size=1, splitrule=gini \n",
      "- Fold3: mtry=36, min.node.size=1, splitrule=gini \n",
      "+ Fold3: mtry=70, min.node.size=1, splitrule=gini \n",
      "- Fold3: mtry=70, min.node.size=1, splitrule=gini \n",
      "+ Fold3: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "- Fold3: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "+ Fold3: mtry=36, min.node.size=1, splitrule=extratrees \n",
      "- Fold3: mtry=36, min.node.size=1, splitrule=extratrees \n",
      "+ Fold3: mtry=70, min.node.size=1, splitrule=extratrees \n",
      "- Fold3: mtry=70, min.node.size=1, splitrule=extratrees \n",
      "+ Fold4: mtry= 2, min.node.size=1, splitrule=gini \n",
      "- Fold4: mtry= 2, min.node.size=1, splitrule=gini \n",
      "+ Fold4: mtry=36, min.node.size=1, splitrule=gini \n",
      "- Fold4: mtry=36, min.node.size=1, splitrule=gini \n",
      "+ Fold4: mtry=70, min.node.size=1, splitrule=gini \n",
      "- Fold4: mtry=70, min.node.size=1, splitrule=gini \n",
      "+ Fold4: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "- Fold4: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "+ Fold4: mtry=36, min.node.size=1, splitrule=extratrees \n",
      "- Fold4: mtry=36, min.node.size=1, splitrule=extratrees \n",
      "+ Fold4: mtry=70, min.node.size=1, splitrule=extratrees \n",
      "- Fold4: mtry=70, min.node.size=1, splitrule=extratrees \n",
      "+ Fold5: mtry= 2, min.node.size=1, splitrule=gini \n",
      "- Fold5: mtry= 2, min.node.size=1, splitrule=gini \n",
      "+ Fold5: mtry=36, min.node.size=1, splitrule=gini \n",
      "- Fold5: mtry=36, min.node.size=1, splitrule=gini \n",
      "+ Fold5: mtry=70, min.node.size=1, splitrule=gini \n",
      "- Fold5: mtry=70, min.node.size=1, splitrule=gini \n",
      "+ Fold5: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "- Fold5: mtry= 2, min.node.size=1, splitrule=extratrees \n",
      "+ Fold5: mtry=36, min.node.size=1, splitrule=extratrees \n",
      "- Fold5: mtry=36, min.node.size=1, splitrule=extratrees \n",
      "+ Fold5: mtry=70, min.node.size=1, splitrule=extratrees \n",
      "- Fold5: mtry=70, min.node.size=1, splitrule=extratrees \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting mtry = 36, splitrule = extratrees, min.node.size = 1 on full training set\n"
     ]
    }
   ],
   "source": [
    "# Fit random forest: model_rf\n",
    "model_rf <- train(\n",
    "  x = churn_x, \n",
    "  y = churn_y,\n",
    "  metric = \"ROC\",\n",
    "  method = \"ranger\",\n",
    "  trControl = myControl\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4) (video) Comparing models\n",
    "After fitting 2 or more models the next step is deciding which one makes the best prediction on new data, first of all, we have make sure they were fit on exact the same training and test set during cross validation ,so , we're sure making an apples to apples comparasion of their result.\n",
    "\n",
    "We wanto to pick the model with the highet avarage AUC across all 10 validation folds, but also typically want a model with a low standar diviation in AUC, fortunately the  'caret' package provides handy function for collecting the result from mutiple models, this function is called 'resamples' and it provides a variety of methods for assessing which of two models is the best for a given dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1) Matching train/test indices\n",
    "What's the primary reason that train/test indices need to match when comparing two models?\n",
    "Because otherwise you wouldn't be doing a fair comparison of your models and your results could be due to chance. it's because Train/test indexes allow you to evaluate your models out of sample so you know that they work\n",
    "\n",
    "#### 5.4.2) Create a resamples object\n",
    "Now that you have fit two models to the `churn` dataset, it's time to compare their out-of-sample predictions and choose which one is the best model for your dataset.\n",
    "\n",
    "You can compare models in `caret` using the `resamples()` function, provided they have the same training data and use the same `trainControl` object with preset `cross-validation` folds. `resamples()` takes as input a list of models and can be used to compare dozens of models at once (though in this case you are only comparing two models).\n",
    "\n",
    "**Exercise**\n",
    "model_glmnet and model_rf are loaded in your workspace.\n",
    "\n",
    "- Create a list() containing the glmnet model as item1 and the ranger model as item2.\n",
    "- Pass this list to the resamples() function and save the resulting object as resamples.\n",
    "- Summarize the results by calling summary() on resamples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "summary.resamples(object = resamples)\n",
       "\n",
       "Models: item1, item2 \n",
       "Number of resamples: 5 \n",
       "\n",
       "ROC \n",
       "           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n",
       "item1 0.3919540 0.5509890 0.6317241 0.5916097 0.6686561 0.7147253    0\n",
       "item2 0.6073626 0.6883289 0.7134066 0.6968541 0.7370115 0.7381609    0\n",
       "\n",
       "Sens \n",
       "           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n",
       "item1 0.9314286 0.9425287 0.9485714 0.9518621 0.9655172 0.9712644    0\n",
       "item2 0.9540230 0.9828571 0.9885057 0.9839278 0.9942529 1.0000000    0\n",
       "\n",
       "Spec \n",
       "      Min.    1st Qu. Median       Mean    3rd Qu.      Max. NA's\n",
       "item1 0.04 0.07692308   0.08 0.15476923 0.19230769 0.3846154    0\n",
       "item2 0.00 0.00000000   0.04 0.03938462 0.07692308 0.0800000    0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create model_list\n",
    "model_list <- list(item1 = model_glmnet, item2 = model_rf)\n",
    "\n",
    "# Pass model_list to resamples(): resamples\n",
    "resamples<-resamples(model_list)\n",
    "\n",
    "# Summarize the results\n",
    "summary(resamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4) (video) More on resamples\n",
    "resamples provides a ton a cool methods for comparing models.\n",
    "\n",
    "- box and box-and-whisker plot of AUC scores like  \n",
    "        bwplot(resamps, metric = \"ROC\")\n",
    "- dotplot show the same information so : \n",
    "        dotplot(resamps, metric = \"ROC\")\n",
    "- a density plor shows the full distribution of AUC scores using kernel density plot.\n",
    "        densityplot(resamps, metric = \"ROC\")\n",
    "- scatterplot to directly compare the AUC on all 10 cross-validation folds\n",
    "        xyplot(resamps, metric = \"ROC\")\n",
    "- if we had many models, we can still sumarize them using the same functions but recomend you to use 'dotplot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1) Create a box-and-whisker plot\n",
    "`caret` provides a variety of methods to use for comparing models. All of these methods are based on the `resamples()` function. My favorite is the *box-and-whisker plot*, which allows you to compare the distribution of predictive accuracy (in this case AUC) for the two models.\n",
    "\n",
    "In general, you want the model with the higher median AUC, as well as a smaller range between min and max AUC.\n",
    "\n",
    "You can make this plot using the `bwplot()` function, which makes a box and whisker plot of the model's out of sample scores. Box and whisker plots show the median of each distribution as a line and the interquartile range of each distribution as a box around the median line. You can pass the `metric = \"ROC\"` argument to the bwplot() function to show a plot of the model's out-of-sample ROC scores and choose the model with the highest median ROC.\n",
    "\n",
    "If you do not specify a metric to plot, `bwplot()` will automatically plot 3 of them.\n",
    "\n",
    "**Exercise**\n",
    "Pass the `resamples` object to the `bwplot()` function to make a box-and-whisker plot. Look at the resulting plot and note which model has the higher median ROC statistic. Be sure to specify which metric you want to plot.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAAM1BMVEUAAAAAgP9NTU1oaGh8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\nExiewPD+B0L8iVf5hOlmAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bwplot\n",
    "bwplot(resamples, metric = \"ROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2)  Create a scatterplot\n",
    "Another useful plot for comparing models is the scatterplot, also known as the xy-plot. This plot shows you how similar the two models' performances are on different folds.\n",
    "\n",
    "It's particularly useful for identifying if one model is consistently better than the other across all folds, or if there are situations when the inferior model produces better predictions on a particular subset of the data.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Pass the `resamples` object to the `xyplot()` function. Look at the resulting plot and note how similar the two models' predictions are (or are not) on the different folds. Be sure to specify which metric you want to plot.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAANlBMVEUAAAAAgP9NTU1oaGh8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\nGBwKBoeCwaFgcCgYHAoGh4LBoWBw/gNtdSH7rgbPjgAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create xyplot\n",
    "xyplot(resamples, metric = \"ROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3) Ensembling models\n",
    "That concludes the course! As a teaser for a future course on making ensembles of `caret models`, I'll show you how to fit a stacked ensemble of models using the `caretEnsemble` package.\n",
    "\n",
    "`caretEnsemble` provides the `caretList()` function for creating multiple `caret` models at once on the same dataset, using the same resampling folds. You can also create your own lists of `caret` models.\n",
    "\n",
    "In this exercise, I've made a `caretList` for you, containing the `glmnet` and `ranger` models you fit on the churn dataset. Use the `caretStack()` function to make a stack of caret models, with the two sub-models (glmnet and ranger) feeding into another (hopefully more accurate!) caret model.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "- Call the caretStack() function with two arguments, model_list and method = \"glm\", to ensemble the two models using a logistic regression. Store the result as stack.\n",
    "- Summarize the resulting model with the summary() function.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in caretStack(model_list, method = \"glm\"): no se pudo encontrar la función \"caretStack\"\n",
     "output_type": "error",
     "traceback": [
      "Error in caretStack(model_list, method = \"glm\"): no se pudo encontrar la función \"caretStack\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# Create ensemble model: stack\n",
    "stack <- caretStack(model_list, method = \"glm\")\n",
    "\n",
    "# Look at summary\n",
    "summary(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(repr)\n",
    "# Change plot size to 4 x 3\n",
    "    options(repr.plot.width=4, repr.plot.height=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "*Answer*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning in R: Classification\n",
    "link: https://www.datacamp.com/courses/supervised-learning-in-r-classification\n",
    "\n",
    "### Course Description\n",
    "This beginner-level introduction to machine learning covers four of the most common classification algorithms. You will come away with a basic understanding of how each algorithm approaches a learning task, as well as learn the R functions needed to apply these tools to your own work.\n",
    "\n",
    "\n",
    "### Note how can Resizing plots in the R kernel for Jupyter notebooks\n",
    "https://blog.revolutionanalytics.com/2015/09/resizing-plots-in-the-r-kernel-for-jupyter-notebooks.html\n",
    "\n",
    "    library(repr)\n",
    "\n",
    "    # Change plot size to 4 x 3\n",
    "    options(repr.plot.width=4, repr.plot.height=3)\n",
    "    \n",
    "### Note2 Generate a table \n",
    "\n",
    "https://www.tablesgenerator.com/markdown_tables\n",
    "\n",
    "\n",
    "other: Book: machine learning with R by Brett Lantz\n",
    "Learn about `attr` function\n",
    "\n",
    "\n",
    "### Note 3 - DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'dplyr' was built under R version 3.5.3\"\n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.5.3\"Warning message:\n",
      "\"package 'readr' was built under R version 3.5.3\"Warning message:\n",
      "\"package 'openintro' was built under R version 3.5.2\"Please visit openintro.org for free statistics materials\n",
      "\n",
      "Attaching package: 'openintro'\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    diamonds\n",
      "\n",
      "The following objects are masked from 'package:datasets':\n",
      "\n",
      "    cars, trees\n",
      "\n",
      "Warning message:\n",
      "\"package 'naivebayes' was built under R version 3.5.3\"naivebayes 0.9.6 loaded\n",
      "Warning message:\n",
      "\"package 'gridExtra' was built under R version 3.5.3\"\n",
      "Attaching package: 'gridExtra'\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    combine\n",
      "\n",
      "Parsed with column specification:\n",
      "cols(\n",
      "  keep = col_double(),\n",
      "  rand = col_double(),\n",
      "  default = col_double(),\n",
      "  loan_amount = col_character(),\n",
      "  emp_length = col_character(),\n",
      "  home_ownership = col_character(),\n",
      "  income = col_character(),\n",
      "  loan_purpose = col_character(),\n",
      "  debt_to_income = col_character(),\n",
      "  credit_score = col_character(),\n",
      "  recent_inquiry = col_character(),\n",
      "  delinquent = col_character(),\n",
      "  credit_accounts = col_character(),\n",
      "  bad_public_record = col_character(),\n",
      "  credit_utilization = col_character(),\n",
      "  past_bankrupt = col_character()\n",
      ")\n",
      "Parsed with column specification:\n",
      "cols(\n",
      "  .default = col_double(),\n",
      "  sample = col_character(),\n",
      "  sign_type = col_character()\n",
      ")\n",
      "See spec(...) for full column specifications.\n",
      "Parsed with column specification:\n",
      "cols(\n",
      "  donated = col_double(),\n",
      "  veteran = col_double(),\n",
      "  bad_address = col_double(),\n",
      "  age = col_double(),\n",
      "  has_children = col_double(),\n",
      "  wealth_rating = col_double(),\n",
      "  interest_veterans = col_double(),\n",
      "  interest_religion = col_double(),\n",
      "  pet_owner = col_double(),\n",
      "  catalog_shopper = col_double(),\n",
      "  recency = col_character(),\n",
      "  frequency = col_character(),\n",
      "  money = col_character()\n",
      ")\n",
      "Parsed with column specification:\n",
      "cols(\n",
      "  month = col_double(),\n",
      "  day = col_double(),\n",
      "  weekday = col_character(),\n",
      "  daytype = col_character(),\n",
      "  hour = col_double(),\n",
      "  hourtype = col_character(),\n",
      "  location = col_character()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(readr)\n",
    "library(tidyr)\n",
    "library(datasets)\n",
    "library(openintro)\n",
    "\n",
    "#knn\n",
    "library(class)\n",
    "#using a naive bayes model\n",
    "library(naivebayes)\n",
    "\n",
    "\n",
    "#to plot two graphs at sime time \n",
    "library(gridExtra)\n",
    "# grid.arrange(p1, p2, nrow=1)\n",
    "\n",
    "\n",
    "#Soccer player positions\n",
    "#lineup<-readRDS(\"D:/Analista Pricing/6.0 Personal/R/DataSources/lineup.rds\")\n",
    "\n",
    "library(readr)\n",
    "path_csv<-\"https://assets.datacamp.com/production/repositories/718/datasets/7805fceacfb205470c0e8800d4ffc37c6944b30c/loans.csv\"\n",
    "loans<-read_csv(path_csv)\n",
    "\n",
    "path_csv<-\"https://assets.datacamp.com/production/repositories/718/datasets/c274ea22cc3d7e12d7bb9fdc9c2bdabe9ab025f4/knn_traffic_signs.csv\"\n",
    "knn_traffic_signs<-read_csv(path_csv)\n",
    "#head(knn_traffic_signs, n = 2)\n",
    "\n",
    "    #knn_traffic_signs[knn_traffic_signs$sample == \"example\",]\n",
    "    signs<-knn_traffic_signs[knn_traffic_signs$sample == \"train\", -c(1, 2) ]\n",
    "    test_signs<-knn_traffic_signs[knn_traffic_signs$sample == \"test\", -c(1, 2) ]\n",
    "    next_sign<-knn_traffic_signs[knn_traffic_signs$sample == \"example\", -c(1, 2, 3) ]\n",
    "    sign_types <- signs$sign_type\n",
    "\n",
    "#cold be required others \n",
    "    signs_test<-test_signs\n",
    "    signs_actual<-test_signs$sign_type\n",
    "    \n",
    "\n",
    "path_csv<-\"https://assets.datacamp.com/production/repositories/718/datasets/9055dac929e4515286728a2a5dae9f25f0e4eff6/donors.csv\"\n",
    "donors<-read_csv(path_csv)\n",
    "\n",
    "path_csv<-\"https://assets.datacamp.com/production/repositories/718/datasets/571628c39048df59c40c9dcfba146a2cf7a4a0e3/locations.csv\"\n",
    "locations<-read_csv(path_csv)\n",
    "where9am<-locations[locations$hour == \"9\",c(\"daytype\",\"location\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Chapter 1: k-Nearest Neighbors (kNN)\n",
    "As the kNN algorithm literally \"learns by example\" it is a case in point for starting to understand supervised machine learning. This chapter will introduce classification while working through the application of kNN to self-driving vehicle road sign recognition.\n",
    "\n",
    "\n",
    "### 1.1) (video) Classification with Nearest Neighbors\n",
    "Machine learning utilizes computers to turn data into insight and action, this course focuses on a subset of machine learning, the sub-domain called `supervised learning` focuses on training a machine to learn from prior examples, when the concept to be learned is a set of categories, the task is called `classification`  from identifying deseases, predecting the wather or detecting whether an image contains a cat, classification tasks are diverse yet common.\n",
    "\n",
    "In this course you will learn classifcation methods while exploring four real-world applications, so let´s get started. \n",
    "\n",
    "if your experiences on the road are anything like mine, self-driving car can't get here soon enough!, it´s easy to imagine aspects of autonomous driving that involve classification.\n",
    "\n",
    "For example when a vehicle's camara observes an object, it must classify the object before it can be react, though the algorithms that govern autonomous car are sophisticated, we can simulate aspects of their beheivor, in this example, we'll suppose the vehicle can see but not distinguish the roadway signs, you job will be to use machinee learning to classify the sign's type \n",
    "\n",
    "To start training a self-driving car , you migh supervised it  by demonstrating the desired behavior as it observe each type of sign, you stopped at intersactions, yield to pedestrians and change speed as needed.\n",
    "\n",
    "After some time under your instrunction, the vehicle has built a database that records the sign as well as the target behavior (we can have images of sign stop, speed limit and other things)\n",
    "\n",
    "I suspect you already see some similarities, the machine can too!,  a nearest neighbor classifier takes advantage of the fact that signs that look alike should be similar to, or  \"nearby\" other signs of same type.\n",
    "\n",
    "for example if the car observes a sign that seem similar to those in the the group of stop signs, the car will probably need to stop, so how does a nearest neighbords learner decided whether two signs are similar? it does so by literally measuring the distance between them, that's not to say that it measures the distances between the signs in physical space, a stop sign in New York is the same as a stop sign in Los Angeles,but instead, it imagines the properties of the signs as coordinates in what is called a feature spaces, cosiderer, for instance , the sign's color,by imagining the color  as a 3-dimensional feature space measuring levels of red,green and blue, signs of similar color are located naturally close  to one another once the feature space has been constructed in this way:\n",
    "\n",
    "    dist(p,q) = sqrt([p1-q1]^2 + [p2-q2]^2 ...+ [pn-qn]^2)\n",
    "\n",
    "you can measure distance using a formula like those you may have seen in a geometry class, many nearest neighbor learners use the Euclidean distance formula.\n",
    "\n",
    "An algorithm called k-nearest Neighbors or KNN, uses the principle of nearest neighborns to classify unlabeled examples \n",
    "\n",
    "We'll get into the specifics later, but for now it suffices to know that, by default, R's knn function searches a dataset for the historic observation most similar to the newly-observerd one, the `knn` function is part of the `class` packages an requires 3 paramateres:\n",
    "\n",
    "    library(class)\n",
    "    pred<-knn(training_data, testing_data, training_labels)\n",
    "\n",
    "#### 1.1.1) Recognizing a road sign with kNN\n",
    "After several trips with a human behind the wheel, it is time for the self-driving car to attempt the test course alone.\n",
    "\n",
    "As it begins to drive away, its camera captures the following image:\n",
    "\n",
    "    [Stop Sign]\n",
    "\n",
    "Can you apply a `kNN` classifier to help the car recognize this sign?\n",
    "\n",
    "The dataset `signs` is loaded in your workspace along with the dataframe `next_sign`, which holds the observation you want to classify.\n",
    "\n",
    "**Exercise**\n",
    "- Load the class package.\n",
    "- Create a vector of sign labels to use with kNN by extracting the column sign_type from signs.\n",
    "- entify the next_sign using the knn() function.\n",
    "    - Set the train argument equal to the signs data frame without the first column.\n",
    "    - Set the test argument equal to the data frame next_sign.\n",
    "    - Use the vector of labels you created as the cl argument\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "stop\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<ol class=list-inline>\n",
       "\t\t<li>'pedestrian'</li>\n",
       "\t\t<li>'speed'</li>\n",
       "\t\t<li>'stop'</li>\n",
       "\t</ol>\n",
       "</details>"
      ],
      "text/latex": [
       "stop\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item 'pedestrian'\n",
       "\\item 'speed'\n",
       "\\item 'stop'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "stop\n",
       "**Levels**: 1. 'pedestrian'\n",
       "2. 'speed'\n",
       "3. 'stop'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] stop\n",
       "Levels: pedestrian speed stop"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the 'class' package\n",
    "library(class)\n",
    "\n",
    "# Create a vector of labels\n",
    "sign_types <- signs$sign_type\n",
    "\n",
    "# Classify the next sign observed\n",
    "knn(train = signs[-1], test = next_sign, cl = sign_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2) Exploring the traffic sign dataset\n",
    "To better understand how the `knn()` function was able to classify the stop sign, it may help to examine the training dataset it used.\n",
    "\n",
    "Each previously observed street sign was divided into a 4x4 grid, and the red, green, and blue level for each of the 16 center pixels is recorded as illustrated here.\n",
    "\n",
    "![](./Imagenes/KNN_StopSign.jpg)\n",
    "\n",
    "The result is a dataset that records the `sign_type` as well as 16 x 3 = 48 color properties of each sign.\n",
    "\n",
    "**Exercise**\n",
    "- Use the str() function to examine the signs dataset.\n",
    "- Use table() to count the number of observations of each sign type by passing it the column containing the labels.\n",
    "- Run the provided aggregate() command to see whether the average red level might vary by sign type.\n",
    "\n",
    "*Answer*\n",
    "\n",
    "As you might have expected, stop signs tend to have a higher average red value. This is how kNN identifies similar signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "pedestrian      speed       stop \n",
       "        46         49         51 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>sign_type</th><th scope=col>r10</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>pedestrian</td><td>113.71739 </td></tr>\n",
       "\t<tr><td>speed     </td><td> 80.63265 </td></tr>\n",
       "\t<tr><td>stop      </td><td>132.39216 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " sign\\_type & r10\\\\\n",
       "\\hline\n",
       "\t pedestrian & 113.71739 \\\\\n",
       "\t speed      &  80.63265 \\\\\n",
       "\t stop       & 132.39216 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "sign_type | r10 | \n",
       "|---|---|---|\n",
       "| pedestrian | 113.71739  | \n",
       "| speed      |  80.63265  | \n",
       "| stop       | 132.39216  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  sign_type  r10      \n",
       "1 pedestrian 113.71739\n",
       "2 speed       80.63265\n",
       "3 stop       132.39216"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine the structure of the signs dataset\n",
    "#str(signs)\n",
    "\n",
    "# Count the number of signs of each type\n",
    "table(signs$sign_type)\n",
    "\n",
    "\n",
    "# Check r10's average red level by sign type\n",
    "aggregate(r10 ~ sign_type, data = signs, mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3) Classifying a collection of road signs\n",
    "Now that the autonomous vehicle has successfully stopped on its own, your team feels confident allowing the car to continue the test course.\n",
    "\n",
    "The test course includes 58 additional road signs divided into three types:\n",
    "\n",
    "    [Stop Sign] [Speed Limit] [Sign Pedestrian Sign]\n",
    "\n",
    "At the conclusion of the trial, you are asked to measure the car's overall performance at recognizing these signs.\n",
    "\n",
    "The `class` package and the dataset `signs` are already loaded in your workspace. So is the dataframe `test_signs`, which holds a set of observations you'll test your model on.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "- Classify the `test_signs` data using `knn()`.\n",
    "    - Set `train` equal to the observations in `signs` without labels.\n",
    "    - Use `test_signs` for the `test` argument, again without labels.\n",
    "    - For the `cl` argument, use the vector of labels provided for you.\n",
    "- Use `table()` to explore the classifier's performance at identifying the three sign types (the confusion matrix).\n",
    "    - Create the vector `signs_actual` by extracting the labels from `test_signs`.\n",
    "    - Pass the vector of predictions and the vector of actual signs to `table()` to cross tabulate them.\n",
    "- Compute the overall accuracy of the kNN learner using the `mean()` function.\n",
    "\n",
    "*Answer*\n",
    "\n",
    "That self-driving car is really coming along! The confusion matrix lets you look for patterns in the classifier's errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            signs_actual\n",
       "signs_pred   pedestrian speed stop\n",
       "  pedestrian         19     2    0\n",
       "  speed               0    17    0\n",
       "  stop                0     2   19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.932203389830508"
      ],
      "text/latex": [
       "0.932203389830508"
      ],
      "text/markdown": [
       "0.932203389830508"
      ],
      "text/plain": [
       "[1] 0.9322034"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use kNN to identify the test road signs\n",
    "sign_types <- signs$sign_type\n",
    "signs_pred <- knn(train = signs[-1], test = test_signs[-1], cl = sign_types)\n",
    "\n",
    "# Create a confusion matrix of the predicted versus actual values\n",
    "signs_actual <- test_signs$sign_type\n",
    "table(signs_pred, signs_actual)\n",
    "\n",
    "# Compute the accuracy\n",
    "mean(signs_pred == signs_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) (video) What about the 'k' in kNN?\n",
    "you maybe wondering why KNN is called `k` Nearest Neighbors, what exactly os K? the letters k is a variable that specifies the numbers of neighbors to consider when making the classification, you can imagine it as determining the size of the neighborhoods until now, we have ignored k, and thus R has used the default value of 1, this means that only the single nearest, most similar, neighbor was used to classify the unlabeled example, while this seems OK on the surface ,let´s work through an example to see why the value of k may have a substancial impact on the performace of our classifier.\n",
    "\n",
    "Suppose our vehicle observed the sign at the center of the image `(Image 1)`, its five nearest neighbors are depicted the single nearest neighbor is a speed limit sign, which share a very similar background color, unfortunately, in this case, a knn classifier with K set to 1 would make an incorrect classifaction,slightly further away are the second, third and fourth nearest neighbors, which are all pedestrian crossing signs, suppose we set k to 3, what would happen? the 3 nearest neighbors ,a spead limit sign and two pedestrian crossing sign, would  take a vote, the category with the majority of nearest neighbors, in this case the pedestrian crossing sign, is the winner, increasing k to 5 allows the 5 nearest neighbors to vote,the pedestrian crossing sign still wins with margin of 3-to-2, note that in the case of a tie the winner is typically decided at random,in the previous example, setting k to a higher value resulted in a correct prediction but it is not always the case that bigger is better,\n",
    "\n",
    "A small k creates a very small neighborhoods; the classifier  is able to discover very subtle patterns, as this image ilustrates `(Image 2)`, you migh imagine it as being able to distinguish between groups even when their boundary is somewhat \"fuzzy\", on the other hand, sometimes a fuzzy boundary is not a true pattern, but rahter due to some other factor that adds randomness into the data this is called noise, setting k larger, as this image shows, ignores some pottencialy-noisy points in an effort to discover a broade more general patterns.\n",
    "\n",
    "![](./Imagenes/KNN_SelectK.jpg)\n",
    "\n",
    "so, how should you set k? unfortunately there is no universal rule, in practice, the optimal values depends on the coplexity of the pattern to be learned, as well as the the impact of the noisy data, some suggest a rule of thumb starting with k equal to the square root of the number of observations in the training data.\n",
    "\n",
    "An even better approach is to test several different values of k and compare the performance on data it has not seen before \n",
    "\n",
    "#### 1.2.1) Understanding the impact of 'k'\n",
    "There is a complex relationship between k and classification accuracy. Bigger is not always better.\n",
    "\n",
    "Which of these is a valid reason for keeping k as small as possible (but no smaller)?\n",
    "Anwer: With smaller neighborhoods, kNN can identify more subtle patterns in the data.\n",
    "\n",
    "#### 1.2.2) Testing other 'k' values\n",
    "By default, the `knn()` function in the `class` package uses only the single nearest neighbor.\n",
    "\n",
    "Setting a `k` parameter allows the algorithm to consider additional nearby neighbors. This enlarges the collection of neighbors which will vote on the predicted class.\n",
    "\n",
    "Compare `k` values of 1, 7, and 15 to examine the impact on traffic sign classification accuracy.\n",
    "\n",
    "The `class` package is already loaded in your workspace along with the datasets `signs`, `signs_test`, and `sign_types`. The object `signs_actual` holds the true values of the signs.\n",
    "\n",
    "**Exercise**\n",
    "- Compute the accuracy of the default `k = 1` model using the given code, then find the accuracy of the model using `mean()` to compare `signs_actual` and the model's predictions.\n",
    "- Modify the `knn()` function call by setting `k = 7` and again find accuracy value.\n",
    "- Revise the code once more by setting `k = 15`, plus find the accuracy value one more time.\n",
    "\n",
    "*Answer*\n",
    "    #Note the answer is different our code becuase, in the exercise they changed some names, so first we need renamed them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.932203389830508"
      ],
      "text/latex": [
       "0.932203389830508"
      ],
      "text/markdown": [
       "0.932203389830508"
      ],
      "text/plain": [
       "[1] 0.9322034"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.966101694915254"
      ],
      "text/latex": [
       "0.966101694915254"
      ],
      "text/markdown": [
       "0.966101694915254"
      ],
      "text/plain": [
       "[1] 0.9661017"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.88135593220339"
      ],
      "text/latex": [
       "0.88135593220339"
      ],
      "text/markdown": [
       "0.88135593220339"
      ],
      "text/plain": [
       "[1] 0.8813559"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#signs\n",
    "#test_signs\n",
    "#next_sign\n",
    "\n",
    "\n",
    "signs_test<-test_signs\n",
    "signs_actual<-test_signs$sign_type\n",
    "\n",
    "#head(signs, n = 1)\n",
    "#head(test_signs, n =1)\n",
    "#head(sign_types, n =1)\n",
    "\n",
    "# Compute the accuracy of the baseline model (default k = 1)\n",
    "k_1 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types)\n",
    "mean(signs_actual == k_1)\n",
    "\n",
    "# Modify the above to set k = 7\n",
    "k_7 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types, k = 7)\n",
    "mean(signs_actual == k_7)\n",
    "\n",
    "# Set k = 15 and compare to the above\n",
    "k_15 <- knn(train = signs[-1], test = signs_test[-1], cl = sign_types, k = 15)\n",
    "mean(signs_actual == k_15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3) Seeing how the neighbors voted\n",
    "When multiple nearest neighbors hold a vote, it can sometimes be useful to examine whether the voters were unanimous or widely separated.\n",
    "\n",
    "For example, knowing more about the voters' confidence in the classification could allow an autonomous vehicle to use caution in the case there is any chance at all that a stop sign is ahead.\n",
    "\n",
    "In this exercise, you will learn how to obtain the voting results from the `knn()` function.\n",
    "\n",
    "The `class` package has already been loaded in your workspace along with the datasets `signs`, `sign_types`, and `signs_test`\n",
    "\n",
    "**Exercise**\n",
    "- Build a `kNN` model with the `prob = TRUE` parameter to compute the vote proportions. Set `k = 7`.\n",
    "- Use the `attr()` function to obtain the vote proportions for the predicted class. These are stored in the attribute `\"prob\"`.\n",
    "- Examine the first several vote outcomes and percentages using the `head()` function to see how the confidence varies from sign to sign\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>pedestrian</li>\n",
       "\t<li>pedestrian</li>\n",
       "\t<li>pedestrian</li>\n",
       "\t<li>stop</li>\n",
       "\t<li>pedestrian</li>\n",
       "\t<li>pedestrian</li>\n",
       "</ol>\n",
       "\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<ol class=list-inline>\n",
       "\t\t<li>'pedestrian'</li>\n",
       "\t\t<li>'speed'</li>\n",
       "\t\t<li>'stop'</li>\n",
       "\t</ol>\n",
       "</details>"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item pedestrian\n",
       "\\item pedestrian\n",
       "\\item pedestrian\n",
       "\\item stop\n",
       "\\item pedestrian\n",
       "\\item pedestrian\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item 'pedestrian'\n",
       "\\item 'speed'\n",
       "\\item 'stop'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. pedestrian\n",
       "2. pedestrian\n",
       "3. pedestrian\n",
       "4. stop\n",
       "5. pedestrian\n",
       "6. pedestrian\n",
       "\n",
       "\n",
       "\n",
       "**Levels**: 1. 'pedestrian'\n",
       "2. 'speed'\n",
       "3. 'stop'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] pedestrian pedestrian pedestrian stop       pedestrian pedestrian\n",
       "Levels: pedestrian speed stop"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.571428571428571</li>\n",
       "\t<li>0.571428571428571</li>\n",
       "\t<li>0.857142857142857</li>\n",
       "\t<li>0.571428571428571</li>\n",
       "\t<li>0.857142857142857</li>\n",
       "\t<li>0.571428571428571</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.571428571428571\n",
       "\\item 0.571428571428571\n",
       "\\item 0.857142857142857\n",
       "\\item 0.571428571428571\n",
       "\\item 0.857142857142857\n",
       "\\item 0.571428571428571\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.571428571428571\n",
       "2. 0.571428571428571\n",
       "3. 0.857142857142857\n",
       "4. 0.571428571428571\n",
       "5. 0.857142857142857\n",
       "6. 0.571428571428571\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.5714286 0.5714286 0.8571429 0.5714286 0.8571429 0.5714286"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the prob parameter to get the proportion of votes for the winning class\n",
    "sign_pred <- knn(signs[-1], signs_test[-1], cl = sign_types,k = 7, prob = TRUE )\n",
    "\n",
    "# Get the \"prob\" attribute from the predicted classes\n",
    "sign_prob <- attr(sign_pred,which = \"prob\")\n",
    "\n",
    "# Examine the first several predictions\n",
    "head(sign_pred)\n",
    "\n",
    "# Examine the proportion of votes for the winning class\n",
    "head(sign_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 (video) Data preparation for kNN\n",
    "you've now seen the KNN algorithm in action, while simulating aspects of a self-driving vehicle , you 've gained an understanding of the impact of K on the algorithm's performance ,and know how to examine the neighbors' votos to better understand which predictions are closer to unanimous, but before applying  knn to your own projects, you 'll need to know one more thing, how to prepare your data for nearest neighbors?\n",
    "\n",
    "as noted previously, nearest neighbor learners use distance functions to identify the most similar or nearest examples, many common distance functions assume that your data are in numerical format, as  it is difficult to define distance between categories, for example:\n",
    "\n",
    "there is no obvious way to define the distance bewtween \"red\" and \"yellow\", consequently , the traffic sign dataset represented these using numeric color intensities but suposse that you have a property that cann´t be measured numerically, such as whether a road sign is a rectangle, diamond or octagon.\n",
    "\n",
    " a common solution uses 1/0 indicators to represent these categories, this is called dummy coding, a binary \"dummy\" variable is created for each category except one.This variable is set to 1 if the category applies and 0 othewise,\n",
    "\n",
    "the category that is left out can be  easily deduced, if the stop sign is not a rectangle or a diamond , then it must be an octagon, dummy coded data can be used directly in a distnace function, two rectangle sign both having values of 1, will be found to be closer toghether than a rectangle and a diamond, it´s also important to be aware that when calculations distance, each feature of the input data should be masured with the same range of values, this was true for the traffic sign data, each color component ranged from a minimun of zero to a maximum of 255, however, suppose that we added the 1/0 dummy variables for sign shapes into the distance calculation, two different shapes may differ by at most one unit, but two diffente colors may differ by as much as 255 units, such as different scale allows the features whit a wider  range to have more influene over the other distance calculation, as this figure ilustrates.\n",
    "\n",
    "![](./Imagenes/KNN_normalized.jpg)\n",
    "\n",
    "here, the topmost speed limit sign is closer to the pedestrian sign that it is to its correct neighbors, simply because the range of blue value is wider that the 0-to-1 range of shape values, compressing the blue axis so that it also follows a 0-to-1 range corrects this issue, and the speed limit signs is now closer to its true neighborhood\n",
    "\n",
    "R does not have a buil-in function in R to rescale data to a given range, so you will need to create one yourself:\n",
    "\n",
    "    #difine a min-ma normalize() function\n",
    "    normalize<- function(x) {\n",
    "    return((x - min (x)) / (max(x) - min(x) )\n",
    "    }\n",
    "    \n",
    "#### 1.3.1) Why normalize data?\n",
    "Before applying kNN to a classification task, it is common practice to rescale the data using a technique like min-max normalization. What is the purpose of this step?\n",
    "Anwer: To ensure all data elements may contribute equal shares to distance, this is because rescaling reduces the influence of extreme values on kNN's distance function.\n",
    "\n",
    "\n",
    "## 2) Naive Bayes\n",
    "Naive Bayes uses principles from the field of statistics to make predictions. This chapter will introduce the basics of Bayesian methods while exploring how to apply these techniques to iPhone-like destination suggestions.\n",
    "\n",
    "### 2.1) (video) Understanding Bayesian methods\n",
    "Some smartphones and apps predict users's destinations to offer routes and traffic estimates without the users even \n",
    " asking, if you did not know about machine learning, the phone's  ability to predict the future this way, might seem a bit like as magic!,  the phone obviously keeps a records of the user's past locations, and it then uses this data the to forecast the usere's most probable future location, much like a  meteorologist estimates the precipitation probability in a wheather report, a branch of statistics called `bayesian methods` apply the work of 18th century statistician Thomas Bayes, who proposed rules for estimating probabilities in light of historic data, by applying these methods to my onw location tracking data, you will learn how probability estimates can forecast action, let's see where the data finds me!\n",
    "\n",
    "This map show the number of times my phone recorded my position at 4 different locations, based on this data, my phone can predict that at any given time my most probable location is at work, because I was there 57.5% of the time: 23 of the past 40 times it checked. \n",
    "\n",
    "![](./Imagenes/bayes1.jpg)\n",
    "\n",
    "this ilustrates how the probability of a event is estimated from historical data, it the numbers of times the event happen, divided by the number of times it could have happend,but even though I am at work a lot, the phone should not predict  I'm there all the time, instead, it should incorporate additional data like time of the day to better tailor its predictions to the situation, this requires an understanding of how combine information from several events into a single probability estimete.\n",
    "\n",
    "When events occurs together, they have a joint probability, their insersaction can be depicted using a Venn diagram like whose show here:\n",
    "\n",
    "![](./Imagenes/bayes2.jpg)\n",
    "\n",
    "These show that there is a much greater probability that I'm at work in afternoon that then evening, the overlap is much greater for work and afternoon, the joint probability of two events is computed by finding the proportion of observations they ocurred together, sometimes one event not influence the probability of another , these are said to bo independent events, for example my location is unrelated to most other users' locations, knowing where they are does not provides information about where I might be.\n",
    "\n",
    "However, many of the other data elements my phone collects such as the time and date, are very predictive of where I may be, when one event is predictive of another, they are called depended, these are the basis of prediction with bayesian methods.\n",
    "\n",
    "Conditional probability expresses exactly how one event depends on another `P(A|B) = P(A and B) / P(B) `\n",
    "\n",
    "\n",
    "![](./Imagenes/bayes3.jpg)\n",
    "\n",
    "We can use this to compute the 4% probabolity that i am at work, given the knowledge that it is evening, in comparision is an 80% chance I'm at work in the afternoon.\n",
    "\n",
    "The algorithm know as `Naive Bayes` applies Bayesian methods to estimate the conditional probability of an outcome, the naivebayes package provides a function to build this model \n",
    "\n",
    "    #building a naive bayes model\n",
    "    library(naivebayes)\n",
    "    m<-naive_bayes(location ~ time_of_day, data = location_history)\n",
    "    # because the location depends on the time of day, it´s specified as location - tilde - time of the day \n",
    "    \n",
    "    #making predictions with naive bayes\n",
    "    future_locations<- predict(m, future_conditions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1) Computing probabilities\n",
    "The `where9am` data frame contains 91 days (thirteen weeks) worth of data in which Brett recorded his `location` at 9am each day as well as whether the daytype was a weekend or weekday.\n",
    "\n",
    "Using the conditional probability formula below, you can compute the probability that Brett is working in the office, given that it is a weekday.\n",
    "\n",
    "    P(A|B)=P(A and B)P(B)\n",
    "    \n",
    "Calculations like these are the basis of the `Naive Bayes` destination prediction model you'll develop in later exercises.\n",
    "\n",
    "**Exercise**\n",
    "- Find P(office) using nrow() and subset() to count rows in the dataset and save the result as p_A.\n",
    "- Find P(weekday), using nrow() and subset() again, and save the result as p_B.\n",
    "- Use nrow() and subset() a final time to find P(office and weekday). Save the result as p_AB.\n",
    "- Compute P(office | weekday) and save the result as p_A_given_B.\n",
    "- Print the value of p_A_given_B.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.6"
      ],
      "text/latex": [
       "0.6"
      ],
      "text/markdown": [
       "0.6"
      ],
      "text/plain": [
       "[1] 0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute P(A) \n",
    "p_A <- nrow(subset(where9am, location == \"office\")) / nrow(where9am)\n",
    "\n",
    "# Compute P(B)\n",
    "p_B <- nrow(subset(where9am, daytype == \"weekday\")) / nrow(where9am)\n",
    "\n",
    "# Compute the observed P(A and B)\n",
    "p_AB <- nrow(subset(where9am, location == \"office\" & daytype == \"weekday\")) / nrow(where9am)\n",
    "\n",
    "# Compute P(A | B) and print its value\n",
    "p_A_given_B <- p_AB / p_B\n",
    "p_A_given_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2) A simple Naive Bayes location model\n",
    "The previous exercises showed that the probability that Brett is at work or at home at 9am is highly dependent on whether it is the weekend or a weekday.\n",
    "\n",
    "To see this finding in action, use the `where9am` data frame to build a `Naive Bayes` model on the same data.\n",
    "\n",
    "You can then use this model to predict the future: where does the model think that Brett will be at 9am on Thursday and at 9am on Saturday?\n",
    "\n",
    "The dataframe `where9am` is available in your workspace. This dataset contains information about Brett's location at 9am on different days.\n",
    "\n",
    "**Exercise**\n",
    "- Load the naivebayes package.\n",
    "- Use naive_bayes() with a formula like y ~ x to build a model of location as a function of daytype.\n",
    "- Forecast the Thursday 9am location using predict() with the thursday9am object as the newdata argument.\n",
    "- Do the same for predicting the saturday9am location.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"naive_bayes(): Feature daytype - zero probabilities are present. Consider Laplace smoothing.\""
     ]
    },
    {
     "data": {
      "text/html": [
       "office\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<ol class=list-inline>\n",
       "\t\t<li>'appointment'</li>\n",
       "\t\t<li>'campus'</li>\n",
       "\t\t<li>'home'</li>\n",
       "\t\t<li>'office'</li>\n",
       "\t</ol>\n",
       "</details>"
      ],
      "text/latex": [
       "office\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item 'appointment'\n",
       "\\item 'campus'\n",
       "\\item 'home'\n",
       "\\item 'office'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "office\n",
       "**Levels**: 1. 'appointment'\n",
       "2. 'campus'\n",
       "3. 'home'\n",
       "4. 'office'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] office\n",
       "Levels: appointment campus home office"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "home\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<ol class=list-inline>\n",
       "\t\t<li>'appointment'</li>\n",
       "\t\t<li>'campus'</li>\n",
       "\t\t<li>'home'</li>\n",
       "\t\t<li>'office'</li>\n",
       "\t</ol>\n",
       "</details>"
      ],
      "text/latex": [
       "home\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item 'appointment'\n",
       "\\item 'campus'\n",
       "\\item 'home'\n",
       "\\item 'office'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "home\n",
       "**Levels**: 1. 'appointment'\n",
       "2. 'campus'\n",
       "3. 'home'\n",
       "4. 'office'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] home\n",
       "Levels: appointment campus home office"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Note it´s very important two have the following data.frame but they need have a factoros\n",
    "thursday9am<-data.frame (daytype = factor(c(\"weekday\"), levels = c (\"weekday\",\"weekend\")))\n",
    "saturday9am<-data.frame (daytype = factor(c(\"weekend\"), levels = c (\"weekday\",\"weekend\")))\n",
    "\n",
    "# Load the naivebayes package\n",
    "library(naivebayes)\n",
    "\n",
    "# Build the location prediction model\n",
    "locmodel <- naive_bayes(location ~ daytype, data = where9am)\n",
    "\n",
    "# Predict Thursday's 9am location\n",
    "predict(locmodel, thursday9am)\n",
    "\n",
    "# Predict Saturdays's 9am location\n",
    "predict(locmodel, saturday9am)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3) Examining \"raw\" probabilities\n",
    "The `naivebayes` package offers several ways to peek inside a Naive Bayes model.\n",
    "\n",
    "Typing the name of the model object provides the **a priori** (overall) and conditional probabilities of each of the model's predictors. If one were so inclined, you might use these for calculating **posterior** (predicted) probabilities by hand.\n",
    "\n",
    "Alternatively, R will compute the posterior probabilities for you if the `type = \"prob\"` parameter is supplied to the `predict()` function.\n",
    "\n",
    "Using these methods, examine how the model's predicted 9am location probability varies from day-to-day. The model `locmodel` that you fit in the previous exercise is in your workspace.\n",
    "\n",
    "**Exercise**\n",
    "- Print the `locmodel` object to the console to view the computed a **priori** and conditional probabilities.\n",
    "- Use the `predict()` function similarly to the previous exercise, but with `type = \"prob\"` to see the predicted probabilities for Thursday at 9am.\n",
    "- Compare these to the predicted probabilities for Saturday at 9am."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>appointment</th><th scope=col>campus</th><th scope=col>home</th><th scope=col>office</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.01538462</td><td>0.1538462 </td><td>0.2307692 </td><td>0.6       </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{llll}\n",
       " appointment & campus & home & office\\\\\n",
       "\\hline\n",
       "\t 0.01538462 & 0.1538462  & 0.2307692  & 0.6       \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "appointment | campus | home | office | \n",
       "|---|\n",
       "| 0.01538462 | 0.1538462  | 0.2307692  | 0.6        | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     appointment campus    home      office\n",
       "[1,] 0.01538462  0.1538462 0.2307692 0.6   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>appointment</th><th scope=col>campus</th><th scope=col>home</th><th scope=col>office</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>3.838772e-05</td><td>0.0003838772</td><td>0.9980806   </td><td>0.001497121 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{llll}\n",
       " appointment & campus & home & office\\\\\n",
       "\\hline\n",
       "\t 3.838772e-05 & 0.0003838772 & 0.9980806    & 0.001497121 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "appointment | campus | home | office | \n",
       "|---|\n",
       "| 3.838772e-05 | 0.0003838772 | 0.9980806    | 0.001497121  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     appointment  campus       home      office     \n",
       "[1,] 3.838772e-05 0.0003838772 0.9980806 0.001497121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The 'naivebayes' package is loaded into the workspace\n",
    "# and the Naive Bayes 'locmodel' has been built\n",
    "\n",
    "# Examine the location prediction model\n",
    "#locmodel\n",
    "\n",
    "# Obtain the predicted probabilities for Thursday at 9am\n",
    "predict(locmodel, thursday9am, type = \"prob\")\n",
    "\n",
    "# Obtain the predicted probabilities for Saturday at 9am\n",
    "predict(locmodel, saturday9am, type = \"prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4) Understanding independence\n",
    "Understanding the idea of event independence will become important as you learn more about how \"naive\" Bayes got its name. Which of the following is true about independent events?\n",
    "\n",
    "Answer : Knowing the outcome of one event does not help predict the other.\n",
    "\n",
    "Correct: One event is independent of another if knowing one doesn't give you information about how likely the other is. For example, knowing if it's raining in New York doesn't help you predict the weather in San Francisco. The weather events in the two cities are independent of each other.\n",
    "\n",
    "\n",
    "\n",
    "### 2.2) (video) Understanding NB's \"naivety\"\n",
    "in the previous exercises, you built a simple Naive Bayes model that used historic location data to predict my future location to build a more sophisticated model, you might add additional data points to help inform the estimated probability of my location, but until now, we've only considered  conditional probability when a single event predicts another, adding more predictors complicates matters and is the reason why this method is called `naive` keep listening to find out way?\n",
    "\n",
    "Whit a single predictor, coditional probability is based on the overllap between the two events, as the Venn diagram here ilustrates, when we start to adding more events, the venn diagram can start to look a bit messy, here it is with 3 events, \n",
    "\n",
    "![](./Imagenes/understanding_naive1.jpg)\n",
    "\n",
    "imagine it with dozens o more! and, as confussing as this look to us , for numbers of reason it also become more inefficient for a computer to calculate the overlap, instead, the naive bayes algorithm uses a shortcut to approximated the conditional probability we hope to compute, rather than treating the problem as the intersection of all of the related events, the algorithm makes a so-called \"naive\" assumption about data, specifically, it assumes that events are independent, when events are independent, the joint probability can be computed by multiplying the individual probabilities, therefore , under the naive assumption, the algorithm doesn´t need to observe all of the possible intersections in the full diagram, instead, it simply multiplies the probabilities from a series of much simpler intersectations.\n",
    "\n",
    "Researches have found that although the naive assumption is rarely true in practice, the naive bayes model still performs admarbly on many real-world task, so there is little worry about potencial downside.\n",
    "\n",
    "There is one other potential issue to be aware of when building a naive bayes model, supposed you have a set of predictors, chained together under the naive assumption, suppose furthere that one of those events has never been observed previously in combination with outcome, for instance , I may never have gone into work on  a weekend, I may do this someday  in the future, I just haven't done so before, in this case, the venn frdiagram for work and weekend has no overlap, the joint probabilty of these two events is Zero and whenever zero multiplied in a chain ,the entire sequence becomes zero, for this reason the weekend event seems to have \"veto\" power over entire prediction, no matter how overwhelming th rest of the evidence, any predicted probability of work on a weekend will allways be zero, the solution to this problem involves adding a small number, usually `1` to each event and outocome combination to eliminate this \"veto\" power, this called the `Laplace correction` or the `Laplace estimator` after adding this corretion each venn diagram now has at least small bit of overlap, there is no longer any joint probabilty of zero as a result, there will be at least some predicted probability for every future outcome even if it has never been seen before , the naive bayes function you have used so for will let you set the laplace parameter.\n",
    "\n",
    "![](./Imagenes/understanding_naive2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1) Who are you calling naive?\n",
    "The Naive Bayes algorithm got its name because it makes a \"naive\" assumption about event independence.\n",
    "\n",
    "What is the purpose of making this assumption?\n",
    "\n",
    "Answer:the joint probability calculation is simpler for independent events\n",
    "\n",
    "\n",
    "#### 2.2.2) A more sophisticated location model\n",
    "The `locations` dataset records Brett's location every hour for 13 weeks. Each hour, the tracking information includes the `daytype` (weekend or weekday) as well as the `hourtype` (morning, afternoon, evening, or night).\n",
    "\n",
    "Using this data, build a more sophisticated model to see how Brett's predicted location not only varies by the day of week but also by the time of day. The dataset `locations` is already loaded in your workspace.\n",
    "\n",
    "**Exercise**\n",
    "- Use the R formula interface to build a model where location depends on both `daytype` and `hourtype`. Recall that the function `naive_bayes()` takes 2 arguments: formula and data.\n",
    "- Predict Brett's location on a weekday afternoon using the dataframe `weekday_afternoon` and the `predict()` function.\n",
    "- Do the same for a `weekday_evening`.\n",
    "\n",
    "*Answer*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"naive_bayes(): Feature daytype - zero probabilities are present. Consider Laplace smoothing.\"Warning message:\n",
      "\"naive_bayes(): Feature hourtype - zero probabilities are present. Consider Laplace smoothing.\"Warning message:\n",
      "\"predict.naive_bayes(): More features in the newdata are provided as there are probability tables in the object. Calculation is performed based on features to be found in the tables.\""
     ]
    },
    {
     "data": {
      "text/html": [
       "office\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<ol class=list-inline>\n",
       "\t\t<li>'appointment'</li>\n",
       "\t\t<li>'campus'</li>\n",
       "\t\t<li>'home'</li>\n",
       "\t\t<li>'office'</li>\n",
       "\t\t<li>'restaurant'</li>\n",
       "\t\t<li>'store'</li>\n",
       "\t\t<li>'theater'</li>\n",
       "\t</ol>\n",
       "</details>"
      ],
      "text/latex": [
       "office\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item 'appointment'\n",
       "\\item 'campus'\n",
       "\\item 'home'\n",
       "\\item 'office'\n",
       "\\item 'restaurant'\n",
       "\\item 'store'\n",
       "\\item 'theater'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "office\n",
       "**Levels**: 1. 'appointment'\n",
       "2. 'campus'\n",
       "3. 'home'\n",
       "4. 'office'\n",
       "5. 'restaurant'\n",
       "6. 'store'\n",
       "7. 'theater'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] office\n",
       "Levels: appointment campus home office restaurant store theater"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"predict.naive_bayes(): More features in the newdata are provided as there are probability tables in the object. Calculation is performed based on features to be found in the tables.\""
     ]
    },
    {
     "data": {
      "text/html": [
       "home\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<ol class=list-inline>\n",
       "\t\t<li>'appointment'</li>\n",
       "\t\t<li>'campus'</li>\n",
       "\t\t<li>'home'</li>\n",
       "\t\t<li>'office'</li>\n",
       "\t\t<li>'restaurant'</li>\n",
       "\t\t<li>'store'</li>\n",
       "\t\t<li>'theater'</li>\n",
       "\t</ol>\n",
       "</details>"
      ],
      "text/latex": [
       "home\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item 'appointment'\n",
       "\\item 'campus'\n",
       "\\item 'home'\n",
       "\\item 'office'\n",
       "\\item 'restaurant'\n",
       "\\item 'store'\n",
       "\\item 'theater'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "home\n",
       "**Levels**: 1. 'appointment'\n",
       "2. 'campus'\n",
       "3. 'home'\n",
       "4. 'office'\n",
       "5. 'restaurant'\n",
       "6. 'store'\n",
       "7. 'theater'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] home\n",
       "Levels: appointment campus home office restaurant store theater"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weekday_afternoon<-data.frame (daytype = factor(c(\"weekday\"), levels = c (\"weekday\",\"weekend\")),\n",
    "                               hourtype = factor(c(\"afternoon\"), levels = c (\"afternoon\",\"evening\",\"morning\",\"night\")),\n",
    "                               daytype = factor(c(\"office\"), levels = c (\"appointment\",\"campus\",\"home\",\"office\",\"restaurant\",\"store\",\"theater\"))\n",
    "                              )\n",
    "\n",
    "weekday_evening<-data.frame (daytype = factor(c(\"weekday\"), levels = c (\"weekday\",\"weekend\")),\n",
    "                               hourtype = factor(c(\"evening\"), levels = c (\"afternoon\",\"evening\",\"morning\",\"night\")),\n",
    "                               daytype = factor(c(\"home\"), levels = c (\"appointment\",\"campus\",\"home\",\"office\",\"restaurant\",\"store\",\"theater\"))\n",
    "                              )\n",
    "\n",
    "# The 'naivebayes' package is loaded into the workspace already\n",
    "\n",
    "# Build a NB model of location\n",
    "locmodel <- naive_bayes(location ~ daytype + hourtype, locations)\n",
    "\n",
    "# Predict Brett's location on a weekday afternoon\n",
    "predict(locmodel, weekday_afternoon)\n",
    "\n",
    "# Predict Brett's location on a weekday evening\n",
    "predict(locmodel, weekday_evening)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3) Preparing for unforeseen circumstances\n",
    "While Brett was tracking his location over 13 weeks, he never went into the office during the weekend. Consequently, the joint probability of `P(office and weekend) = 0`.\n",
    "\n",
    "Explore how this impacts the predicted probability that Brett may go to work on the weekend in the future. Additionally, you can see how using the `Laplace correction` will allow a small chance for these types of unforeseen circumstances.\n",
    "\n",
    "The model `locmodel` is already in your workspace, along with the dataframe `weekend_afternoon`.\n",
    "\n",
    "**Exercise**\n",
    "- Use the `locmodel` to output predicted probabilities for a weekend afternoon by using the `predict()` function. Remember to set the type argument.\n",
    "- Create a new naive Bayes model with the Laplace smoothing parameter set to `1`. You can do this by setting the laplace argument in your call to `naive_bayes()`. Save this as `locmodel2`.\n",
    "- See how the new predicted probabilities compare by using the `predict()` function on your new model.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"predict.naive_bayes(): More features in the newdata are provided as there are probability tables in the object. Calculation is performed based on features to be found in the tables.\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>appointment</th><th scope=col>campus</th><th scope=col>home</th><th scope=col>office</th><th scope=col>restaurant</th><th scope=col>store</th><th scope=col>theater</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.02462883  </td><td>0.0004802622</td><td>0.8439145   </td><td>0.003349521 </td><td>0.1111338   </td><td>0.01641922  </td><td>7.38865e-05 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lllllll}\n",
       " appointment & campus & home & office & restaurant & store & theater\\\\\n",
       "\\hline\n",
       "\t 0.02462883   & 0.0004802622 & 0.8439145    & 0.003349521  & 0.1111338    & 0.01641922   & 7.38865e-05 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "appointment | campus | home | office | restaurant | store | theater | \n",
       "|---|\n",
       "| 0.02462883   | 0.0004802622 | 0.8439145    | 0.003349521  | 0.1111338    | 0.01641922   | 7.38865e-05  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     appointment campus       home      office      restaurant store     \n",
       "[1,] 0.02462883  0.0004802622 0.8439145 0.003349521 0.1111338  0.01641922\n",
       "     theater    \n",
       "[1,] 7.38865e-05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"predict.naive_bayes(): More features in the newdata are provided as there are probability tables in the object. Calculation is performed based on features to be found in the tables.\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>appointment</th><th scope=col>campus</th><th scope=col>home</th><th scope=col>office</th><th scope=col>restaurant</th><th scope=col>store</th><th scope=col>theater</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.02013872 </td><td>0.006187715</td><td>0.8308154  </td><td>0.007929249</td><td>0.1098743  </td><td>0.01871085 </td><td>0.006343697</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lllllll}\n",
       " appointment & campus & home & office & restaurant & store & theater\\\\\n",
       "\\hline\n",
       "\t 0.02013872  & 0.006187715 & 0.8308154   & 0.007929249 & 0.1098743   & 0.01871085  & 0.006343697\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "appointment | campus | home | office | restaurant | store | theater | \n",
       "|---|\n",
       "| 0.02013872  | 0.006187715 | 0.8308154   | 0.007929249 | 0.1098743   | 0.01871085  | 0.006343697 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     appointment campus      home      office      restaurant store     \n",
       "[1,] 0.02013872  0.006187715 0.8308154 0.007929249 0.1098743  0.01871085\n",
       "     theater    \n",
       "[1,] 0.006343697"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weekend_afternoon<-data.frame (daytype = factor(c(\"weekend\"), levels = c (\"weekday\",\"weekend\")),\n",
    "                               hourtype = factor(c(\"afternoon\"), levels = c (\"afternoon\",\"evening\",\"morning\",\"night\")),\n",
    "                               daytype = factor(c(\"home\"), levels = c (\"appointment\",\"campus\",\"home\",\"office\",\"restaurant\",\"store\",\"theater\"))\n",
    "                              )\n",
    "\n",
    "\n",
    "# The 'naivebayes' package is loaded into the workspace already\n",
    "# The Naive Bayes location model (locmodel) has already been built\n",
    "\n",
    "# Observe the predicted probabilities for a weekend afternoon\n",
    "predict(locmodel, weekend_afternoon, type = \"prob\")\n",
    "\n",
    "# Build a new model using the Laplace correction\n",
    "locmodel2 <- naive_bayes(location ~ daytype + hourtype, locations, laplace = 1)\n",
    "\n",
    "# Observe the new predicted probabilities for a weekend afternoon\n",
    "predict(locmodel2, weekend_afternoon, type = \"prob\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4) Understanding the Laplace correction\n",
    "By default, the `naive_bayes()` function in the naivebayes package does not use the Laplace correction. What is the risk of leaving this parameter unset?\n",
    "\n",
    "Answer : Some potential outcomes may be predicted to be impossible, Correct! The small probability added to every outcome ensures that they are all possible even if never previously observed.\n",
    "\n",
    "### 2.3 (video)  Applying Naive Bayes to other problems\n",
    "\n",
    "Naive bayes tends to work well on problem where the information from multiple attributes needs to be considered simultaneously  and evaluated as a whole.\n",
    "\n",
    "Historically, Naive Bayes has also been frequently used for classifying text data, like identifying whether or no an email is spam \n",
    "\n",
    "Other real cases are:\n",
    "\n",
    "Considerer the fact that Naive Bayes makes predictions by cumpiting conditional probabilities of events and outcomes, beginning with a tabular dataset, it builds frequency tables that count the number of times each event overlaps with the outcome of interest, the probabilities are then multipled, naively, in a chain of all the events a conequence of this approach is that each of the predictors used in Naive Bayes typically comprises a set of categories.\n",
    "\n",
    "Numeric properties, like age or time-of-day, are difficult for Naive Bayes to use as-is without knowing more about the properties of the data, similary, unstructured text data also defies categorization.\n",
    "\n",
    "Thus, it´s generally necessary to prepare these type of data before using them with Naive Bayes. \n",
    "\n",
    "A technique called `binning` is a simple method for creating categories from numeric data, the idea is to divide a range of numers into a series of sets called `bins` \n",
    "\n",
    "For instance, you might divide a numbers into bins based on percentiles by creating a category for the bottom 25%, the next 25%, and so on, perhaps a better approach is to group ranges of values into meanningful bins, for instancem you might group times into categories like afternoon and evening, an temperature readings into values like hot, warm and cold, you can use R's data preparation functions to recode data this way.\n",
    "\n",
    "Text documents are considerer unstructured data because they do not conform to the typical table or spreadsheet format of most datasets, a common process for adding structure to text data uses a model called bag-of-words, the bag-of-words model does not consider word order, grammar or semantics, it simply creates an events for each word that appears in a particular colletion of text documents \n",
    "\n",
    "For example, the bag-of-words for this document on Naive Bayes would include events for words like \"naive\" and \"bayes\" and \"understanding\" , in spreadsheet form, this result in a wide table where the rows are documents and the columns are words that may appear in the documents, each spredsheet cell indicates whether or not the word appeared in that document, when the Naive Bayes algorithm is applied to the bag of words, it can estimate the probabilty of the outcome given the evidence provided by words in the text, For Instance, a document with the words \"viagra\" and \"prescription\" is more likely to be spam that a document with the words \"naive\" and \"bayes\" \n",
    "\n",
    "**Note you can learn more about this in Datacamp's course of text minning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Logistic Regression\n",
    "Logistic regression involves fitting a curve to numeric data to make predictions about binary events. Arguably one of the most widely used machine learning methods, this chapter will provide an overview of the technique while illustrating how to apply it to fundraising data.\n",
    "\n",
    "\n",
    "### 3.1) (video) Making binary predictions with regression\n",
    "if you've spent any time at all studyng data science, you are likely to have encountered regression analysis, which is a branch of statistics interested in modeling numerica relationship within data, regression methods are perhaps the single most common form of machine learning, the technique can be adapted to virtually any type of problem in any domain, in this video, you will see how this methods can be used to classify a binary outocome, later you will use what you learn to predict whether or not someone will donate to charity.\n",
    "\n",
    "in its most basic form, regression involves predicting an outocome \"Y\" using one o more predictors, labeled as \"X's\" variables, the \"Y\" variable is know as the dependent variable, as it seems to depend upon the \"X\"'s.\n",
    "\n",
    "Suppose you have a numeric \"Y\", which you plot versus a numeric \"X\"  term, lineal regression involves fitting the straight  line to this data that best captures the relationship between \"X\" and \"Y\" terms, but suppose you have a binary Y outcome instead that, somethig can take onlye  1-0 values like \"Donate\" or \"No Donate\" , constructing a plot of Y versus X, the points fall in two flat rows rather spread along the diagonal, you can still apply a straight line tothe data, but this doesn't seems to fit very well, addtionally,  for some values of X's the model will predict values less then  zero or greater than one, this obviously not ideal, but now imagine the same binary outcome, but rather than trying to model it with a straight line, we use a curve instead, this is the idea behind logistic regression. \n",
    "\n",
    "It type of S-shaped  curve called a logistic function has the property that for any  input value of X, the output is always between 0-1 just like a probability, the greater this probability, the more likely the outcome is to be the one labeled 1.\n",
    "\n",
    "in R, lofistic regression uses the `glm` function with the sintax as show here, once the model has been built, it can be used to estimate probabilities, supplying the  `type = response` parameter to the predict function produces the predicted probabilities which are easier to interpret than the dafult log-odds values \n",
    "\n",
    "To make predictions, the probabilities must be converted into the outcome of interest using  an `ifelse` step\n",
    "\n",
    "Sometime you may need to set this threshold higher or lower to make the model more or less aggressive. \n",
    "\n",
    "![](./Imagenes/LogisticRegression1.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1) Building simple logistic regression models\n",
    "The `donors` dataset contains 93,462 examples of people mailed in a fundraising solicitation for paralyzed military veterans. The `donated` column is `1` if the person made a donation in response to the mailing and `0` otherwise. This binary outcome will be the dependent variable for the logistic regression model.\n",
    "\n",
    "The remaining columns are features of the prospective donors that may influence their donation behavior. These are the model's independent variables.\n",
    "\n",
    "When building a regression model, it is often helpful to form a hypothesis about which independent variables will be predictive of the dependent variable. The `bad_address` column, which is set to `1` for an invalid mailing address and `0` otherwise, seems like it might reduce the chances of a donation. Similarly, one might suspect that religious interest (`interest_religion`) and interest in veterans affairs (`interest_veterans`) would be associated with greater charitable giving.\n",
    "\n",
    "In this exercise, you will use these three factors to create a simple model of donation behavior. The dataset donors is available in your workspace.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "- Examine `donors` using the `str()` function.\n",
    "- Count the number of occurrences of each level of the donated variable using the `table()` function.\n",
    "- Fit a logistic regression model using the formula interface and the three independent variables described above.\n",
    "    - Call `glm()` with the formula as its first argument and the dataframe as the data argument.\n",
    "    - Save the result as `donation_model`.\n",
    "- Summarize the model object with `summary()`.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     1 \n",
       "88751  4711 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = donated ~ bad_address + interest_veterans + interest_religion, \n",
       "    family = \"binomial\", data = donors)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-0.3480  -0.3192  -0.3192  -0.3192   2.5678  \n",
       "\n",
       "Coefficients:\n",
       "                  Estimate Std. Error  z value Pr(>|z|)    \n",
       "(Intercept)       -2.95139    0.01652 -178.664   <2e-16 ***\n",
       "bad_address       -0.30780    0.14348   -2.145   0.0319 *  \n",
       "interest_veterans  0.11009    0.04676    2.354   0.0186 *  \n",
       "interest_religion  0.06724    0.05069    1.327   0.1847    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 37330  on 93461  degrees of freedom\n",
       "Residual deviance: 37316  on 93458  degrees of freedom\n",
       "AIC: 37324\n",
       "\n",
       "Number of Fisher Scoring iterations: 5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine the dataset to identify potential independent variables\n",
    "#str(donors)\n",
    "\n",
    "# Explore the dependent variable\n",
    "table(donors$donated)\n",
    "\n",
    "# Build the donation model\n",
    "donation_model <- glm(donated ~ bad_address + interest_veterans + interest_religion, \n",
    "                      data = donors, family = \"binomial\")\n",
    "\n",
    "# Summarize the model results\n",
    "summary(donation_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2) Making a binary prediction\n",
    "In the previous exercise, you used the `glm()` function to build a logistic regression model of donor behavior. As with many of R's machine learning methods, you can apply the `predict()` function to the model object to forecast future behavior. By default, `predict()` outputs predictions in terms of log odds unless `type = \"response\"` is specified. This converts the log odds to probabilities.\n",
    "\n",
    "Because a logistic regression model estimates the probability of the outcome, it is up to **you to determine the threshold at which the probability implies action**. One must balance the extremes of being too cautious versus being too aggressive. For example, if you were to solicit only the people with a 99% or greater donation probability, you may miss out on many people with lower estimated probabilities that still choose to donate. This balance is particularly important to consider for severely imbalanced outcomes, such as in this dataset where donations are relatively rare.\n",
    "\n",
    "The dataset `donors` and the model `donation_model` are already loaded in your workspace.\n",
    "\n",
    "**Exercise**\n",
    "- Use the `predict()` function to estimate each person's donation probability. Use the `type` argument to get probabilities. Assign the predictions to a new column called `donation_prob`.\n",
    "- Find the actual probability that an average person would donate by passing the `mean()` function the appropriate column of the donors dataframe.\n",
    "- Use `ifelse()` to predict a donation if their predicted donation probability is greater than average. Assign the predictions to a new column `called donation_pred`.\n",
    "- Use the `mean()` function to calculate the model's accuracy.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.0504055124007618"
      ],
      "text/latex": [
       "0.0504055124007618"
      ],
      "text/markdown": [
       "0.0504055124007618"
      ],
      "text/plain": [
       "[1] 0.05040551"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.794815005028782"
      ],
      "text/latex": [
       "0.794815005028782"
      ],
      "text/markdown": [
       "0.794815005028782"
      ],
      "text/plain": [
       "[1] 0.794815"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Estimate the donation probability\n",
    "donors$donation_prob <- predict(donation_model, type = \"response\")\n",
    "\n",
    "# Find the donation probability of the average prospect\n",
    "mean(donors$donated)\n",
    "\n",
    "# Predict a donation if probability of donation is greater than average (0.0504)\n",
    "donors$donation_pred <- ifelse(donors$donation_prob  > 0.0504, 1, 0)\n",
    "\n",
    "# Calculate the model's accuracy\n",
    "mean(donors$donated == donors$donation_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 ) The limitations of accuracy\n",
    "In the previous exercise, you found that the logistic regression model made a correct prediction nearly 80% of the time. Despite this relatively high accuracy, the result is misleading due to the rarity of outcome being predicted.\n",
    "\n",
    "The donors dataset is available in your workspace. What would the accuracy have been if a model had simply predicted \"no donation\" for each person? it´s 95% \n",
    "\n",
    "Correct! With an accuracy of only 80%, the model is actually performing WORSE than if it were to predict non-donor for every record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     1 \n",
       "88751  4711 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "         0          1 \n",
       "0.94959449 0.05040551 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(donors$donated)\n",
    "prop.table(table(donors$donated))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 (video) Model performance tradeoffs\n",
    "As the previous exercises illustrated, rare events create challenges for classification models, when one outcome is very rare, predicting the opposite can result in a very high accuracy, this was the case in the donation dataset, because only about 5% of people were donors, predicting non-donation resulted in an overall accurancy of 95% but an accurency of zero on the outcome that mattered themost, the donotions,in cases like these, it may be necessary to to sacrifice a bit of overall acurrancy in order to better target the outcome of interest.\n",
    "\n",
    "A visualization called an ROC Curves provides a way to better understand a model's ability to distinguish between positive or negative predictions the outcome of interest versus all others, to understand how it works, Imagine that you were working on a project where the positive outcome is X and the negative outcome is O, the classifier is trying to distinguish between the two groups, if the classifier is poor, the X's and O's will remain very mixed as show here (Image 1) the ROC curve depicts the relationship between the percentage of possitive examples as it relates to the  percentage of the others outcomes. Here, because the X's and O´s are even, the ROC curve makes a diagonal line showing that the proportion of interesting examples rises evenly with the proportion of the negative examples.\n",
    "\n",
    "On the other hand, suppose we have a machine learning model that is able to sort the examples of interest so they appear at the front of the dataset, the outcomes might be arranged as shown here, whit more X' on the left that on the right, when the ROC curve is drawn for this arrangment, it is no longer on the diagonal because the model is able to identify several positive examples for each negative example it accidentally priorized.\n",
    "\n",
    "The diagonal line is the baseline performance for a very poor model, the further another curve is away from this, the better it is performing , conversely, a model that is very close to the diagonal line is not performing very well at all.\n",
    "\n",
    "To quantify this performance, a measurement called AUC or area under the courve, is used, the AUC literally measures the area under the ROC curve, the baseline model that is no better than random chance has an AUC of .5 because it's divide one by one unit square perfectly in half, a perfect model has AUC of 1 with a curve all the way in the upper-left of the square, most real-world results are somewhere in between, generally speaking, the closer the AUC is to 1 the better, but there are some cases where AUC can be misleading.\n",
    "\n",
    "![](./Imagenes/LogisticRegression2.jpg)\n",
    "\n",
    "Curves of varying shapes can have the same AUC value, for this reason, it is important to look not only at the AUC but also how the  shape of each curve indicates how a model is performing across the range of predictions.\n",
    "\n",
    "For example, one model may do extremely well at identifying a few easy cases at first but perform poorly on more difficult cases, another model may do just the opposite,as this figure shows, both may end up with the same AUC, ROC curve are an iportant toll for comparing models and selection the best model for your specific project needs, when used with a single model, it can help to visualize the tradeoff bewteen true positive and false positives for the outcome of interest.\n",
    "\n",
    "![](./Imagenes/LogisticRegression3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1) Calculating ROC Curves and AUC\n",
    "The previous exercises have demonstrated that accuracy is a very misleading measure of model performance on imbalanced datasets. Graphing the model's performance better illustrates the tradeoff between a model that is overly agressive and one that is overly passive.\n",
    "\n",
    "In this exercise you will create a ROC curve and compute the area under the curve (AUC) to evaluate the logistic regression model of donations you built earlier.\n",
    "\n",
    "The dataset `donors` with the column of predicted probabilities, `donation_prob` ,is already loaded in your workspace.\n",
    "\n",
    "**Exercise**\n",
    "- Load the `pROC` package.\n",
    "- Create a ROC curve with `roc()` and the columns of actual and predicted donations. Store the result as `ROC`.\n",
    "- Use plot() to draw the ROC object. Specify `col = \"blue\"` to color the curve blue.\n",
    "- Compute the area under the curve with `auc()`\n",
    "\n",
    "*Answer*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = 0, case = 1\n",
      "Setting direction: controls < cases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.510166058120372"
      ],
      "text/latex": [
       "0.510166058120372"
      ],
      "text/markdown": [
       "0.510166058120372"
      ],
      "text/plain": [
       "Area under the curve: 0.5102"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAANlBMVEUAAAAAAP9NTU1oaGh8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\n/getjeu9er8w3gAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pROC package\n",
    "library(pROC)\n",
    "\n",
    "# Create a ROC curve\n",
    "ROC <- roc(donors$donated, donors$donation_prob)\n",
    "\n",
    "\n",
    "# Plot the ROC curve\n",
    "plot(ROC, col = \"blue\")\n",
    "\n",
    "# Calculate the area under the curve (AUC)\n",
    "auc(ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 (video) Dummy variables, missing data, and interactions\n",
    "\n",
    "All of the predictors used in a regression analysis must be numeric, this means that all categorical data must be represented as a number, missing data also poses a problem, as the empty value can´not be used to make a predictions.\n",
    "\n",
    "In chapter 1, you learned about dummy coding, which creates a set of binary 1-0 variables that represent each category except one that serves as the reference group, dummy coding is the most common method for handling categorical data for logistic regression, The `glm` function will automatically dummy code any factors type variables used in the model, simply apply the `factor` function to the data, e.g:\n",
    "\n",
    "    #create gender factor\n",
    "    my_data$gender<-factor(my_data$gender, levels = c(0, 1, 2),  labels = c(\"Male\", \"Femele\", \"Other\"))\n",
    "\n",
    "Keep in mid that you may run into a case where a cetegorical feature is represented with numbers, such as 1,2,3 for hot,warm and cold, even in the case, it may be advisable to conber this to a factor, this allows each category to have a unique impact on the outcome.\n",
    "\n",
    "Now, what happend with the `missing data`?, well by default, the regression model will exclude any  observations with missing values on its predictors, this means a big deal for a small amounts of missing data, but can be very quicly become a much large problem, with categorical missing data, a missing value can be treated like any other category, you might construct categories for male, female, other and missing but when a numerical values is missing, the solutions is less clear, one potencial solution uses a technique called `imputation` this fill or imputes, themissing value with a guess about what the value may be, a very simple strategy is called `mean imputations` which you as you might expect, imputes the average.\n",
    "\n",
    "Because records having missing data may differ systematically for those without, a binary 1/0 missing value indicator can be added to model the fact that a value was imputed, sometimes, this becomes one of the model's most important predictos.\n",
    "\n",
    "Note: this method is not appropriate for every regression application, more sophisticatef forms of imputation use models to predict the missing data based on the non-missing values \n",
    "\n",
    "An `interacion effects` considers the fact that two predictors, when combined, may have a different impact on an outcome than the sum of their separate individual impacts, their combinations may strengthen, weaken or completely eliminate the impact of the individual predictors, as illustrated here, the R formula interface uses the multiplication symbol to create an interaction between two predictors \n",
    "\n",
    "    #interaction of obesity and smoking\n",
    "    glm(disease ~ obesity*smoking, data = health, family = binomial)\n",
    "    \n",
    "#### 3.3.1) Coding categorical features\n",
    "Sometimes a dataset contains numeric values that represent a categorical feature.\n",
    "\n",
    "In the donors dataset, `wealth_rating` uses numbers to indicate the donor's wealth level:\n",
    "\n",
    "    0 = Unknown\n",
    "    1 = Low\n",
    "    2 = Medium\n",
    "    3 = High\n",
    "\n",
    "This exercise illustrates how to prepare this type of categorical feature and the examines its impact on a logistic regression model. The dataframe `donors` is loaded in your workspace.   \n",
    "\n",
    "**Exercise**\n",
    "- Create a factor from the numeric `wealth_rating` with labels as shown above by passing the factor() function the column you want to convert, the individual levels, and the labels.\n",
    "- Use `relevel()` to change the reference category to `Medium`. The first argument should be your factor column.\n",
    "- Build a logistic regression model using the column `wealth_rating` to predict `donated` and display the result with `summary()`.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = donated ~ wealth_rating, family = \"binomial\", data = donors)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-0.3320  -0.3243  -0.3175  -0.3175   2.4582  \n",
       "\n",
       "Coefficients:\n",
       "                     Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)          -2.91894    0.03614 -80.772   <2e-16 ***\n",
       "wealth_ratingUnknown -0.04373    0.04243  -1.031    0.303    \n",
       "wealth_ratingLow     -0.05245    0.05332  -0.984    0.325    \n",
       "wealth_ratingHigh     0.04804    0.04768   1.008    0.314    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 37330  on 93461  degrees of freedom\n",
       "Residual deviance: 37323  on 93458  degrees of freedom\n",
       "AIC: 37331\n",
       "\n",
       "Number of Fisher Scoring iterations: 5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the wealth rating to a factor\n",
    "donors$wealth_rating <- factor(donors$wealth_rating, levels = c(0, 1, 2, 3), labels = c(\"Unknown\", \"Low\", \"Medium\", \"High\"))\n",
    "\n",
    "# Use relevel() to change reference category\n",
    "donors$wealth_rating <- relevel(donors$wealth_rating, ref = \"Medium\")\n",
    "\n",
    "# See how our factor coding impacts the model\n",
    "summary(glm(donated ~ wealth_rating, data = donors, family = \"binomial\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2) Handling missing data\n",
    "Some of the prospective `donors` have missing age data. Unfortunately, R will exclude any cases with `NA` values when building a regression model.\n",
    "\n",
    "One workaround is to replace, or impute, the missing values with an estimated value. After doing so, you may also create a missing data indicator to model the possibility that cases with missing data are different in some way from those without.\n",
    "\n",
    "The dataframe donors is loaded in your workspace.\n",
    "\n",
    "**Exercise**\n",
    "- Use `summary()` on `donors$age` to find the average age of prospects with non-missing data.\n",
    "- Use `ifelse()` and the test `is.na(donors$age)` to impute the average (rounded to 2 decimal places) for cases with missing age. Be sure to also ignore `NAs`.\n",
    "- Create a binary dummy variable named `missing_age` indicating the presence of missing data using another `ifelse()` call and the same test.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n",
       "   1.00   48.00   62.00   61.65   75.00   98.00   22546 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the average age among non-missing values\n",
    "summary(donors$age)\n",
    "\n",
    "# Impute missing age values with the mean age\n",
    "donors$imputed_age <- ifelse( is.na(donors$age) == TRUE, 61.65, donors$age )\n",
    "\n",
    "# Create missing value indicator for age\n",
    "donors$missing_age <- ifelse( is.na(donors$age) == TRUE, 1,0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a more sophisticated model\n",
    "One of the best predictors of future giving is a history of recent, frequent, and large gifts. In marketing terms, this is known as R/F/M:\n",
    "\n",
    "    Recency\n",
    "    Frequency\n",
    "    Money\n",
    "\n",
    "Donors that haven't given both recently and frequently may be especially likely to give again; in other words, the combined impact of recency and frequency may be greater than the sum of the separate effects.\n",
    "\n",
    "Because these predictors together have a greater impact on the dependent variable, their joint effect must be modeled as an interaction. The `donors` dataset has been loaded for you.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "- Create a logistic regression model of `donated` as a function of `money` plus the interaction of `recency` and `frequency`. Use `*` to add the interaction term.\n",
    "- Examine the model's `summary()` to confirm the interaction effect was added.\n",
    "- Save the model's predicted probabilities as `rfm_prob`. Use the `predict()` function, and remember to set the type argument.\n",
    "- Plot a ROC curve by using the function `roc()`. Remember, this function takes the column of outcomes and the vector of predictions.\n",
    "- Compute the AUC for the new model with the function `auc()` and compare performance to the simpler model.\n",
    "\n",
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = donated ~ money + recency * frequency, family = \"binomial\", \n",
       "    data = donors)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-0.3696  -0.3696  -0.2895  -0.2895   2.7924  \n",
       "\n",
       "Coefficients:\n",
       "                                  Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)                       -3.01142    0.04279 -70.375   <2e-16 ***\n",
       "moneyMEDIUM                        0.36186    0.04300   8.415   <2e-16 ***\n",
       "recencyLAPSED                     -0.86677    0.41434  -2.092   0.0364 *  \n",
       "frequencyINFREQUENT               -0.50148    0.03107 -16.143   <2e-16 ***\n",
       "recencyLAPSED:frequencyINFREQUENT  1.01787    0.51713   1.968   0.0490 *  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 37330  on 93461  degrees of freedom\n",
       "Residual deviance: 36938  on 93457  degrees of freedom\n",
       "AIC: 36948\n",
       "\n",
       "Number of Fisher Scoring iterations: 6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = 0, case = 1\n",
      "Setting direction: controls < cases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.578489371262516"
      ],
      "text/latex": [
       "0.578489371262516"
      ],
      "text/markdown": [
       "0.578489371262516"
      ],
      "text/plain": [
       "Area under the curve: 0.5785"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAABNTU1oaGh8fHyM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\nAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pROC package\n",
    "library(pROC)\n",
    "\n",
    "library(repr)\n",
    "# Change plot size to 4 x 3\n",
    "    options(repr.plot.width=4, repr.plot.height=3)\n",
    "\n",
    "\n",
    "\n",
    "# Build a recency, frequency, and money (RFM) model\n",
    "rfm_model <- glm(donated ~ money + recency*frequency, data = donors, family = \"binomial\")\n",
    "\n",
    "# Summarize the RFM model to see how the parameters were coded\n",
    "summary(rfm_model)\n",
    "\n",
    "# Compute predicted probabilities for the RFM model\n",
    "rfm_prob <- predict(rfm_model, type = \"response\")\n",
    "\n",
    "# Plot the ROC curve and find AUC for the new model\n",
    "library(pROC)\n",
    "ROC <- roc(donors$donated, rfm_prob)\n",
    "plot(ROC, col = \"red\")\n",
    "auc(ROC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 (video) Automatic feature selection\n",
    "\n",
    "unlike some machine learning methods, regression typically asks the human to specify the model's predictors ahead of time, thus, each of the donation models you have built so far required a little bit of found-raising subject-metter expertise to identify the variables that may be predictive of donations, sometimes you may not have this type insight ahead of time, you may not know what all of the predictors mean, or you may have so many predictors there is no easy way to sort through them all.\n",
    "\n",
    "A process called automatic feature selection can be used here! but as you will soon see , whti this great power comes a great responsability to apply it carefully.\n",
    "\n",
    "`Stepwise Regression` involves building a regression models, step-by-step evaluating each predictor to see which ones add value to the final model, a procedure called `backward deletion` begins with a model containing all of the predictors and then checks to see what happens when each one of the predictors is removed from the model, if removing a predictor does not substantially impact the model's ability to predict the outcome, then can be safely deleted at each step, the predictor that impacts the model the least is removed, assuming, of course, it has minimal impact\n",
    "\n",
    "this continues step by step until only important predictors remain, the same idea applied in the other direction is called `forward selection` begining with a model containing no predictors, it examines each potential predictor to see wich one, if any, offers the greatest improvent to the model's predictive power, predictors are added step by step until no new predictors add substantial value to the model.\n",
    "\n",
    "![](./Imagenes/LogisticRegression4.jpg)\n",
    "\n",
    "Note: It´s possible that the two methods could come to completely diffrent conclusion about he most importat predictors. \n",
    "\n",
    "This is just one of the potential caveats of stepwise regression, other is that neither is guaranteed to find the best possible model \n",
    "\n",
    "#### 3.4.1) The dangers of stepwise regression\n",
    "In spite of its utility for feature selection, stepwise regression is not frequently used in disciplines outside of machine learning due to some important caveats. Which of these is NOT one of these concerns?\n",
    "\n",
    "- It is not guaranteed to find the best possible model\n",
    "- A stepwise model's predictions can not be trusted (answer)\n",
    "- The stepwise regression procedure violates some statistical assumptions\n",
    "- It can result in a model that makes little sense in the real world\n",
    "\n",
    "#### 3.4.2) Building a stepwise regression model\n",
    "In the absence of subject-matter expertise, **stepwise regression** can assist with the search for the most important predictors of the outcome of interest.\n",
    "\n",
    "In this exercise, you will use a forward stepwise approach to add predictors to the model one-by-one until no additional benefit is seen. The `donors` dataset has been loaded for you.\n",
    "\n",
    "**Exercise**\n",
    "- Use the R formula interface with `glm()` to specify the base model with no predictors. Set the explanatory variable equal to `1`.\n",
    "- Use the R formula interface again with `glm()` to specify the model with all predictors.\n",
    "- Apply `step()` to these models to perform forward stepwise regression. Set the first argument to `null_model` and set `direction = \"forward\"`. This might take a while (up to 10 or 15 seconds) as your computer has to fit quite a few different models to perform stepwise selection.\n",
    "- Create a vector of predicted probabilities using the `predict()` function.\n",
    "- Plot the ROC curve with `roc()` and `plot()` and compute the AUC of the stepwise model with `auc()`.\n",
    "\n",
    "\n",
    "*Answer*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  AIC=37332.13\n",
      "donated ~ 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, :\n",
      "\"using the 70916/93462 rows from a combined fit\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Df Deviance   AIC\n",
      "+ frequency          1    28502 37122\n",
      "+ money              1    28621 37241\n",
      "+ wealth_rating      1    28705 37326\n",
      "+ has_children       1    28705 37326\n",
      "+ age                1    28707 37328\n",
      "+ interest_veterans  1    28709 37330\n",
      "+ catalog_shopper    1    28710 37330\n",
      "+ pet_owner          1    28711 37331\n",
      "<none>                    28714 37332\n",
      "+ interest_religion  1    28712 37333\n",
      "+ recency            1    28713 37333\n",
      "+ bad_address        1    28714 37334\n",
      "+ veteran            1    28714 37334\n",
      "\n",
      "Step:  AIC=37024.77\n",
      "donated ~ frequency\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, :\n",
      "\"using the 70916/93462 rows from a combined fit\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Df Deviance   AIC\n",
      "+ money              1    28441 36966\n",
      "+ wealth_rating      1    28493 37018\n",
      "+ has_children       1    28494 37019\n",
      "+ interest_veterans  1    28498 37023\n",
      "+ catalog_shopper    1    28499 37024\n",
      "+ age                1    28499 37024\n",
      "+ pet_owner          1    28499 37024\n",
      "<none>                    28502 37025\n",
      "+ interest_religion  1    28501 37026\n",
      "+ recency            1    28501 37026\n",
      "+ bad_address        1    28502 37026\n",
      "+ veteran            1    28502 37027\n",
      "\n",
      "Step:  AIC=36949.71\n",
      "donated ~ frequency + money\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, :\n",
      "\"using the 70916/93462 rows from a combined fit\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Df Deviance   AIC\n",
      "+ wealth_rating      1    28431 36942\n",
      "+ has_children       1    28432 36943\n",
      "+ interest_veterans  1    28438 36948\n",
      "+ catalog_shopper    1    28438 36949\n",
      "+ age                1    28439 36949\n",
      "+ pet_owner          1    28439 36949\n",
      "<none>                    28441 36950\n",
      "+ interest_religion  1    28440 36951\n",
      "+ recency            1    28441 36951\n",
      "+ bad_address        1    28441 36951\n",
      "+ veteran            1    28441 36952\n",
      "\n",
      "Step:  AIC=36945.26\n",
      "donated ~ frequency + money + wealth_rating\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, :\n",
      "\"using the 70916/93462 rows from a combined fit\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Df Deviance   AIC\n",
      "+ has_children       1    28421 36937\n",
      "+ interest_veterans  1    28429 36945\n",
      "+ catalog_shopper    1    28429 36945\n",
      "+ age                1    28429 36945\n",
      "<none>                    28431 36945\n",
      "+ pet_owner          1    28430 36945\n",
      "+ interest_religion  1    28431 36947\n",
      "+ recency            1    28431 36947\n",
      "+ bad_address        1    28431 36947\n",
      "+ veteran            1    28431 36947\n",
      "\n",
      "Step:  AIC=36938.08\n",
      "donated ~ frequency + money + wealth_rating + has_children\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, :\n",
      "\"using the 70916/93462 rows from a combined fit\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Df Deviance   AIC\n",
      "+ pet_owner          1    28418 36937\n",
      "+ catalog_shopper    1    28418 36937\n",
      "+ interest_veterans  1    28418 36937\n",
      "<none>                    28421 36938\n",
      "+ interest_religion  1    28420 36939\n",
      "+ recency            1    28421 36940\n",
      "+ age                1    28421 36940\n",
      "+ bad_address        1    28421 36940\n",
      "+ veteran            1    28421 36940\n",
      "\n",
      "Step:  AIC=36932.08\n",
      "donated ~ frequency + money + wealth_rating + has_children + \n",
      "    pet_owner\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, :\n",
      "\"using the 70916/93462 rows from a combined fit\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Df Deviance   AIC\n",
      "<none>                    28418 36932\n",
      "+ interest_veterans  1    28416 36932\n",
      "+ catalog_shopper    1    28416 36932\n",
      "+ age                1    28417 36933\n",
      "+ recency            1    28417 36934\n",
      "+ interest_religion  1    28417 36934\n",
      "+ bad_address        1    28418 36934\n",
      "+ veteran            1    28418 36934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'pROC' was built under R version 3.5.3\"Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "Attaching package: 'pROC'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    cov, smooth, var\n",
      "\n",
      "Setting levels: control = 0, case = 1\n",
      "Setting direction: controls < cases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.585470794088965"
      ],
      "text/latex": [
       "0.585470794088965"
      ],
      "text/markdown": [
       "0.585470794088965"
      ],
      "text/plain": [
       "Area under the curve: 0.5855"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAANlBMVEUAAABNTU1oaGh8fHyM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\nAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(repr)\n",
    "# Change plot size to 4 x 3\n",
    "    options(repr.plot.width=4, repr.plot.height=3)\n",
    "\n",
    "\n",
    "\n",
    "# Specify a null model with no predictors\n",
    "null_model <- glm(donated ~ 1, data = donors, family = \"binomial\")\n",
    "\n",
    "# Specify the full model using all of the potential predictors\n",
    "full_model <- glm(donated ~ . , data = donors, family = \"binomial\")\n",
    "\n",
    "# Use a forward stepwise algorithm to build a parsimonious model\n",
    "step_model <- step(null_model, scope = list(lower = null_model, upper = full_model), direction = \"forward\")\n",
    "\n",
    "# Estimate the stepwise donation probability\n",
    "step_prob <- predict(step_model, type = \"response\")\n",
    "\n",
    "# Plot the ROC of the stepwise model\n",
    "library(pROC)\n",
    "ROC <- roc(donors$donated, step_prob)\n",
    "plot(ROC, col = \"red\")\n",
    "auc(ROC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Classification Trees\n",
    "Classification trees use flowchart-like structures to make decisions. Because humans can readily understand these tree structures, classification trees are useful when transparency is needed, such as in loan approval. We'll use the Lending Club dataset to simulate this scenario.\n",
    "\n",
    "\n",
    "### 1.1) (video) Making decisions with trees\n",
    "Sometimes a difficult or complex decision can be made simpler by breaking it down into a series of smaller decisions, if you're considering whether to take a new job offer, you might define requirements for accepting the position. \n",
    "\n",
    "does it offer a high enough salary?, does it have a long commute or require long hours? does it provide free coffe?.\n",
    "\n",
    "![](./Imagenes/decision_tree1.jpg)\n",
    "\n",
    "`Classification trees` also know as `decision trees` work much the same way, they are used to find a of set of if-else conditions that are  helpulf for taking action, as you will see soon because their decisions are easily  understood without statistics, they can be useful for bussines strategy, especially areas where transperency is neded, like loan application approval.\n",
    "\n",
    "Let's start by considering the  `decision tree structure` as you might expect, it closely resembles real-world trees, the goal is to model the relantionship between predictors and an outcome of interest, beginning at the root node, the data flows through if-else decision nodes that split the data according to its atributes, the branches indicate the potencial choices, and the leaf nodes denote the final decision, \n",
    "\n",
    "these are also know as terminal nodes because they terminate the decision making process,  to understan how the `tree strucutre ` is build, let's consider a bussines process like whether or not to provide someone a loan, after an applicant filsl out a form with his personal information like income, credit history and loan purpose, the bank must quickly decide whether or not individual is likely to repay the debt, using historical applicant data and loan outcomes, a classification tree can be built to learn the criteria that were most predictive of future loan repayment.\n",
    "\n",
    "Growing the decision tree uses a process called `divide and conquer` because it attempts to divde the dataset into partitions with similar values for the outcome of interest for loan applications, it needs to seprate the applicatns who are likely to repay dro those who are likely to default on the debt. \n",
    "\n",
    "Suppose the tree considers 2 aspects of each applicant : the credit score and requested loan amount (image 2) to divide and conquer, the algorithm look for for an initial split that creates the two most momogeneous groups, first, it split into groups high and low credit scores, then, it divides and conquers again with another split, creating groups for high and low requested loan amounts each one of these splits results in an `if-else` decision in the tree structure.\n",
    "\n",
    "![](./Imagenes/decision_tree2.jpg)\n",
    "\n",
    "Obviously, a decision tree built on actual lending data is likely to be much more complex. \n",
    "\n",
    "There are several packags taht can be used to build classifcation trees in R, one of the most widely used is called `rpart` for recursive partitioning, as synonym for  divide-and-conquer\n",
    "\n",
    "    #building a simplerpart classification tree\n",
    "    library(rpart)\n",
    "    m<- rpart(outcome ~ loan_amount + credit_score, data = loans, method = \"class\")\n",
    "\n",
    "    #makng predictions from an rpart tree\n",
    "    p<-predict(m, test_Data, type = \"class\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1) Building a simple decision tree\n",
    "The `loans` dataset contains 11,312 randomly-selected people who applied for and later received loans from Lending Club, a US-based peer-to-peer lending company.\n",
    "\n",
    "You will use a decision tree to try to learn patterns in the outcome of these loans (either repaid or default) based on the requested loan amount and credit score at the time of application.\n",
    "\n",
    "Then, see how the tree's predictions differ for an applicant with good credit versus one with bad credit.\n",
    "\n",
    "The dataset `loans` is already in your workspace.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "- Load the `rpart` package.\n",
    "- Fit a decision tree model with the function `rpart()`.\n",
    "    - Supply the R formula that specifies `outcome` as a function of `loan_amount` and `credit_score` as the first argument.\n",
    "    - Leave the control argument alone for now. (You'll learn more about that later!)\n",
    "- Use `predict()` with the resulting loan model to predict the outcome for the `good_credit` applicant. Use the `type` argument to predict the `\"class\"` of the outcome.\n",
    "- Do the same for the `bad_credit` applicant.\n",
    "\n",
    "*Answer*\n",
    "\n",
    "    # Load the rpart package\n",
    "    library(rpart)\n",
    "\n",
    "    # Build a lending model predicting loan outcome versus loan amount and credit score\n",
    "    loan_model <- rpart(outcome ~ loan_amount + credit_score, data = loans, method = \"class\", control = rpart.control(cp = 0))\n",
    "\n",
    "    # Make a prediction for someone with good credit\n",
    "    predict(loan_model, good_credit, type = \"class\")\n",
    "\n",
    "    # Make a prediction for someone with bad credit\n",
    "    predict(loan_model, bad_credit, type = \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2) Visualizing classification trees\n",
    "Due to government rules to prevent illegal discrimination, lenders are required to explain why a loan application was rejected.\n",
    "\n",
    "The structure of classification trees can be depicted visually, which helps to understand how the tree makes its decisions. The model `loan_model` that you fit in the last exercise is in your workspace.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "- Type `loan_model` to see a text representation of the classification tree.\n",
    "- Load the `rpart.plot` package.\n",
    "- Apply the `rpart.plot()` function to the loan model to visualize the tree.\n",
    "- See how changing other plotting parameters impacts the visualization by running the supplied command.\n",
    "\n",
    "*Answer*\n",
    "\n",
    "    # Examine the loan_model object\n",
    "    loan_model\n",
    "\n",
    "    # Load the rpart.plot package\n",
    "    library(rpart.plot)\n",
    "\n",
    "    # Plot the loan_model with default settings\n",
    "    rpart.plot(loan_model)\n",
    "\n",
    "    # Plot the loan_model with customized settings\n",
    "    rpart.plot(loan_model, type = 3, box.palette = c(\"red\", \"green\"), fallen.leaves = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>keep</th><th scope=col>rand</th><th scope=col>default</th><th scope=col>loan_amount</th><th scope=col>emp_length</th><th scope=col>home_ownership</th><th scope=col>income</th><th scope=col>loan_purpose</th><th scope=col>debt_to_income</th><th scope=col>credit_score</th><th scope=col>recent_inquiry</th><th scope=col>delinquent</th><th scope=col>credit_accounts</th><th scope=col>bad_public_record</th><th scope=col>credit_utilization</th><th scope=col>past_bankrupt</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1                    </td><td>0.1304652            </td><td>0                    </td><td>LOW                  </td><td>10+ years            </td><td>RENT                 </td><td>LOW                  </td><td>credit_card          </td><td>HIGH                 </td><td>AVERAGE              </td><td>YES                  </td><td>NEVER                </td><td>FEW                  </td><td>NO                   </td><td>HIGH                 </td><td>NO                   </td></tr>\n",
       "\t<tr><td>1                                                            </td><td>0.9981510                                                    </td><td>1                                                            </td><td><span style=white-space:pre-wrap>LOW   </span>               </td><td><span style=white-space:pre-wrap>&lt; 2 years  </span>       </td><td>RENT                                                         </td><td><span style=white-space:pre-wrap>LOW   </span>               </td><td><span style=white-space:pre-wrap>car           </span>       </td><td><span style=white-space:pre-wrap>LOW    </span>              </td><td>AVERAGE                                                      </td><td>YES                                                          </td><td><span style=white-space:pre-wrap>NEVER                </span></td><td><span style=white-space:pre-wrap>FEW    </span>              </td><td>NO                                                           </td><td><span style=white-space:pre-wrap>LOW   </span>               </td><td>NO                                                           </td></tr>\n",
       "\t<tr><td>0                    </td><td>0.6282756            </td><td>0                    </td><td>LOW                  </td><td>10+ years            </td><td>RENT                 </td><td>LOW                  </td><td>small_business       </td><td>AVERAGE              </td><td>AVERAGE              </td><td>YES                  </td><td>NEVER                </td><td>FEW                  </td><td>NO                   </td><td>HIGH                 </td><td>NO                   </td></tr>\n",
       "\t<tr><td>0                    </td><td>0.2524072            </td><td>0                    </td><td>MEDIUM               </td><td>10+ years            </td><td>RENT                 </td><td>MEDIUM               </td><td>other                </td><td>HIGH                 </td><td>AVERAGE              </td><td>YES                  </td><td>MORE THAN 2 YEARS AGO</td><td>AVERAGE              </td><td>NO                   </td><td>LOW                  </td><td>NO                   </td></tr>\n",
       "\t<tr><td>0                                                     </td><td>0.4744546                                             </td><td>0                                                     </td><td><span style=white-space:pre-wrap>LOW   </span>        </td><td><span style=white-space:pre-wrap>&lt; 2 years  </span></td><td>RENT                                                  </td><td><span style=white-space:pre-wrap>HIGH  </span>        </td><td><span style=white-space:pre-wrap>other         </span></td><td>AVERAGE                                               </td><td>AVERAGE                                               </td><td>NO                                                    </td><td>MORE THAN 2 YEARS AGO                                 </td><td><span style=white-space:pre-wrap>MANY   </span>       </td><td>NO                                                    </td><td>MEDIUM                                                </td><td>NO                                                    </td></tr>\n",
       "\t<tr><td>0                    </td><td>0.7499317            </td><td>0                    </td><td>LOW                  </td><td>2 - 5 years          </td><td>RENT                 </td><td>LOW                  </td><td>wedding              </td><td>AVERAGE              </td><td>AVERAGE              </td><td>YES                  </td><td>NEVER                </td><td>AVERAGE              </td><td>NO                   </td><td>MEDIUM               </td><td>NO                   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllll}\n",
       " keep & rand & default & loan\\_amount & emp\\_length & home\\_ownership & income & loan\\_purpose & debt\\_to\\_income & credit\\_score & recent\\_inquiry & delinquent & credit\\_accounts & bad\\_public\\_record & credit\\_utilization & past\\_bankrupt\\\\\n",
       "\\hline\n",
       "\t 1                     & 0.1304652             & 0                     & LOW                   & 10+ years             & RENT                  & LOW                   & credit\\_card         & HIGH                  & AVERAGE               & YES                   & NEVER                 & FEW                   & NO                    & HIGH                  & NO                   \\\\\n",
       "\t 1                     & 0.9981510             & 1                     & LOW                   & < 2 years             & RENT                  & LOW                   & car                   & LOW                   & AVERAGE               & YES                   & NEVER                 & FEW                   & NO                    & LOW                   & NO                   \\\\\n",
       "\t 0                     & 0.6282756             & 0                     & LOW                   & 10+ years             & RENT                  & LOW                   & small\\_business      & AVERAGE               & AVERAGE               & YES                   & NEVER                 & FEW                   & NO                    & HIGH                  & NO                   \\\\\n",
       "\t 0                     & 0.2524072             & 0                     & MEDIUM                & 10+ years             & RENT                  & MEDIUM                & other                 & HIGH                  & AVERAGE               & YES                   & MORE THAN 2 YEARS AGO & AVERAGE               & NO                    & LOW                   & NO                   \\\\\n",
       "\t 0                     & 0.4744546             & 0                     & LOW                   & < 2 years             & RENT                  & HIGH                  & other                 & AVERAGE               & AVERAGE               & NO                    & MORE THAN 2 YEARS AGO & MANY                  & NO                    & MEDIUM                & NO                   \\\\\n",
       "\t 0                     & 0.7499317             & 0                     & LOW                   & 2 - 5 years           & RENT                  & LOW                   & wedding               & AVERAGE               & AVERAGE               & YES                   & NEVER                 & AVERAGE               & NO                    & MEDIUM                & NO                   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "keep | rand | default | loan_amount | emp_length | home_ownership | income | loan_purpose | debt_to_income | credit_score | recent_inquiry | delinquent | credit_accounts | bad_public_record | credit_utilization | past_bankrupt | \n",
       "|---|---|---|---|---|---|\n",
       "| 1                     | 0.1304652             | 0                     | LOW                   | 10+ years             | RENT                  | LOW                   | credit_card           | HIGH                  | AVERAGE               | YES                   | NEVER                 | FEW                   | NO                    | HIGH                  | NO                    | \n",
       "| 1                     | 0.9981510             | 1                     | LOW                   | < 2 years             | RENT                  | LOW                   | car                   | LOW                   | AVERAGE               | YES                   | NEVER                 | FEW                   | NO                    | LOW                   | NO                    | \n",
       "| 0                     | 0.6282756             | 0                     | LOW                   | 10+ years             | RENT                  | LOW                   | small_business        | AVERAGE               | AVERAGE               | YES                   | NEVER                 | FEW                   | NO                    | HIGH                  | NO                    | \n",
       "| 0                     | 0.2524072             | 0                     | MEDIUM                | 10+ years             | RENT                  | MEDIUM                | other                 | HIGH                  | AVERAGE               | YES                   | MORE THAN 2 YEARS AGO | AVERAGE               | NO                    | LOW                   | NO                    | \n",
       "| 0                     | 0.4744546             | 0                     | LOW                   | < 2 years             | RENT                  | HIGH                  | other                 | AVERAGE               | AVERAGE               | NO                    | MORE THAN 2 YEARS AGO | MANY                  | NO                    | MEDIUM                | NO                    | \n",
       "| 0                     | 0.7499317             | 0                     | LOW                   | 2 - 5 years           | RENT                  | LOW                   | wedding               | AVERAGE               | AVERAGE               | YES                   | NEVER                 | AVERAGE               | NO                    | MEDIUM                | NO                    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  keep rand      default loan_amount emp_length  home_ownership income\n",
       "1 1    0.1304652 0       LOW         10+ years   RENT           LOW   \n",
       "2 1    0.9981510 1       LOW         < 2 years   RENT           LOW   \n",
       "3 0    0.6282756 0       LOW         10+ years   RENT           LOW   \n",
       "4 0    0.2524072 0       MEDIUM      10+ years   RENT           MEDIUM\n",
       "5 0    0.4744546 0       LOW         < 2 years   RENT           HIGH  \n",
       "6 0    0.7499317 0       LOW         2 - 5 years RENT           LOW   \n",
       "  loan_purpose   debt_to_income credit_score recent_inquiry\n",
       "1 credit_card    HIGH           AVERAGE      YES           \n",
       "2 car            LOW            AVERAGE      YES           \n",
       "3 small_business AVERAGE        AVERAGE      YES           \n",
       "4 other          HIGH           AVERAGE      YES           \n",
       "5 other          AVERAGE        AVERAGE      NO            \n",
       "6 wedding        AVERAGE        AVERAGE      YES           \n",
       "  delinquent            credit_accounts bad_public_record credit_utilization\n",
       "1 NEVER                 FEW             NO                HIGH              \n",
       "2 NEVER                 FEW             NO                LOW               \n",
       "3 NEVER                 FEW             NO                HIGH              \n",
       "4 MORE THAN 2 YEARS AGO AVERAGE         NO                LOW               \n",
       "5 MORE THAN 2 YEARS AGO MANY            NO                MEDIUM            \n",
       "6 NEVER                 AVERAGE         NO                MEDIUM            \n",
       "  past_bankrupt\n",
       "1 NO           \n",
       "2 NO           \n",
       "3 NO           \n",
       "4 NO           \n",
       "5 NO           \n",
       "6 NO           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "39732"
      ],
      "text/latex": [
       "39732"
      ],
      "text/markdown": [
       "39732"
      ],
      "text/plain": [
       "[1] 39732"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 (video) Growing larger classification trees\n",
    "You will learn more about how trees grow, branch out, and sometmes even outgrow their environment. \n",
    "\n",
    "Earlier, you learned  that classification trees use divide-and-conquer , to identify splits that create the most \"pure\" or homogeneous, partition, to see how this works in practice, let's considerer the tree bein built with data on loan applicats' credit and requested amount, for each of these predictors, the algorithm attempts a split on the feature values and then calculates the purity of the resulting partitions, the split that produces the purest partitions will be used first (first split \"A\", divides the data into partitions high and low credit score) and while (split \"B\" ,divides  the data into large and small loan amounts) split B results in one very homogeneous partition, but its other partition is very mixed, in comparison, split A results in 2 partitions that are both relatively pure, as a result, the tree will choose split A first, in then continues to divide and conquer,always choosing the split that result in almost the most homogeneous partition, as the tree continuos to  grow, it create smaller and more homogeneous partitions as show here! \n",
    "\n",
    "![](./Imagenes/decision_tree3.jpg)\n",
    "\n",
    "You may noticed, however, that there was an easie way to create a set of perfectly-pure partitions, simple use a diagonal line to divide the outcomes, the reason tree didn't discover by itself, because a diagonal line requires the tree consider combination of two or more features, which is not possible in a divide and conquer process,  insted, a decission tree always creates so called axis-parallel splits because the split line output axis, unfortunately this create a potencial weakness of decision trees, they can be overly complex whe modeling certain paterns in the data\n",
    "\n",
    "Gennerally speaking, decision trees have the tendency to become very complex very quickly a tree can happily \"divide-and-conquer\" until it classifies every example correctly, or until it runs out of feature values to split upon, when a tree has grown overly large and overly complex, it may experience the problem of overfitting, rather than modeling the most important trends in the data, a tree that has been over-fitted tend to model the noise it focuses on extremely subtle patters that may not apply more generally, more-so that other machine learning algorithms classification trees have thisrendency to overfit the dataset it is trainen on.\n",
    "\n",
    "Thus is important construncting a test dataset that the algorithm cannot see when building the model, because if the tree performs much more poorly on the evaluation set thatn the training set, it suggets the model may have benn over-fitted \n",
    "\n",
    "![](./Imagenes/decision_tree4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1) Why do some branches split?\n",
    "A classification tree grows using a divide-and-conquer process. Each time the tree grows larger, it splits groups of data into smaller subgroups, creating new branches in the tree.\n",
    "\n",
    "Given a dataset to divide-and-conquer, which groups would the algorithm prioritize to split first?\n",
    "\n",
    "Answer: The group it can split to create the greatest improvement in subgroup homogeneity.\n",
    "\n",
    "#### 4.2.2) Creating random test datasets\n",
    "Before building a more sophisticated lending model, it is important to hold out a portion of the loan data to simulate how well it will predict the outcomes of future loan applicants.\n",
    "\n",
    "You can use 75% of the observations for training and 25% for testing the model.\n",
    "\n",
    "The `sample()` function can be used to generate a random sample of rows to include in the training set. Simply supply it the total number of observations and the number needed for training.\n",
    "\n",
    "Use the resulting vector of row IDs to subset the loans into training and testing datasets. The dataset `loans` is loaded in your workspace\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "- Apply the nrow() function to determine how many observations are in the loans dataset, and the number needed for a 75% sample.\n",
    "- Use the sample() function to create an integer vector of row IDs for the 75% sample. The first argument of sample() should be the number of rows in the data set, and the second is the number of rows you need in your training set.\n",
    "- Subset the loans data using the row IDs to create the training dataset. Save this as loans_train.\n",
    "- Subset loans again, but this time select all the rows that are not in sample_rows. Save this as loans_test\n",
    "\n",
    "*Answer*\n",
    "\n",
    "    # Determine the number of rows for training\n",
    "    nrow(loans) * 0.75\n",
    "\n",
    "    # Create a random sample of row IDs\n",
    "    sample_rows <- sample(nrow(loans), nrow(loans) * 0.75)\n",
    "\n",
    "    # Create the training dataset\n",
    "    loans_train <- loans[sample_rows, ]\n",
    "\n",
    "    # Create the test dataset\n",
    "    loans_test <- loans[-sample_rows, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3) Building and evaluating a larger tree\n",
    "Previously, you created a simple decision tree that used the applicant's credit score and requested loan amount to predict the loan outcome.\n",
    "\n",
    "Lending Club has additional information about the applicants, such as home ownership status, length of employment, loan purpose, and past bankruptcies, that may be useful for making more accurate predictions.\n",
    "\n",
    "Using all of the available applicant data, build a more sophisticated lending model using the random training dataset created previously. Then, use this model to make predictions on the testing dataset to estimate the performance of the model on future loan applications.\n",
    "\n",
    "The `rpart` package is loaded into the workspace and the `loans_train` and `loans_test` datasets have been created.\n",
    "\n",
    "**Exercise**\n",
    "- Use rpart() to build a loan model using the training dataset and all of the available predictors. Again, leave the control argument alone.\n",
    "- Applying the predict() function to the testing dataset, create a vector of predicted outcomes. Don't forget the type argument.\n",
    "- Create a table() to compare the predicted values to the actual outcome values.\n",
    "- Compute the accuracy of the predictions using the mean() function.\n",
    "\n",
    "\n",
    "*Answer*\n",
    "\n",
    "    library(rpart)\n",
    "    # Grow a tree using all of the available applicant data\n",
    "    loan_model <- rpart(outcome ~ . , data = loans_train, method = \"class\", control = rpart.control(cp = 0))\n",
    "\n",
    "    # Make predictions on the test dataset\n",
    "    loans_test$pred <- predict(loan_model, loans_test, type = \"class\")\n",
    "\n",
    "    # Examine the confusion matrix\n",
    "    table(loans_test$outcome, loans_test$pred)\n",
    "\n",
    "    # Compute the accuracy on the test dataset\n",
    "    mean(loans_test$outcome == loans_test$pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 (video) Tending to classification trees\n",
    "\n",
    "In the previous video, you learned that \"decision tree\" have a tendency to grow overly large and complex very quickly, if this were to happen to trees in your yard, you would be outside with clippers, looking to trim away of excess greenery\n",
    "\n",
    "Grooming healthy classification trees  likewise requires this kind of attention , in this leasson, your will learn about pruning strategies, which help ensure the trees are just right not too large, not too small.\n",
    "\n",
    "One method of preventing a tree from becoming too large involves sttoping the growing process early, this is know  as `pre-prunig`, perhaps the simplest approach to \"pre-pruning \" stops divide-and-conquer once the tree reaches a predefine size (for example a maximun depth of 3 levels), another \"pre-pruning\" method requieres a minimun number of observations at a node in order for a split to occur (for example stopping the growing any branch with fewer than 10 observations) both of these pre-pruning strategies prevent the tree from growing too large, however, a tree stopped too early may fail to discover subtle or important patterns it might have discovered later, to address this concern, it also possible to grow very large  tree, knowing that it will be overly complex, but then prune it back to reduce the size, this is know as `post-pruning`, in this, node and of branches with only a minor impact on  the tree's overall accurency are removed after the fact.\n",
    "\n",
    "![](./Imagenes/decision_tree5.jpg)\n",
    "\n",
    "The relationship between the trees complexity and the accurency can be depicited visually, as the tree becomes increasingly complex, the model makes fewer errors, however, though the performance improve a lot at first, it then improves only slightly for the later increases in complexity, this trend provides insight into the optimal point at which to prune the tree, simply look for the point at which the curve flattens, the horizontal dotted line identifies the point at which the error rate becomes statistically similar to the most complex model, typically, you shoul prune the tree at the compleecity level that results in a classification error rate just under this line.\n",
    "\n",
    "the rpart decision tree package provide a function creating this visualization, as well as performing pre and post pruning.\n",
    "\n",
    "![](./Imagenes/decision_tree6.jpg)\n",
    "\n",
    "`Pre-pruning` is performed when building the decision tree model whre `maxdepth` control the maximun depth of the decision tree, or a `minsplit` parameter that dictates the minimun number of observations a branch must contain in order for the tree to be allowed to split.\n",
    "\n",
    "    #pre-pruning with rpart\n",
    "    library(rpart)\n",
    "    prune_control <- rpart.control(maxdepth = 30, minsplit = 20)\n",
    "\n",
    "    m<-rpart(repaid ~ credit_score + request_amt) , data = loans, method = \"class\", control = prune_control)\n",
    "    \n",
    "`post-pruning` is applied to a decission tree model that has been previously built and the `plotcp` function will generate a visualization of the error rate versus model complexity, which provide insight into the optimal cutpoint for pruning, when this value has been identified, it can be supplied to the prune function's complecity paramter, cp, to create a simper pruned tree\n",
    "\n",
    "    #post-pruning with rpart\n",
    "    m<-rpart(repaid ~ credit_score + request_amt, data = loans, method = \"class\")\n",
    "\n",
    "    plotcp(m)\n",
    "    \n",
    "    m_pruned<-prune(m, cp = 0.20)\n",
    "    \n",
    "#### 4.3.1) Preventing overgrown trees\n",
    "The tree grown on the full set of applicant data grew to be extremely large and extremely complex, with hundreds of splits and leaf nodes containing only a handful of applicants. This tree would be almost impossible for a loan officer to interpret.\n",
    "\n",
    "Using the pre-pruning methods for early stopping, you can prevent a tree from growing too large and complex. See how the rpart control options for maximum tree depth and minimum split count impact the resulting tree.\n",
    "\n",
    "rpart is loaded.\n",
    "\n",
    "**Exercise** \n",
    "1 y 2\n",
    "\n",
    "- Use rpart() to build a loan model using the training dataset and all of the available predictors.\n",
    "    - Set the model `control`s using `rpart.control(`) with parameters `cp` set to `0` and `maxdepth` set to `6`.\n",
    "- See how the test set accuracy of the simpler model compares to the original accuracy of 58.3%.\n",
    "    - First create a vector of predictions using the `predict()` function.\n",
    "    - Compare the predictions to the actual outcomes and use `mean()` to calculate the accuracy.\n",
    "\n",
    "*Answer*\n",
    "\n",
    "    # Grow a tree with maxdepth of 6\n",
    "    loan_model <-  rpart(outcome ~ . , data = loans_train, method = \"class\", control = rpart.control(cp = 0, maxdepth = 6))\n",
    "\n",
    "    # Make a class prediction on the test set\n",
    "    loans_test$pred <- predict(loan_model, loans_test, type = \"class\")\n",
    "\n",
    "    # Compute the accuracy of the simpler tree\n",
    "    mean(loans_test$outcome == loans_test$pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- In the model `control`s, remove `maxdepth` and add a minimum split parameter, `minsplit`, set to 500.\n",
    "\n",
    "*Answer*\n",
    "\n",
    "    # Swap maxdepth for a minimum split of 500 \n",
    "    loan_model <- rpart(outcome ~ ., data = loans_train, method = \"class\", control = rpart.control(cp = 0, minsplit = 500))\n",
    "\n",
    "    # Run this. How does the accuracy change?\n",
    "    loans_test$pred <- predict(loan_model, loans_test, type = \"class\")\n",
    "    mean(loans_test$pred == loans_test$outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2) Creating a nicely pruned tree\n",
    "Stopping a tree from growing all the way can lead it to ignore some aspects of the data or miss important trends it may have discovered later.\n",
    "\n",
    "By using post-pruning, you can intentionally grow a large and complex tree then prune it to be smaller and more efficient later on.\n",
    "\n",
    "In this exercise, you will have the opportunity to construct a visualization of the tree's performance versus complexity, and use this information to prune the tree to an appropriate level.\n",
    "\n",
    "The rpart package is loaded into the workspace, along with loans_test and loans_train.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "- Use all of the applicant variables and no pre-pruning to create an overly complex tree. Make sure to set `cp = 0` in `rpart.control()` to prevent pre-pruning.\n",
    "- Create a complexity plot by using `plotcp()` on the model.\n",
    "- Based on the complexity plot, prune the tree to a complexity of 0.0014 using the `prune()` function with the tree and the complexity parameter.\n",
    "- Compare the accuracy of the pruned tree to the original accuracy of 58.3%. To calculate the accuracy use the `predict()` and `mean()` functions.\n",
    "\n",
    "*Answer*\n",
    "\n",
    "    # Grow an overly complex tree\n",
    "    loan_model <- rpart(outcome ~ ., data = loans_train, method = \"class\" , control = rpart.control(cp = 0))\n",
    "\n",
    "    # Examine the complexity plot\n",
    "    plotcp(loan_model)\n",
    "\n",
    "    # Prune the tree\n",
    "    loan_model_pruned <- prune(loan_model, cp = .0014)\n",
    "\n",
    "    # Compute the accuracy of the pruned tree\n",
    "    loans_test$pred <- predict(loan_model_pruned, loans_test, type = \"class\")\n",
    "    mean(loans_test$pred == loans_test$outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 (video) Seeing the forest from the trees\n",
    "Consider the way that decision trees parallel trees in the natural environment: from a root node that grow into branches and leaf nodes that sometimes need pruning, you might think that by now we would have exhausted the tree metaphors, in fact, there is one more, just as living trees can be grouped as a forest, a number of classification trees can be combined into a colletion known as a decision tree forest, they are among the most powerful machine learning classifiers, yet remain remarkably efficiente and easy to use, because of their combine versatility and power, decision tree forest have become one of the most popular approaches for classification this power does not come form a single tree that has grown large and comple, but rather from a collection of smaller,  simplier trees that together reflect the data's complexity each of the forest's trees is diverse, and may reflect some subtle pattern in the outcome to be modeled generating this diversity is the key to building powerful decision tree forests, however, if you were to grow 100 tress on the same set of data, you'd have 100 times the same tree, so growing diverse trees requires the growing conditions to be varied from tree to tree this is done by allocating each tree a random subset of data, one may receive a vastly diffrente training set than another then term random forest refers to a specific growing algorithm in which both the features and examples may differ from tree to tree.\n",
    "\n",
    "it seems somewhat counterintuitive to think that a group of trees built on small, randomsubsets of the data could perform any better than a single really complex tree that had the benefit of learning the entire dataset, but the forest's power is based on the same principles that govern successful team work business or on the athletic field in these cases, it iscertainly advantageous to have team members that are extremely good at some tasks, however, thse people typically have weaknesses in other areas for this reason, it is even better for the team to have members with complementary skills even if none of the memvers is especially strong, good teamwork usually wins.\n",
    "\n",
    "Machine learning methods like random forest that apply this principle are called `ensemble methods`  all ensemble methods are based on the principle that weaker learners become stronger with teamwork in a random forest, each tree is asked to make a prediction, and the group's overall prediction is determined by a majority vote though each tree may reflect only a narrow portion of the data the overall consensus is strengthened by these diverse perspectives.\n",
    "\n",
    "The R package `randomForest` implements the random forest algorithm, the function offers 2 parameters of note. \n",
    "\n",
    "- **ntree**: dictates the number of trees to include in the forest (setting this sufficiently large)\n",
    "- **mtry**: is the number of features selected at random for each tree, by default it uses the square root of the total number of predictors \n",
    "\n",
    "\n",
    "    # building a simple random forest\n",
    "    library(randomForest)\n",
    "    m<-randomForest(repaid ~ credit_score + request_amt, data = loans,\n",
    "    ntree = 500,  #number of trees in the forest\n",
    "    mtry = sqrt(p) # number of predictors (p) per tree\n",
    "    )\n",
    "\n",
    "    #maging predictions from a random forest\n",
    "    p<-predict(m, test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1) Building a random forest model\n",
    "In spite of the fact that a forest can contain hundreds of trees, growing a decision tree forest is perhaps even easier than creating a single highly-tuned tree.\n",
    "\n",
    "Using the `randomForest` package, build a random forest and see how it compares to the single trees you built previously.\n",
    "\n",
    "Keep in mind that due to the random nature of the forest, the results may vary slightly each time you create the forest.\n",
    "\n",
    "**Exercise**\n",
    "- Load the `randomForest` package.\n",
    "- Build a random forest model using all of the loan application variables. The `randomForest` function also uses the formula interface.\n",
    "- Compute the accuracy of the random forest model to compare to the original tree's accuracy of 58.3% using `predict()` and `mean()`.\n",
    "\n",
    "*Answer*\n",
    "\n",
    "    # Load the randomForest package\n",
    "    library(randomForest)\n",
    "\n",
    "    # Build a random forest model\n",
    "    loan_model <- randomForest(outcome ~ . , data = loans_train)\n",
    "\n",
    "    # Compute the accuracy of the random forest\n",
    "    loans_test$pred <- predict(loan_model, loans_test)\n",
    "    mean(loans_test$outcome == loans_test$pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(repr)\n",
    "# Change plot size to 4 x 3\n",
    "    options(repr.plot.width=4, repr.plot.height=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "*Answer*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
